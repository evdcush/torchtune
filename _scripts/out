W0222 17:10:37.100000 140502762804224 torch/distributed/run.py:717] 
W0222 17:10:37.100000 140502762804224 torch/distributed/run.py:717] *****************************************
W0222 17:10:37.100000 140502762804224 torch/distributed/run.py:717] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 17:10:37.100000 140502762804224 torch/distributed/run.py:717] *****************************************
RV: freqs device meta
RV: freqs device meta
RV: freqs device meta
RV: freqs device meta
RV: freqs device meta
RV: freqs device meta
RV: freqs device meta
RV: freqs device meta
RV: fair_ckpt_dtype_set: {torch.bfloat16}
RV: fair_ckpt_dtype_set: {torch.bfloat16}
RV: fair_ckpt_dtype_set: {torch.bfloat16}
RV: fair_ckpt_dtype_set: {torch.bfloat16}
RV: fair_ckpt_dtype_set: {torch.bfloat16}
RV: fair_ckpt_dtype_set: {torch.bfloat16}
RV: fair_ckpt_dtype_set: {torch.bfloat16}
RV: fair_ckpt_dtype_set: {torch.bfloat16}
WARNING:root:input_dtypes {torch.bfloat16}
WARNING:root:input_dtypes {torch.bfloat16}
WARNING:root:input_dtypes {torch.bfloat16}
WARNING:root:input_dtypes {torch.bfloat16}
WARNING:root:input_dtypes {torch.bfloat16}
WARNING:root:input_dtypes {torch.bfloat16}
WARNING:root:input_dtypes {torch.bfloat16}
WARNING:root:input_dtypes {torch.bfloat16}
NCCL version 2.19.3+cuda12.1
WARNING:root:layers.0.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.0.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.0.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.0.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.0.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.0.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.0.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.0.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.1.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.1.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.1.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.1.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.1.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.1.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.1.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.1.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.2.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.2.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.2.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.2.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.2.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.2.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.2.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.2.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.3.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.3.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.3.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.3.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.3.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.3.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.3.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.3.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.4.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.4.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.4.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.4.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.4.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.4.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.4.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.4.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.5.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.5.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.5.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.5.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.5.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.5.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.5.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.5.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.6.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.6.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.6.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.6.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.6.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.6.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.6.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.6.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.7.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.7.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.7.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.7.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.7.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.7.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.7.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.7.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.8.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.8.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.8.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.8.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.8.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.8.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.8.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.8.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.9.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.9.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.9.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.9.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.9.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.9.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.9.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.9.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.10.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.10.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.10.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.10.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.10.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.10.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.10.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.10.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.11.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.11.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.11.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.11.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.11.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.11.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.11.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.11.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.12.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.12.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.12.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.12.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.12.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.12.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.12.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.12.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.13.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.13.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.13.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.13.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.13.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.13.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.13.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.13.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.14.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.14.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.14.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.14.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.14.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.14.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.14.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.14.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.15.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.15.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.15.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.15.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.15.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.15.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.15.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.15.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.16.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.16.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.16.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.16.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.16.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.16.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.16.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.16.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.17.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.17.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.17.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.17.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.17.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.17.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.17.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.17.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.18.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.18.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.18.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.18.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.18.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.18.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.18.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.18.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.19.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.19.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.19.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.19.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.19.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.19.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.19.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.19.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.20.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.20.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.20.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.20.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.20.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.20.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.20.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.20.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.21.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.21.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.21.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.21.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.21.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.21.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.21.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.21.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.22.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.22.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.22.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.22.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.22.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.22.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.22.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.22.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.23.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.23.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.23.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.23.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.23.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.23.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.23.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.23.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.24.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.24.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.24.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.24.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.24.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.24.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.24.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.24.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.25.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.25.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.25.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.25.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.25.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.25.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.25.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.25.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.26.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.26.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.26.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.26.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.26.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.26.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.26.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.26.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.27.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.27.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.27.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.27.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.27.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.27.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.27.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.27.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.28.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.28.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.28.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.28.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.28.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.28.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.28.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.28.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.29.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.29.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.29.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.29.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.29.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.29.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.29.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.29.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.30.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.30.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.30.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.30.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.30.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.30.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.30.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.30.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.31.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.31.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.31.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.31.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.31.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.31.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.31.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.31.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.32.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.32.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.32.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.32.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.32.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.32.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.32.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.32.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.33.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.33.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.33.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.33.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.33.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.33.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.33.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.33.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.34.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.34.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.34.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.34.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.34.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.34.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.34.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.34.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.35.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.35.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.35.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.35.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.35.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.35.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.35.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.35.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.36.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.36.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.36.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.36.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.36.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.36.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.36.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.36.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.37.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.37.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.37.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.37.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.37.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.37.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.37.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.37.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.38.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.38.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.38.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.38.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.38.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.38.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.38.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.38.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.39.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.39.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.39.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.39.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.39.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.39.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.39.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.39.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.40.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.40.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.40.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.40.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.40.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.40.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.40.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.40.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.41.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.41.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.41.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.41.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.41.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.41.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.41.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.41.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.42.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.42.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.42.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.42.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.42.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.42.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.42.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.42.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.43.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.43.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.43.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.43.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.43.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.43.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.43.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.43.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.44.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.44.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.44.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.44.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.44.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.44.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.44.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.44.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.45.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.45.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.45.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.45.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.45.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.45.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.45.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.45.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.46.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.46.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.46.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.46.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.46.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.46.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.46.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.46.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.47.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.47.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.47.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.47.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.47.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.47.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.47.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.47.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.48.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.48.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.48.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.48.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.48.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.48.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.48.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.48.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.49.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.49.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.49.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.49.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.49.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.49.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.49.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.49.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.50.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.50.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.50.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.50.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.50.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.50.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.50.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.50.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.51.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.51.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.51.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.51.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.51.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.51.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.51.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.51.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.52.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.52.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.52.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.52.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.52.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.52.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.52.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.52.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.53.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.53.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.53.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.53.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.53.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.53.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.53.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.53.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.54.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.54.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.54.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.54.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.54.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.54.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.54.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.54.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.55.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.55.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.55.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.55.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.55.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.55.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.55.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.55.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.56.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.56.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.56.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.56.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.56.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.56.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.56.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.56.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.57.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.57.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.57.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.57.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.57.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.57.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.57.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.57.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.58.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.58.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.58.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.58.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.58.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.58.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.58.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.58.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.59.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.59.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.59.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.59.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.59.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.59.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.59.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.59.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.60.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.60.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.60.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.60.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.60.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.60.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.60.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.60.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.61.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.61.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.61.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.61.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.61.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.61.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.61.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.61.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.62.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.62.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.62.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.62.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.62.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.62.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.62.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.62.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.63.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.63.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.63.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.63.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.63.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.63.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.63.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.63.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.64.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.64.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.64.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.64.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.64.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.64.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.64.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.64.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.65.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.65.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.65.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.65.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.65.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.65.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.65.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.65.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.66.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.66.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.66.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.66.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.66.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.66.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.66.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.66.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.67.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.67.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.67.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.67.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.67.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.67.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.67.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.67.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.68.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.68.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.68.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.68.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.68.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.68.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.68.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.68.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.69.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.69.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.69.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.69.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.69.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.69.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.69.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.69.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.70.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.70.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.70.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.70.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.70.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.70.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.70.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.70.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.71.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.71.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.71.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.71.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.71.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.71.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.71.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.71.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.72.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.72.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.72.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.72.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.72.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.72.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.72.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.72.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.73.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.73.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.73.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.73.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.73.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.73.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.73.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.73.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.74.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.74.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.74.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.74.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.74.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.74.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.74.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.74.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.75.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.75.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.75.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.75.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.75.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.75.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.75.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.75.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.76.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.76.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.76.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.76.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.76.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.76.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.76.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.76.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.77.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.77.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.77.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.77.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.77.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.77.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.77.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.77.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.78.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.78.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.78.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.78.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.78.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.78.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.78.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.78.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.79.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.79.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.79.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.79.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.79.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.79.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.79.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
WARNING:root:layers.79.attention_norm.weight torch.Size([8192]) torch.Size([8192]) - not sharded
/home/rvarm1/.conda/envs/tune/lib/python3.10/site-packages/torch/distributed/_shard/sharded_tensor/api.py:1132: UserWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  warnings.warn(DEPRECATE_MSG)
/home/rvarm1/.conda/envs/tune/lib/python3.10/site-packages/torch/distributed/_shard/sharded_tensor/api.py:1132: UserWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  warnings.warn(DEPRECATE_MSG)
/home/rvarm1/.conda/envs/tune/lib/python3.10/site-packages/torch/distributed/_shard/sharded_tensor/api.py:1132: UserWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  warnings.warn(DEPRECATE_MSG)
/home/rvarm1/.conda/envs/tune/lib/python3.10/site-packages/torch/distributed/_shard/sharded_tensor/api.py:1132: UserWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  warnings.warn(DEPRECATE_MSG)
/home/rvarm1/.conda/envs/tune/lib/python3.10/site-packages/torch/distributed/_shard/sharded_tensor/api.py:1132: UserWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  warnings.warn(DEPRECATE_MSG)
/home/rvarm1/.conda/envs/tune/lib/python3.10/site-packages/torch/distributed/_shard/sharded_tensor/api.py:1132: UserWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  warnings.warn(DEPRECATE_MSG)
/home/rvarm1/.conda/envs/tune/lib/python3.10/site-packages/torch/distributed/_shard/sharded_tensor/api.py:1132: UserWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  warnings.warn(DEPRECATE_MSG)
/home/rvarm1/.conda/envs/tune/lib/python3.10/site-packages/torch/distributed/_shard/sharded_tensor/api.py:1132: UserWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  warnings.warn(DEPRECATE_MSG)
WARNING:root:Made dist_state_dict with dtypes {torch.bfloat16}
WARNING:root:Made dist_state_dict with dtypes {torch.bfloat16}
WARNING:root:Made dist_state_dict with dtypes {torch.bfloat16}
WARNING:root:Made dist_state_dict with dtypes {torch.bfloat16}
WARNING:root:Made dist_state_dict with dtypes {torch.bfloat16}
RV: about to map weights
RV: about to map weights
WARNING:root:Made dist_state_dict with dtypes {torch.bfloat16}
Loaded original state dict
Loaded original state dict
RV: about to map weightsINFO:torchtune.utils.logging:Loaded original state dict
INFO:torchtune.utils.logging:Loaded original state dict

RV: about to map weights
Loaded original state dict
Loaded original state dict
INFO:torchtune.utils.logging:Loaded original state dict
INFO:torchtune.utils.logging:Loaded original state dict
RV: about to map weights
Loaded original state dict
WARNING:root:Made dist_state_dict with dtypes {torch.bfloat16}
INFO:torchtune.utils.logging:Loaded original state dict
RV: about to map weights
Loaded original state dict
INFO:torchtune.utils.logging:Loaded original state dict
WARNING:root:Made dist_state_dict with dtypes {torch.bfloat16}
Mapping weights:   0%|          | 0/724 [00:00<?, ?it/s]Mapping weights:   0%|          | 0/724 [00:00<?, ?it/s]Mapping weights:   0%|          | 0/724 [00:00<?, ?it/s]Mapping weights:   0%|          | 0/724 [00:00<?, ?it/s]Mapping weights:   0%|          | 0/724 [00:00<?, ?it/s]RV: about to map weightsMapping weights:   0%|          | 0/724 [00:00<?, ?it/s]
Loaded original state dict
INFO:torchtune.utils.logging:Loaded original state dict
RV: about to map weights
Loaded original state dict
INFO:torchtune.utils.logging:Loaded original state dict
Mapping weights:   0%|          | 0/724 [00:00<?, ?it/s]Mapping weights: 100%|██████████| 724/724 [00:00<00:00, 566544.05it/s]
Mapping weights: 100%|██████████| 724/724 [00:00<00:00, 569305.60it/s]Mapping weights: 100%|██████████| 724/724 [00:00<00:00, 569412.36it/s]RV: DONE mapping weights, now converting state_dict
Mapping weights: 100%|██████████| 724/724 [00:00<00:00, 571340.75it/s]

RV: DONE mapping weights, now converting state_dict
RV: DONE mapping weights, now converting state_dict
RV: DONE mapping weights, now converting state_dict

Mapping weights: 100%|██████████| 724/724 [00:00<00:00, 566016.05it/s]
Mapping weights:   0%|          | 0/724 [00:00<?, ?it/s]RV: DONE mapping weights, now converting state_dictMapping weights: 100%|██████████| 724/724 [00:00<00:00, 557392.82it/s]

RV: DONE mapping weights, now converting state_dict
Mapping weights: 100%|██████████| 724/724 [00:00<00:00, 522573.76it/s]
RV: DONE mapping weights, now converting state_dict
Mapping weights: 100%|██████████| 724/724 [00:00<00:00, 552223.33it/s]
RV: DONE mapping weights, now converting state_dict
Expected tok_embeddings.weight to be bf16, got torch.float32
Expected layers.0.sa_norm.scale to be bf16, got torch.float32
Expected layers.0.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.0.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.0.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.0.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.0.mlp_norm.scale to be bf16, got torch.float32
Expected layers.0.mlp.w1.weight to be bf16, got torch.float32
Expected layers.0.mlp.w2.weight to be bf16, got torch.float32
Expected layers.0.mlp.w3.weight to be bf16, got torch.float32
Expected layers.1.sa_norm.scale to be bf16, got torch.float32Expected tok_embeddings.weight to be bf16, got torch.float32

Expected layers.1.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.0.sa_norm.scale to be bf16, got torch.float32
Expected layers.1.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.0.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.1.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.0.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.1.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.0.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.1.mlp_norm.scale to be bf16, got torch.float32
Expected layers.0.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.1.mlp.w1.weight to be bf16, got torch.float32
Expected layers.0.mlp_norm.scale to be bf16, got torch.float32
Expected layers.1.mlp.w2.weight to be bf16, got torch.float32
Expected layers.0.mlp.w1.weight to be bf16, got torch.float32
Expected layers.1.mlp.w3.weight to be bf16, got torch.float32
Expected layers.0.mlp.w2.weight to be bf16, got torch.float32
Expected layers.2.sa_norm.scale to be bf16, got torch.float32
Expected layers.0.mlp.w3.weight to be bf16, got torch.float32
Expected layers.2.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.1.sa_norm.scale to be bf16, got torch.float32
Expected layers.2.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.1.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.2.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.1.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.2.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.1.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.2.mlp_norm.scale to be bf16, got torch.float32
Expected layers.1.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.2.mlp.w1.weight to be bf16, got torch.float32
Expected layers.1.mlp_norm.scale to be bf16, got torch.float32
Expected layers.2.mlp.w2.weight to be bf16, got torch.float32
Expected layers.1.mlp.w1.weight to be bf16, got torch.float32
Expected layers.2.mlp.w3.weight to be bf16, got torch.float32
Expected layers.1.mlp.w2.weight to be bf16, got torch.float32
Expected layers.3.sa_norm.scale to be bf16, got torch.float32
Expected layers.1.mlp.w3.weight to be bf16, got torch.float32
Expected layers.3.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.2.sa_norm.scale to be bf16, got torch.float32
Expected layers.3.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.2.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.3.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.2.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.3.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.2.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.3.mlp_norm.scale to be bf16, got torch.float32
Expected layers.2.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.3.mlp.w1.weight to be bf16, got torch.float32
Expected layers.2.mlp_norm.scale to be bf16, got torch.float32Expected tok_embeddings.weight to be bf16, got torch.float32
Expected layers.3.mlp.w2.weight to be bf16, got torch.float32

Expected layers.2.mlp.w1.weight to be bf16, got torch.float32
Expected layers.0.sa_norm.scale to be bf16, got torch.float32Expected layers.3.mlp.w3.weight to be bf16, got torch.float32

Expected layers.2.mlp.w2.weight to be bf16, got torch.float32
Expected layers.4.sa_norm.scale to be bf16, got torch.float32
Expected layers.0.attn.q_proj.weight to be bf16, got torch.float32Expected layers.2.mlp.w3.weight to be bf16, got torch.float32

Expected layers.4.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.3.sa_norm.scale to be bf16, got torch.float32
Expected layers.4.attn.k_proj.weight to be bf16, got torch.float32Expected layers.0.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.3.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.4.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.3.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.0.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.4.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.3.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.4.mlp_norm.scale to be bf16, got torch.float32Expected layers.0.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.3.attn.output_proj.weight to be bf16, got torch.float32

Expected tok_embeddings.weight to be bf16, got torch.float32Expected layers.4.mlp.w1.weight to be bf16, got torch.float32

Expected layers.0.mlp_norm.scale to be bf16, got torch.float32Expected layers.3.mlp_norm.scale to be bf16, got torch.float32

Expected layers.0.sa_norm.scale to be bf16, got torch.float32Expected layers.4.mlp.w2.weight to be bf16, got torch.float32

Expected layers.0.mlp.w1.weight to be bf16, got torch.float32Expected layers.3.mlp.w1.weight to be bf16, got torch.float32

Expected layers.0.attn.q_proj.weight to be bf16, got torch.float32Expected layers.4.mlp.w3.weight to be bf16, got torch.float32

Expected layers.0.mlp.w2.weight to be bf16, got torch.float32Expected layers.3.mlp.w2.weight to be bf16, got torch.float32

Expected layers.0.attn.k_proj.weight to be bf16, got torch.float32Expected layers.5.sa_norm.scale to be bf16, got torch.float32

Expected layers.0.mlp.w3.weight to be bf16, got torch.float32Expected layers.3.mlp.w3.weight to be bf16, got torch.float32

Expected layers.0.attn.v_proj.weight to be bf16, got torch.float32Expected layers.5.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.1.sa_norm.scale to be bf16, got torch.float32Expected layers.4.sa_norm.scale to be bf16, got torch.float32
Expected tok_embeddings.weight to be bf16, got torch.float32
Expected layers.0.attn.output_proj.weight to be bf16, got torch.float32Expected layers.5.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.1.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.4.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.0.sa_norm.scale to be bf16, got torch.float32Expected layers.0.mlp_norm.scale to be bf16, got torch.float32
Expected layers.5.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.1.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.4.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.0.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.0.mlp.w1.weight to be bf16, got torch.float32
Expected layers.5.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.1.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.4.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.0.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.0.mlp.w2.weight to be bf16, got torch.float32
Expected layers.5.mlp_norm.scale to be bf16, got torch.float32
Expected layers.1.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.4.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.0.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.0.mlp.w3.weight to be bf16, got torch.float32
Expected layers.5.mlp.w1.weight to be bf16, got torch.float32

Expected layers.1.mlp_norm.scale to be bf16, got torch.float32Expected layers.4.mlp_norm.scale to be bf16, got torch.float32
Expected layers.0.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.1.sa_norm.scale to be bf16, got torch.float32
Expected layers.5.mlp.w2.weight to be bf16, got torch.float32
Expected layers.1.mlp.w1.weight to be bf16, got torch.float32
Expected layers.4.mlp.w1.weight to be bf16, got torch.float32
Expected layers.0.mlp_norm.scale to be bf16, got torch.float32
Expected layers.1.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.5.mlp.w3.weight to be bf16, got torch.float32
Expected layers.1.mlp.w2.weight to be bf16, got torch.float32
Expected layers.4.mlp.w2.weight to be bf16, got torch.float32
Expected layers.0.mlp.w1.weight to be bf16, got torch.float32
Expected layers.1.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.6.sa_norm.scale to be bf16, got torch.float32

Expected layers.1.mlp.w3.weight to be bf16, got torch.float32Expected layers.4.mlp.w3.weight to be bf16, got torch.float32
Expected layers.0.mlp.w2.weight to be bf16, got torch.float32
Expected layers.1.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.6.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.2.sa_norm.scale to be bf16, got torch.float32Expected layers.5.sa_norm.scale to be bf16, got torch.float32
Expected layers.0.mlp.w3.weight to be bf16, got torch.float32
Expected layers.1.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.6.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.2.attn.q_proj.weight to be bf16, got torch.float32Expected layers.5.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.1.sa_norm.scale to be bf16, got torch.float32
Expected layers.1.mlp_norm.scale to be bf16, got torch.float32

Expected layers.6.attn.v_proj.weight to be bf16, got torch.float32Expected layers.2.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.5.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.1.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.1.mlp.w1.weight to be bf16, got torch.float32
Expected layers.6.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.2.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.5.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.1.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.1.mlp.w2.weight to be bf16, got torch.float32
Expected layers.6.mlp_norm.scale to be bf16, got torch.float32
Expected layers.2.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.5.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.1.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.1.mlp.w3.weight to be bf16, got torch.float32

Expected layers.6.mlp.w1.weight to be bf16, got torch.float32Expected layers.2.mlp_norm.scale to be bf16, got torch.float32
Expected layers.5.mlp_norm.scale to be bf16, got torch.float32

Expected layers.1.attn.output_proj.weight to be bf16, got torch.float32Expected layers.2.sa_norm.scale to be bf16, got torch.float32
Expected layers.6.mlp.w2.weight to be bf16, got torch.float32
Expected layers.2.mlp.w1.weight to be bf16, got torch.float32
Expected layers.5.mlp.w1.weight to be bf16, got torch.float32
Expected layers.2.attn.q_proj.weight to be bf16, got torch.float32Expected layers.1.mlp_norm.scale to be bf16, got torch.float32

Expected layers.6.mlp.w3.weight to be bf16, got torch.float32

Expected layers.2.mlp.w2.weight to be bf16, got torch.float32Expected layers.5.mlp.w2.weight to be bf16, got torch.float32
Expected layers.2.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.1.mlp.w1.weight to be bf16, got torch.float32
Expected layers.7.sa_norm.scale to be bf16, got torch.float32
Expected layers.2.mlp.w3.weight to be bf16, got torch.float32
Expected layers.5.mlp.w3.weight to be bf16, got torch.float32
Expected layers.2.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.1.mlp.w2.weight to be bf16, got torch.float32
Expected layers.7.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.3.sa_norm.scale to be bf16, got torch.float32
Expected layers.6.sa_norm.scale to be bf16, got torch.float32
Expected layers.2.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.1.mlp.w3.weight to be bf16, got torch.float32Expected layers.7.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.3.attn.q_proj.weight to be bf16, got torch.float32Expected layers.6.attn.q_proj.weight to be bf16, got torch.float32Expected layers.2.mlp_norm.scale to be bf16, got torch.float32


Expected layers.2.sa_norm.scale to be bf16, got torch.float32Expected layers.7.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.6.attn.k_proj.weight to be bf16, got torch.float32Expected layers.2.mlp.w1.weight to be bf16, got torch.float32

Expected layers.3.attn.k_proj.weight to be bf16, got torch.float32Expected layers.2.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.7.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.6.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.2.mlp.w2.weight to be bf16, got torch.float32
Expected layers.3.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.2.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.7.mlp_norm.scale to be bf16, got torch.float32
Expected layers.6.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.2.mlp.w3.weight to be bf16, got torch.float32
Expected layers.3.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.2.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.7.mlp.w1.weight to be bf16, got torch.float32
Expected layers.6.mlp_norm.scale to be bf16, got torch.float32
Expected layers.3.sa_norm.scale to be bf16, got torch.float32
Expected layers.3.mlp_norm.scale to be bf16, got torch.float32
Expected layers.2.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.7.mlp.w2.weight to be bf16, got torch.float32
Expected layers.6.mlp.w1.weight to be bf16, got torch.float32
Expected layers.3.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.3.mlp.w1.weight to be bf16, got torch.float32
Expected layers.2.mlp_norm.scale to be bf16, got torch.float32
Expected layers.7.mlp.w3.weight to be bf16, got torch.float32
Expected layers.6.mlp.w2.weight to be bf16, got torch.float32
Expected layers.3.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.3.mlp.w2.weight to be bf16, got torch.float32
Expected layers.2.mlp.w1.weight to be bf16, got torch.float32
Expected layers.8.sa_norm.scale to be bf16, got torch.float32
Expected layers.6.mlp.w3.weight to be bf16, got torch.float32
Expected layers.3.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.3.mlp.w3.weight to be bf16, got torch.float32

Expected layers.2.mlp.w2.weight to be bf16, got torch.float32Expected layers.8.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.7.sa_norm.scale to be bf16, got torch.float32Expected layers.4.sa_norm.scale to be bf16, got torch.float32
Expected layers.3.attn.output_proj.weight to be bf16, got torch.float32


Expected layers.2.mlp.w3.weight to be bf16, got torch.float32Expected layers.8.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.7.attn.q_proj.weight to be bf16, got torch.float32Expected layers.4.attn.q_proj.weight to be bf16, got torch.float32Expected layers.3.mlp_norm.scale to be bf16, got torch.float32


Expected layers.3.sa_norm.scale to be bf16, got torch.float32Expected layers.8.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.7.attn.k_proj.weight to be bf16, got torch.float32Expected layers.4.attn.k_proj.weight to be bf16, got torch.float32Expected layers.3.mlp.w1.weight to be bf16, got torch.float32


Expected layers.3.attn.q_proj.weight to be bf16, got torch.float32Expected layers.8.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.7.attn.v_proj.weight to be bf16, got torch.float32Expected layers.4.attn.v_proj.weight to be bf16, got torch.float32Expected layers.3.mlp.w2.weight to be bf16, got torch.float32


Expected layers.3.attn.k_proj.weight to be bf16, got torch.float32Expected layers.8.mlp_norm.scale to be bf16, got torch.float32

Expected layers.7.attn.output_proj.weight to be bf16, got torch.float32Expected layers.4.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.3.mlp.w3.weight to be bf16, got torch.float32

Expected layers.3.attn.v_proj.weight to be bf16, got torch.float32Expected layers.8.mlp.w1.weight to be bf16, got torch.float32
Expected layers.7.mlp_norm.scale to be bf16, got torch.float32
Expected tok_embeddings.weight to be bf16, got torch.float32Expected layers.4.mlp_norm.scale to be bf16, got torch.float32
Expected layers.4.sa_norm.scale to be bf16, got torch.float32
Expected layers.3.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.8.mlp.w2.weight to be bf16, got torch.float32

Expected layers.7.mlp.w1.weight to be bf16, got torch.float32
Expected layers.0.sa_norm.scale to be bf16, got torch.float32
Expected layers.4.mlp.w1.weight to be bf16, got torch.float32
Expected layers.4.attn.q_proj.weight to be bf16, got torch.float32Expected layers.3.mlp_norm.scale to be bf16, got torch.float32
Expected layers.8.mlp.w3.weight to be bf16, got torch.float32


Expected layers.7.mlp.w2.weight to be bf16, got torch.float32Expected layers.0.attn.q_proj.weight to be bf16, got torch.float32Expected layers.4.mlp.w2.weight to be bf16, got torch.float32

Expected layers.4.attn.k_proj.weight to be bf16, got torch.float32Expected layers.3.mlp.w1.weight to be bf16, got torch.float32Expected layers.9.sa_norm.scale to be bf16, got torch.float32



Expected layers.7.mlp.w3.weight to be bf16, got torch.float32Expected layers.0.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.4.mlp.w3.weight to be bf16, got torch.float32
Expected layers.4.attn.v_proj.weight to be bf16, got torch.float32Expected layers.3.mlp.w2.weight to be bf16, got torch.float32
Expected layers.9.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.8.sa_norm.scale to be bf16, got torch.float32
Expected layers.0.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.5.sa_norm.scale to be bf16, got torch.float32
Expected layers.4.attn.output_proj.weight to be bf16, got torch.float32Expected layers.3.mlp.w3.weight to be bf16, got torch.float32
Expected layers.9.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.8.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.0.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.5.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.4.mlp_norm.scale to be bf16, got torch.float32Expected layers.4.sa_norm.scale to be bf16, got torch.float32
Expected layers.9.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.8.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.0.mlp_norm.scale to be bf16, got torch.float32
Expected layers.5.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.4.mlp.w1.weight to be bf16, got torch.float32Expected layers.4.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.9.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.8.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.0.mlp.w1.weight to be bf16, got torch.float32
Expected layers.5.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.4.mlp.w2.weight to be bf16, got torch.float32Expected layers.4.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.9.mlp_norm.scale to be bf16, got torch.float32

Expected layers.8.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.0.mlp.w2.weight to be bf16, got torch.float32
Expected layers.5.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.4.mlp.w3.weight to be bf16, got torch.float32Expected layers.4.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.9.mlp.w1.weight to be bf16, got torch.float32

Expected layers.8.mlp_norm.scale to be bf16, got torch.float32
Expected layers.0.mlp.w3.weight to be bf16, got torch.float32
Expected layers.5.mlp_norm.scale to be bf16, got torch.float32
Expected layers.5.sa_norm.scale to be bf16, got torch.float32Expected layers.4.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.9.mlp.w2.weight to be bf16, got torch.float32

Expected layers.8.mlp.w1.weight to be bf16, got torch.float32
Expected layers.1.sa_norm.scale to be bf16, got torch.float32
Expected layers.5.mlp.w1.weight to be bf16, got torch.float32
Expected layers.5.attn.q_proj.weight to be bf16, got torch.float32Expected layers.4.mlp_norm.scale to be bf16, got torch.float32
Expected layers.9.mlp.w3.weight to be bf16, got torch.float32

Expected layers.8.mlp.w2.weight to be bf16, got torch.float32
Expected layers.1.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.5.mlp.w2.weight to be bf16, got torch.float32
Expected layers.5.attn.k_proj.weight to be bf16, got torch.float32Expected layers.4.mlp.w1.weight to be bf16, got torch.float32
Expected layers.10.sa_norm.scale to be bf16, got torch.float32

Expected layers.8.mlp.w3.weight to be bf16, got torch.float32
Expected layers.1.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.5.mlp.w3.weight to be bf16, got torch.float32
Expected layers.5.attn.v_proj.weight to be bf16, got torch.float32Expected layers.4.mlp.w2.weight to be bf16, got torch.float32
Expected layers.10.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.5.attn.output_proj.weight to be bf16, got torch.float32Expected layers.9.sa_norm.scale to be bf16, got torch.float32


Expected layers.4.mlp.w3.weight to be bf16, got torch.float32Expected layers.1.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.6.sa_norm.scale to be bf16, got torch.float32Expected layers.5.mlp_norm.scale to be bf16, got torch.float32
Expected layers.9.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.10.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.5.sa_norm.scale to be bf16, got torch.float32
Expected layers.1.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.6.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.5.mlp.w1.weight to be bf16, got torch.float32Expected layers.9.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.10.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.5.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.1.mlp_norm.scale to be bf16, got torch.float32

Expected layers.6.attn.k_proj.weight to be bf16, got torch.float32Expected layers.5.mlp.w2.weight to be bf16, got torch.float32Expected layers.9.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.10.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.5.attn.k_proj.weight to be bf16, got torch.float32Expected layers.1.mlp.w1.weight to be bf16, got torch.float32


Expected layers.6.attn.v_proj.weight to be bf16, got torch.float32Expected layers.5.mlp.w3.weight to be bf16, got torch.float32Expected layers.9.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.10.mlp_norm.scale to be bf16, got torch.float32
Expected layers.5.attn.v_proj.weight to be bf16, got torch.float32Expected layers.1.mlp.w2.weight to be bf16, got torch.float32

Expected layers.6.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.6.sa_norm.scale to be bf16, got torch.float32Expected layers.9.mlp_norm.scale to be bf16, got torch.float32

Expected layers.10.mlp.w1.weight to be bf16, got torch.float32
Expected layers.5.attn.output_proj.weight to be bf16, got torch.float32Expected layers.1.mlp.w3.weight to be bf16, got torch.float32


Expected layers.6.mlp_norm.scale to be bf16, got torch.float32Expected layers.6.attn.q_proj.weight to be bf16, got torch.float32Expected layers.9.mlp.w1.weight to be bf16, got torch.float32


Expected layers.10.mlp.w2.weight to be bf16, got torch.float32Expected layers.5.mlp_norm.scale to be bf16, got torch.float32Expected layers.2.sa_norm.scale to be bf16, got torch.float32


Expected layers.6.mlp.w1.weight to be bf16, got torch.float32Expected layers.6.attn.k_proj.weight to be bf16, got torch.float32Expected layers.9.mlp.w2.weight to be bf16, got torch.float32

Expected layers.10.mlp.w3.weight to be bf16, got torch.float32
Expected layers.5.mlp.w1.weight to be bf16, got torch.float32Expected layers.2.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.6.mlp.w2.weight to be bf16, got torch.float32
Expected layers.6.attn.v_proj.weight to be bf16, got torch.float32Expected layers.9.mlp.w3.weight to be bf16, got torch.float32


Expected layers.11.sa_norm.scale to be bf16, got torch.float32Expected layers.5.mlp.w2.weight to be bf16, got torch.float32Expected layers.2.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.6.mlp.w3.weight to be bf16, got torch.float32
Expected layers.6.attn.output_proj.weight to be bf16, got torch.float32Expected layers.10.sa_norm.scale to be bf16, got torch.float32


Expected layers.11.attn.q_proj.weight to be bf16, got torch.float32Expected layers.5.mlp.w3.weight to be bf16, got torch.float32Expected layers.2.attn.v_proj.weight to be bf16, got torch.float32


Expected layers.7.sa_norm.scale to be bf16, got torch.float32Expected layers.6.mlp_norm.scale to be bf16, got torch.float32Expected layers.10.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.11.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.6.sa_norm.scale to be bf16, got torch.float32Expected layers.2.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.7.attn.q_proj.weight to be bf16, got torch.float32Expected layers.6.mlp.w1.weight to be bf16, got torch.float32

Expected layers.10.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.11.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.6.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.2.mlp_norm.scale to be bf16, got torch.float32
Expected layers.7.attn.k_proj.weight to be bf16, got torch.float32Expected layers.6.mlp.w2.weight to be bf16, got torch.float32
Expected layers.10.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.11.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.6.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.7.attn.v_proj.weight to be bf16, got torch.float32Expected layers.2.mlp.w1.weight to be bf16, got torch.float32
Expected layers.6.mlp.w3.weight to be bf16, got torch.float32

Expected layers.10.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.11.mlp_norm.scale to be bf16, got torch.float32
Expected layers.6.attn.v_proj.weight to be bf16, got torch.float32Expected layers.2.mlp.w2.weight to be bf16, got torch.float32
Expected layers.7.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.7.sa_norm.scale to be bf16, got torch.float32
Expected layers.10.mlp_norm.scale to be bf16, got torch.float32
Expected layers.11.mlp.w1.weight to be bf16, got torch.float32
Expected layers.6.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.2.mlp.w3.weight to be bf16, got torch.float32Expected layers.7.mlp_norm.scale to be bf16, got torch.float32
Expected layers.7.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.10.mlp.w1.weight to be bf16, got torch.float32
Expected layers.11.mlp.w2.weight to be bf16, got torch.float32
Expected layers.6.mlp_norm.scale to be bf16, got torch.float32
Expected layers.3.sa_norm.scale to be bf16, got torch.float32Expected layers.7.mlp.w1.weight to be bf16, got torch.float32
Expected layers.7.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.10.mlp.w2.weight to be bf16, got torch.float32
Expected layers.11.mlp.w3.weight to be bf16, got torch.float32
Expected layers.6.mlp.w1.weight to be bf16, got torch.float32
Expected layers.3.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.7.mlp.w2.weight to be bf16, got torch.float32Expected layers.7.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.10.mlp.w3.weight to be bf16, got torch.float32

Expected layers.12.sa_norm.scale to be bf16, got torch.float32
Expected layers.6.mlp.w2.weight to be bf16, got torch.float32
Expected layers.3.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.7.mlp.w3.weight to be bf16, got torch.float32Expected layers.7.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.11.sa_norm.scale to be bf16, got torch.float32
Expected layers.12.attn.q_proj.weight to be bf16, got torch.float32Expected layers.6.mlp.w3.weight to be bf16, got torch.float32

Expected layers.3.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.8.sa_norm.scale to be bf16, got torch.float32Expected layers.7.mlp_norm.scale to be bf16, got torch.float32

Expected layers.11.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.12.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.7.sa_norm.scale to be bf16, got torch.float32
Expected layers.3.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.8.attn.q_proj.weight to be bf16, got torch.float32Expected layers.7.mlp.w1.weight to be bf16, got torch.float32
Expected layers.11.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.12.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.7.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.3.mlp_norm.scale to be bf16, got torch.float32
Expected layers.8.attn.k_proj.weight to be bf16, got torch.float32Expected layers.7.mlp.w2.weight to be bf16, got torch.float32
Expected layers.11.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.12.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.7.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.3.mlp.w1.weight to be bf16, got torch.float32
Expected layers.8.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.7.mlp.w3.weight to be bf16, got torch.float32
Expected layers.11.attn.output_proj.weight to be bf16, got torch.float32Expected layers.12.mlp_norm.scale to be bf16, got torch.float32
Expected layers.7.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.3.mlp.w2.weight to be bf16, got torch.float32
Expected layers.8.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.8.sa_norm.scale to be bf16, got torch.float32
Expected layers.11.mlp_norm.scale to be bf16, got torch.float32Expected layers.12.mlp.w1.weight to be bf16, got torch.float32
Expected layers.7.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.3.mlp.w3.weight to be bf16, got torch.float32
Expected layers.8.mlp_norm.scale to be bf16, got torch.float32
Expected layers.8.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.11.mlp.w1.weight to be bf16, got torch.float32
Expected layers.12.mlp.w2.weight to be bf16, got torch.float32
Expected layers.7.mlp_norm.scale to be bf16, got torch.float32
Expected layers.4.sa_norm.scale to be bf16, got torch.float32Expected layers.8.mlp.w1.weight to be bf16, got torch.float32
Expected layers.8.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.11.mlp.w2.weight to be bf16, got torch.float32
Expected layers.12.mlp.w3.weight to be bf16, got torch.float32
Expected layers.7.mlp.w1.weight to be bf16, got torch.float32
Expected layers.4.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.8.mlp.w2.weight to be bf16, got torch.float32Expected layers.8.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.11.mlp.w3.weight to be bf16, got torch.float32

Expected layers.13.sa_norm.scale to be bf16, got torch.float32
Expected layers.7.mlp.w2.weight to be bf16, got torch.float32
Expected layers.4.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.8.mlp.w3.weight to be bf16, got torch.float32Expected layers.8.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.12.sa_norm.scale to be bf16, got torch.float32

Expected layers.13.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.7.mlp.w3.weight to be bf16, got torch.float32
Expected layers.4.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.9.sa_norm.scale to be bf16, got torch.float32Expected layers.8.mlp_norm.scale to be bf16, got torch.float32
Expected layers.12.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.13.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.8.sa_norm.scale to be bf16, got torch.float32
Expected layers.4.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.9.attn.q_proj.weight to be bf16, got torch.float32Expected layers.8.mlp.w1.weight to be bf16, got torch.float32
Expected layers.12.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.13.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.8.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.4.mlp_norm.scale to be bf16, got torch.float32
Expected layers.9.attn.k_proj.weight to be bf16, got torch.float32Expected layers.8.mlp.w2.weight to be bf16, got torch.float32
Expected layers.12.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.13.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.8.attn.k_proj.weight to be bf16, got torch.float32Expected layers.9.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.4.mlp.w1.weight to be bf16, got torch.float32

Expected layers.8.mlp.w3.weight to be bf16, got torch.float32
Expected layers.12.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.13.mlp_norm.scale to be bf16, got torch.float32
Expected layers.8.attn.v_proj.weight to be bf16, got torch.float32Expected layers.9.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.4.mlp.w2.weight to be bf16, got torch.float32

Expected layers.9.sa_norm.scale to be bf16, got torch.float32Expected layers.12.mlp_norm.scale to be bf16, got torch.float32

Expected layers.13.mlp.w1.weight to be bf16, got torch.float32
Expected layers.8.attn.output_proj.weight to be bf16, got torch.float32Expected layers.9.mlp_norm.scale to be bf16, got torch.float32

Expected layers.4.mlp.w3.weight to be bf16, got torch.float32Expected layers.9.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.12.mlp.w1.weight to be bf16, got torch.float32

Expected layers.13.mlp.w2.weight to be bf16, got torch.float32
Expected layers.8.mlp_norm.scale to be bf16, got torch.float32
Expected layers.9.mlp.w1.weight to be bf16, got torch.float32
Expected layers.5.sa_norm.scale to be bf16, got torch.float32
Expected layers.9.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.12.mlp.w2.weight to be bf16, got torch.float32
Expected layers.13.mlp.w3.weight to be bf16, got torch.float32
Expected layers.8.mlp.w1.weight to be bf16, got torch.float32Expected layers.9.mlp.w2.weight to be bf16, got torch.float32

Expected layers.5.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.9.attn.v_proj.weight to be bf16, got torch.float32Expected layers.12.mlp.w3.weight to be bf16, got torch.float32

Expected layers.14.sa_norm.scale to be bf16, got torch.float32
Expected layers.8.mlp.w2.weight to be bf16, got torch.float32Expected layers.9.mlp.w3.weight to be bf16, got torch.float32

Expected layers.5.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.9.attn.output_proj.weight to be bf16, got torch.float32Expected layers.13.sa_norm.scale to be bf16, got torch.float32

Expected layers.14.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.8.mlp.w3.weight to be bf16, got torch.float32
Expected layers.10.sa_norm.scale to be bf16, got torch.float32
Expected layers.5.attn.v_proj.weight to be bf16, got torch.float32Expected layers.9.mlp_norm.scale to be bf16, got torch.float32
Expected layers.13.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.14.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.9.sa_norm.scale to be bf16, got torch.float32
Expected layers.10.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.5.attn.output_proj.weight to be bf16, got torch.float32Expected layers.9.mlp.w1.weight to be bf16, got torch.float32
Expected layers.13.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.14.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.9.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.10.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.5.mlp_norm.scale to be bf16, got torch.float32Expected layers.9.mlp.w2.weight to be bf16, got torch.float32
Expected layers.13.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.14.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.9.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.5.mlp.w1.weight to be bf16, got torch.float32Expected layers.10.attn.v_proj.weight to be bf16, got torch.float32Expected tok_embeddings.weight to be bf16, got torch.float32
Expected layers.9.mlp.w3.weight to be bf16, got torch.float32

Expected layers.13.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.14.mlp_norm.scale to be bf16, got torch.float32

Expected layers.9.attn.v_proj.weight to be bf16, got torch.float32Expected tok_embeddings.weight to be bf16, got torch.float32
Expected layers.5.mlp.w2.weight to be bf16, got torch.float32Expected layers.10.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.0.sa_norm.scale to be bf16, got torch.float32
Expected layers.10.sa_norm.scale to be bf16, got torch.float32Expected layers.13.mlp_norm.scale to be bf16, got torch.float32


Expected layers.14.mlp.w1.weight to be bf16, got torch.float32

Expected layers.5.mlp.w3.weight to be bf16, got torch.float32Expected layers.9.attn.output_proj.weight to be bf16, got torch.float32Expected layers.0.sa_norm.scale to be bf16, got torch.float32
Expected layers.10.mlp_norm.scale to be bf16, got torch.float32Expected layers.0.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.10.attn.q_proj.weight to be bf16, got torch.float32Expected layers.13.mlp.w1.weight to be bf16, got torch.float32


Expected layers.14.mlp.w2.weight to be bf16, got torch.float32

Expected layers.6.sa_norm.scale to be bf16, got torch.float32Expected layers.9.mlp_norm.scale to be bf16, got torch.float32
Expected layers.0.attn.q_proj.weight to be bf16, got torch.float32Expected layers.10.mlp.w1.weight to be bf16, got torch.float32
Expected layers.0.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.10.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.13.mlp.w2.weight to be bf16, got torch.float32

Expected layers.14.mlp.w3.weight to be bf16, got torch.float32
Expected layers.6.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.9.mlp.w1.weight to be bf16, got torch.float32
Expected layers.0.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.10.mlp.w2.weight to be bf16, got torch.float32Expected layers.0.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.10.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.13.mlp.w3.weight to be bf16, got torch.float32

Expected layers.15.sa_norm.scale to be bf16, got torch.float32
Expected layers.6.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.9.mlp.w2.weight to be bf16, got torch.float32
Expected layers.0.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.10.mlp.w3.weight to be bf16, got torch.float32Expected layers.0.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.10.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.14.sa_norm.scale to be bf16, got torch.float32

Expected layers.15.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.6.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.9.mlp.w3.weight to be bf16, got torch.float32
Expected layers.0.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.11.sa_norm.scale to be bf16, got torch.float32Expected layers.0.mlp_norm.scale to be bf16, got torch.float32
Expected layers.10.mlp_norm.scale to be bf16, got torch.float32
Expected layers.14.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.15.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.6.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.10.sa_norm.scale to be bf16, got torch.float32
Expected layers.0.mlp_norm.scale to be bf16, got torch.float32
Expected layers.11.attn.q_proj.weight to be bf16, got torch.float32Expected layers.0.mlp.w1.weight to be bf16, got torch.float32
Expected layers.10.mlp.w1.weight to be bf16, got torch.float32

Expected layers.14.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.15.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.6.mlp_norm.scale to be bf16, got torch.float32
Expected layers.10.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.0.mlp.w1.weight to be bf16, got torch.float32Expected layers.11.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.0.mlp.w2.weight to be bf16, got torch.float32
Expected layers.10.mlp.w2.weight to be bf16, got torch.float32

Expected layers.14.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.15.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.6.mlp.w1.weight to be bf16, got torch.float32
Expected layers.10.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.0.mlp.w2.weight to be bf16, got torch.float32Expected layers.11.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.0.mlp.w3.weight to be bf16, got torch.float32
Expected layers.10.mlp.w3.weight to be bf16, got torch.float32

Expected layers.14.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.15.mlp_norm.scale to be bf16, got torch.float32
Expected layers.6.mlp.w2.weight to be bf16, got torch.float32Expected layers.10.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.0.mlp.w3.weight to be bf16, got torch.float32Expected layers.11.attn.output_proj.weight to be bf16, got torch.float32


Expected layers.1.sa_norm.scale to be bf16, got torch.float32
Expected layers.11.sa_norm.scale to be bf16, got torch.float32
Expected layers.14.mlp_norm.scale to be bf16, got torch.float32

Expected layers.15.mlp.w1.weight to be bf16, got torch.float32Expected layers.6.mlp.w3.weight to be bf16, got torch.float32
Expected layers.10.attn.output_proj.weight to be bf16, got torch.float32Expected layers.1.sa_norm.scale to be bf16, got torch.float32

Expected layers.11.mlp_norm.scale to be bf16, got torch.float32Expected layers.1.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.11.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.14.mlp.w1.weight to be bf16, got torch.float32

Expected layers.15.mlp.w2.weight to be bf16, got torch.float32
Expected layers.7.sa_norm.scale to be bf16, got torch.float32
Expected layers.10.mlp_norm.scale to be bf16, got torch.float32
Expected layers.1.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.11.mlp.w1.weight to be bf16, got torch.float32Expected layers.1.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.11.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.14.mlp.w2.weight to be bf16, got torch.float32

Expected layers.15.mlp.w3.weight to be bf16, got torch.float32
Expected layers.7.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.10.mlp.w1.weight to be bf16, got torch.float32
Expected layers.1.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.1.attn.v_proj.weight to be bf16, got torch.float32Expected layers.11.mlp.w2.weight to be bf16, got torch.float32
Expected layers.11.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.14.mlp.w3.weight to be bf16, got torch.float32
Expected layers.16.sa_norm.scale to be bf16, got torch.float32
Expected layers.7.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.10.mlp.w2.weight to be bf16, got torch.float32
Expected layers.1.attn.v_proj.weight to be bf16, got torch.float32Expected layers.1.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.11.mlp.w3.weight to be bf16, got torch.float32
Expected layers.11.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.15.sa_norm.scale to be bf16, got torch.float32
Expected layers.16.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.7.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.10.mlp.w3.weight to be bf16, got torch.float32
Expected layers.1.attn.output_proj.weight to be bf16, got torch.float32Expected layers.1.mlp_norm.scale to be bf16, got torch.float32
Expected layers.12.sa_norm.scale to be bf16, got torch.float32
Expected layers.11.mlp_norm.scale to be bf16, got torch.float32

Expected layers.15.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.16.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.7.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.11.sa_norm.scale to be bf16, got torch.float32
Expected layers.1.mlp_norm.scale to be bf16, got torch.float32Expected layers.1.mlp.w1.weight to be bf16, got torch.float32
Expected layers.12.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.11.mlp.w1.weight to be bf16, got torch.float32

Expected layers.15.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.16.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.7.mlp_norm.scale to be bf16, got torch.float32
Expected layers.11.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.1.mlp.w1.weight to be bf16, got torch.float32Expected layers.1.mlp.w2.weight to be bf16, got torch.float32
Expected layers.12.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.11.mlp.w2.weight to be bf16, got torch.float32

Expected layers.15.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.16.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.7.mlp.w1.weight to be bf16, got torch.float32Expected layers.11.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.1.mlp.w2.weight to be bf16, got torch.float32Expected layers.1.mlp.w3.weight to be bf16, got torch.float32

Expected layers.12.attn.v_proj.weight to be bf16, got torch.float32Expected layers.11.mlp.w3.weight to be bf16, got torch.float32

Expected layers.15.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.16.mlp_norm.scale to be bf16, got torch.float32
Expected layers.7.mlp.w2.weight to be bf16, got torch.float32
Expected layers.11.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.1.mlp.w3.weight to be bf16, got torch.float32Expected layers.2.sa_norm.scale to be bf16, got torch.float32
Expected layers.12.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.12.sa_norm.scale to be bf16, got torch.float32

Expected layers.15.mlp_norm.scale to be bf16, got torch.float32
Expected layers.16.mlp.w1.weight to be bf16, got torch.float32
Expected layers.7.mlp.w3.weight to be bf16, got torch.float32
Expected layers.11.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.2.sa_norm.scale to be bf16, got torch.float32Expected layers.2.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.12.mlp_norm.scale to be bf16, got torch.float32
Expected layers.12.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.15.mlp.w1.weight to be bf16, got torch.float32
Expected layers.16.mlp.w2.weight to be bf16, got torch.float32
Expected layers.8.sa_norm.scale to be bf16, got torch.float32
Expected layers.11.mlp_norm.scale to be bf16, got torch.float32
Expected layers.2.attn.q_proj.weight to be bf16, got torch.float32Expected layers.2.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.12.mlp.w1.weight to be bf16, got torch.float32
Expected layers.12.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.15.mlp.w2.weight to be bf16, got torch.float32
Expected layers.16.mlp.w3.weight to be bf16, got torch.float32
Expected layers.8.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.11.mlp.w1.weight to be bf16, got torch.float32
Expected layers.2.attn.k_proj.weight to be bf16, got torch.float32Expected layers.2.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.12.mlp.w2.weight to be bf16, got torch.float32
Expected layers.12.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.15.mlp.w3.weight to be bf16, got torch.float32
Expected layers.17.sa_norm.scale to be bf16, got torch.float32
Expected layers.8.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.11.mlp.w2.weight to be bf16, got torch.float32
Expected layers.2.attn.v_proj.weight to be bf16, got torch.float32Expected layers.2.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.12.mlp.w3.weight to be bf16, got torch.float32
Expected layers.12.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.16.sa_norm.scale to be bf16, got torch.float32
Expected layers.17.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.8.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.11.mlp.w3.weight to be bf16, got torch.float32
Expected layers.2.attn.output_proj.weight to be bf16, got torch.float32Expected layers.2.mlp_norm.scale to be bf16, got torch.float32
Expected layers.13.sa_norm.scale to be bf16, got torch.float32
Expected layers.12.mlp_norm.scale to be bf16, got torch.float32

Expected layers.16.attn.q_proj.weight to be bf16, got torch.float32Expected layers.8.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.17.attn.k_proj.weight to be bf16, got torch.float32


Expected layers.12.sa_norm.scale to be bf16, got torch.float32Expected layers.2.mlp_norm.scale to be bf16, got torch.float32
Expected layers.2.mlp.w1.weight to be bf16, got torch.float32
Expected layers.13.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.12.mlp.w1.weight to be bf16, got torch.float32Expected layers.16.attn.k_proj.weight to be bf16, got torch.float32Expected layers.8.mlp_norm.scale to be bf16, got torch.float32

Expected layers.17.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.12.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.2.mlp.w1.weight to be bf16, got torch.float32

Expected layers.2.mlp.w2.weight to be bf16, got torch.float32Expected layers.13.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.12.mlp.w2.weight to be bf16, got torch.float32Expected layers.16.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.8.mlp.w1.weight to be bf16, got torch.float32
Expected layers.17.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.12.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.2.mlp.w2.weight to be bf16, got torch.float32
Expected layers.2.mlp.w3.weight to be bf16, got torch.float32
Expected layers.13.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.12.mlp.w3.weight to be bf16, got torch.float32Expected layers.16.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.8.mlp.w2.weight to be bf16, got torch.float32
Expected layers.17.mlp_norm.scale to be bf16, got torch.float32

Expected layers.12.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.2.mlp.w3.weight to be bf16, got torch.float32
Expected layers.3.sa_norm.scale to be bf16, got torch.float32
Expected layers.13.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.13.sa_norm.scale to be bf16, got torch.float32Expected layers.16.mlp_norm.scale to be bf16, got torch.float32
Expected layers.8.mlp.w3.weight to be bf16, got torch.float32
Expected layers.17.mlp.w1.weight to be bf16, got torch.float32

Expected layers.12.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.3.sa_norm.scale to be bf16, got torch.float32
Expected layers.3.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.13.mlp_norm.scale to be bf16, got torch.float32
Expected layers.13.attn.q_proj.weight to be bf16, got torch.float32Expected layers.16.mlp.w1.weight to be bf16, got torch.float32

Expected layers.9.sa_norm.scale to be bf16, got torch.float32
Expected layers.17.mlp.w2.weight to be bf16, got torch.float32
Expected layers.12.mlp_norm.scale to be bf16, got torch.float32Expected layers.13.mlp.w1.weight to be bf16, got torch.float32
Expected layers.3.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.3.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.13.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.16.mlp.w2.weight to be bf16, got torch.float32
Expected layers.9.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.17.mlp.w3.weight to be bf16, got torch.float32
Expected layers.12.mlp.w1.weight to be bf16, got torch.float32Expected layers.13.mlp.w2.weight to be bf16, got torch.float32

Expected layers.3.attn.k_proj.weight to be bf16, got torch.float32Expected layers.3.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.13.attn.v_proj.weight to be bf16, got torch.float32Expected layers.16.mlp.w3.weight to be bf16, got torch.float32

Expected layers.9.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.18.sa_norm.scale to be bf16, got torch.float32
Expected layers.12.mlp.w2.weight to be bf16, got torch.float32
Expected layers.13.mlp.w3.weight to be bf16, got torch.float32
Expected layers.3.attn.v_proj.weight to be bf16, got torch.float32Expected layers.3.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.13.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.17.sa_norm.scale to be bf16, got torch.float32

Expected layers.9.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.18.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.12.mlp.w3.weight to be bf16, got torch.float32
Expected layers.14.sa_norm.scale to be bf16, got torch.float32
Expected layers.3.attn.output_proj.weight to be bf16, got torch.float32Expected layers.3.mlp_norm.scale to be bf16, got torch.float32
Expected layers.13.mlp_norm.scale to be bf16, got torch.float32
Expected layers.17.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.9.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.18.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.13.sa_norm.scale to be bf16, got torch.float32Expected layers.14.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.3.mlp_norm.scale to be bf16, got torch.float32Expected layers.3.mlp.w1.weight to be bf16, got torch.float32
Expected layers.13.mlp.w1.weight to be bf16, got torch.float32
Expected layers.17.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.9.mlp_norm.scale to be bf16, got torch.float32
Expected layers.18.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.13.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.14.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.3.mlp.w1.weight to be bf16, got torch.float32Expected layers.3.mlp.w2.weight to be bf16, got torch.float32

Expected layers.13.mlp.w2.weight to be bf16, got torch.float32
Expected layers.17.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.9.mlp.w1.weight to be bf16, got torch.float32
Expected layers.18.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.13.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.14.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.3.mlp.w2.weight to be bf16, got torch.float32Expected layers.3.mlp.w3.weight to be bf16, got torch.float32

Expected layers.13.mlp.w3.weight to be bf16, got torch.float32Expected layers.17.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.9.mlp.w2.weight to be bf16, got torch.float32
Expected layers.18.mlp_norm.scale to be bf16, got torch.float32
Expected layers.13.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.14.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.3.mlp.w3.weight to be bf16, got torch.float32
Expected layers.4.sa_norm.scale to be bf16, got torch.float32
Expected layers.14.sa_norm.scale to be bf16, got torch.float32
Expected layers.17.mlp_norm.scale to be bf16, got torch.float32
Expected layers.9.mlp.w3.weight to be bf16, got torch.float32Expected layers.18.mlp.w1.weight to be bf16, got torch.float32
Expected layers.13.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.14.mlp_norm.scale to be bf16, got torch.float32

Expected layers.4.sa_norm.scale to be bf16, got torch.float32
Expected layers.4.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.14.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.17.mlp.w1.weight to be bf16, got torch.float32
Expected layers.10.sa_norm.scale to be bf16, got torch.float32Expected layers.18.mlp.w2.weight to be bf16, got torch.float32
Expected layers.13.mlp_norm.scale to be bf16, got torch.float32

Expected layers.14.mlp.w1.weight to be bf16, got torch.float32
Expected layers.14.attn.k_proj.weight to be bf16, got torch.float32Expected layers.4.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.4.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.17.mlp.w2.weight to be bf16, got torch.float32
Expected layers.10.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.18.mlp.w3.weight to be bf16, got torch.float32
Expected layers.13.mlp.w1.weight to be bf16, got torch.float32
Expected layers.14.mlp.w2.weight to be bf16, got torch.float32
Expected layers.14.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.4.attn.k_proj.weight to be bf16, got torch.float32Expected layers.4.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.17.mlp.w3.weight to be bf16, got torch.float32

Expected layers.10.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.19.sa_norm.scale to be bf16, got torch.float32
Expected layers.13.mlp.w2.weight to be bf16, got torch.float32
Expected layers.14.mlp.w3.weight to be bf16, got torch.float32Expected layers.14.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.4.attn.v_proj.weight to be bf16, got torch.float32Expected layers.4.attn.output_proj.weight to be bf16, got torch.float32


Expected layers.18.sa_norm.scale to be bf16, got torch.float32
Expected layers.10.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.19.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.13.mlp.w3.weight to be bf16, got torch.float32Expected layers.15.sa_norm.scale to be bf16, got torch.float32Expected layers.4.mlp_norm.scale to be bf16, got torch.float32Expected layers.14.mlp_norm.scale to be bf16, got torch.float32
Expected layers.4.attn.output_proj.weight to be bf16, got torch.float32


Expected layers.18.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.10.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.19.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.14.sa_norm.scale to be bf16, got torch.float32Expected layers.15.attn.q_proj.weight to be bf16, got torch.float32Expected layers.4.mlp.w1.weight to be bf16, got torch.float32
Expected layers.14.mlp.w1.weight to be bf16, got torch.float32
Expected layers.4.mlp_norm.scale to be bf16, got torch.float32
Expected layers.18.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.10.mlp_norm.scale to be bf16, got torch.float32

Expected layers.19.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.14.attn.q_proj.weight to be bf16, got torch.float32Expected layers.15.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.4.mlp.w2.weight to be bf16, got torch.float32Expected layers.14.mlp.w2.weight to be bf16, got torch.float32
Expected layers.4.mlp.w1.weight to be bf16, got torch.float32
Expected layers.18.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.10.mlp.w1.weight to be bf16, got torch.float32
Expected layers.15.attn.v_proj.weight to be bf16, got torch.float32Expected layers.19.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.14.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.4.mlp.w3.weight to be bf16, got torch.float32Expected layers.4.mlp.w2.weight to be bf16, got torch.float32
Expected layers.14.mlp.w3.weight to be bf16, got torch.float32

Expected layers.18.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.10.mlp.w2.weight to be bf16, got torch.float32Expected layers.15.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.19.mlp_norm.scale to be bf16, got torch.float32Expected layers.14.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.5.sa_norm.scale to be bf16, got torch.float32
Expected layers.4.mlp.w3.weight to be bf16, got torch.float32
Expected layers.15.sa_norm.scale to be bf16, got torch.float32

Expected layers.18.mlp_norm.scale to be bf16, got torch.float32
Expected layers.10.mlp.w3.weight to be bf16, got torch.float32
Expected layers.15.mlp_norm.scale to be bf16, got torch.float32
Expected layers.19.mlp.w1.weight to be bf16, got torch.float32
Expected layers.14.attn.output_proj.weight to be bf16, got torch.float32Expected layers.5.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.5.sa_norm.scale to be bf16, got torch.float32
Expected layers.15.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.18.mlp.w1.weight to be bf16, got torch.float32
Expected layers.11.sa_norm.scale to be bf16, got torch.float32
Expected layers.14.mlp_norm.scale to be bf16, got torch.float32Expected layers.15.mlp.w1.weight to be bf16, got torch.float32
Expected layers.19.mlp.w2.weight to be bf16, got torch.float32
Expected layers.5.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.5.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.15.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.18.mlp.w2.weight to be bf16, got torch.float32
Expected layers.11.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.14.mlp.w1.weight to be bf16, got torch.float32Expected layers.15.mlp.w2.weight to be bf16, got torch.float32
Expected layers.19.mlp.w3.weight to be bf16, got torch.float32
Expected layers.5.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.5.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.15.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.18.mlp.w3.weight to be bf16, got torch.float32Expected layers.11.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.14.mlp.w2.weight to be bf16, got torch.float32Expected layers.15.mlp.w3.weight to be bf16, got torch.float32
Expected layers.20.sa_norm.scale to be bf16, got torch.float32
Expected layers.5.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.5.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.15.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.19.sa_norm.scale to be bf16, got torch.float32Expected layers.11.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.14.mlp.w3.weight to be bf16, got torch.float32Expected layers.16.sa_norm.scale to be bf16, got torch.float32
Expected layers.20.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.5.mlp_norm.scale to be bf16, got torch.float32
Expected layers.5.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.15.mlp_norm.scale to be bf16, got torch.float32

Expected layers.19.attn.q_proj.weight to be bf16, got torch.float32Expected layers.11.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.15.sa_norm.scale to be bf16, got torch.float32Expected layers.16.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.20.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.5.mlp.w1.weight to be bf16, got torch.float32
Expected layers.5.mlp_norm.scale to be bf16, got torch.float32

Expected layers.15.mlp.w1.weight to be bf16, got torch.float32
Expected layers.19.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.11.mlp_norm.scale to be bf16, got torch.float32
Expected layers.15.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.16.attn.k_proj.weight to be bf16, got torch.float32Expected layers.20.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.5.mlp.w2.weight to be bf16, got torch.float32
Expected layers.5.mlp.w1.weight to be bf16, got torch.float32

Expected layers.15.mlp.w2.weight to be bf16, got torch.float32
Expected layers.19.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.11.mlp.w1.weight to be bf16, got torch.float32Expected layers.20.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.15.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.16.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.5.mlp.w3.weight to be bf16, got torch.float32
Expected layers.5.mlp.w2.weight to be bf16, got torch.float32
Expected layers.15.mlp.w3.weight to be bf16, got torch.float32
Expected layers.19.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.11.mlp.w2.weight to be bf16, got torch.float32Expected layers.20.mlp_norm.scale to be bf16, got torch.float32
Expected layers.15.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.16.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.6.sa_norm.scale to be bf16, got torch.float32
Expected layers.5.mlp.w3.weight to be bf16, got torch.float32
Expected layers.16.sa_norm.scale to be bf16, got torch.float32
Expected layers.19.mlp_norm.scale to be bf16, got torch.float32
Expected layers.11.mlp.w3.weight to be bf16, got torch.float32Expected layers.20.mlp.w1.weight to be bf16, got torch.float32
Expected layers.15.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.16.mlp_norm.scale to be bf16, got torch.float32
Expected layers.6.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.6.sa_norm.scale to be bf16, got torch.float32
Expected layers.16.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.19.mlp.w1.weight to be bf16, got torch.float32Expected layers.12.sa_norm.scale to be bf16, got torch.float32
Expected layers.20.mlp.w2.weight to be bf16, got torch.float32
Expected layers.15.mlp_norm.scale to be bf16, got torch.float32
Expected layers.16.mlp.w1.weight to be bf16, got torch.float32
Expected layers.6.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.6.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.16.attn.k_proj.weight to be bf16, got torch.float32Expected layers.19.mlp.w2.weight to be bf16, got torch.float32
Expected layers.12.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.20.mlp.w3.weight to be bf16, got torch.float32
Expected layers.15.mlp.w1.weight to be bf16, got torch.float32
Expected layers.16.mlp.w2.weight to be bf16, got torch.float32Expected layers.6.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.19.mlp.w3.weight to be bf16, got torch.float32
Expected layers.6.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.16.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.12.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.21.sa_norm.scale to be bf16, got torch.float32
Expected layers.15.mlp.w2.weight to be bf16, got torch.float32
Expected layers.16.mlp.w3.weight to be bf16, got torch.float32
Expected layers.6.attn.output_proj.weight to be bf16, got torch.float32Expected layers.20.sa_norm.scale to be bf16, got torch.float32

Expected layers.6.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.16.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.12.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.21.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.15.mlp.w3.weight to be bf16, got torch.float32
Expected layers.17.sa_norm.scale to be bf16, got torch.float32
Expected layers.6.mlp_norm.scale to be bf16, got torch.float32Expected layers.20.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.6.attn.output_proj.weight to be bf16, got torch.float32Expected layers.16.mlp_norm.scale to be bf16, got torch.float32

Expected layers.12.attn.output_proj.weight to be bf16, got torch.float32Expected layers.21.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.16.sa_norm.scale to be bf16, got torch.float32Expected layers.17.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.6.mlp.w1.weight to be bf16, got torch.float32Expected layers.20.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.6.mlp_norm.scale to be bf16, got torch.float32Expected layers.16.mlp.w1.weight to be bf16, got torch.float32

Expected layers.12.mlp_norm.scale to be bf16, got torch.float32Expected layers.21.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.16.attn.q_proj.weight to be bf16, got torch.float32Expected layers.17.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.6.mlp.w2.weight to be bf16, got torch.float32Expected layers.20.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.6.mlp.w1.weight to be bf16, got torch.float32Expected layers.16.mlp.w2.weight to be bf16, got torch.float32

Expected layers.12.mlp.w1.weight to be bf16, got torch.float32Expected layers.21.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.16.attn.k_proj.weight to be bf16, got torch.float32Expected layers.17.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.6.mlp.w3.weight to be bf16, got torch.float32Expected layers.20.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.6.mlp.w2.weight to be bf16, got torch.float32Expected layers.16.mlp.w3.weight to be bf16, got torch.float32

Expected layers.12.mlp.w2.weight to be bf16, got torch.float32Expected layers.21.mlp_norm.scale to be bf16, got torch.float32

Expected layers.16.attn.v_proj.weight to be bf16, got torch.float32Expected layers.17.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.7.sa_norm.scale to be bf16, got torch.float32Expected layers.20.mlp_norm.scale to be bf16, got torch.float32

Expected layers.6.mlp.w3.weight to be bf16, got torch.float32Expected layers.17.sa_norm.scale to be bf16, got torch.float32

Expected layers.12.mlp.w3.weight to be bf16, got torch.float32Expected layers.21.mlp.w1.weight to be bf16, got torch.float32

Expected layers.16.attn.output_proj.weight to be bf16, got torch.float32Expected layers.17.mlp_norm.scale to be bf16, got torch.float32

Expected layers.7.attn.q_proj.weight to be bf16, got torch.float32Expected layers.20.mlp.w1.weight to be bf16, got torch.float32

Expected layers.7.sa_norm.scale to be bf16, got torch.float32Expected layers.17.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.13.sa_norm.scale to be bf16, got torch.float32Expected layers.21.mlp.w2.weight to be bf16, got torch.float32

Expected layers.16.mlp_norm.scale to be bf16, got torch.float32Expected layers.17.mlp.w1.weight to be bf16, got torch.float32

Expected layers.7.attn.k_proj.weight to be bf16, got torch.float32Expected layers.20.mlp.w2.weight to be bf16, got torch.float32

Expected layers.7.attn.q_proj.weight to be bf16, got torch.float32Expected layers.17.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.13.attn.q_proj.weight to be bf16, got torch.float32Expected layers.21.mlp.w3.weight to be bf16, got torch.float32

Expected layers.16.mlp.w1.weight to be bf16, got torch.float32Expected layers.17.mlp.w2.weight to be bf16, got torch.float32

Expected layers.7.attn.v_proj.weight to be bf16, got torch.float32Expected layers.20.mlp.w3.weight to be bf16, got torch.float32

Expected layers.13.attn.k_proj.weight to be bf16, got torch.float32Expected layers.22.sa_norm.scale to be bf16, got torch.float32Expected layers.7.attn.k_proj.weight to be bf16, got torch.float32Expected layers.17.attn.v_proj.weight to be bf16, got torch.float32




Expected layers.16.mlp.w2.weight to be bf16, got torch.float32
Expected layers.17.mlp.w3.weight to be bf16, got torch.float32Expected layers.7.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.21.sa_norm.scale to be bf16, got torch.float32Expected layers.13.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.22.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.7.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.17.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.7.mlp_norm.scale to be bf16, got torch.float32Expected layers.16.mlp.w3.weight to be bf16, got torch.float32

Expected layers.18.sa_norm.scale to be bf16, got torch.float32
Expected layers.21.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.13.attn.output_proj.weight to be bf16, got torch.float32Expected layers.22.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.7.attn.output_proj.weight to be bf16, got torch.float32Expected layers.17.mlp_norm.scale to be bf16, got torch.float32

Expected layers.7.mlp.w1.weight to be bf16, got torch.float32Expected layers.17.sa_norm.scale to be bf16, got torch.float32

Expected layers.18.attn.q_proj.weight to be bf16, got torch.float32Expected layers.21.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.13.mlp_norm.scale to be bf16, got torch.float32Expected layers.22.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.7.mlp_norm.scale to be bf16, got torch.float32Expected layers.17.mlp.w1.weight to be bf16, got torch.float32

Expected layers.7.mlp.w2.weight to be bf16, got torch.float32Expected layers.17.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.18.attn.k_proj.weight to be bf16, got torch.float32Expected layers.21.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.13.mlp.w1.weight to be bf16, got torch.float32Expected layers.22.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.7.mlp.w1.weight to be bf16, got torch.float32
Expected layers.17.mlp.w2.weight to be bf16, got torch.float32
Expected layers.7.mlp.w3.weight to be bf16, got torch.float32Expected layers.17.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.18.attn.v_proj.weight to be bf16, got torch.float32Expected layers.21.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.13.mlp.w2.weight to be bf16, got torch.float32
Expected layers.22.mlp_norm.scale to be bf16, got torch.float32

Expected layers.7.mlp.w2.weight to be bf16, got torch.float32
Expected layers.17.mlp.w3.weight to be bf16, got torch.float32
Expected layers.8.sa_norm.scale to be bf16, got torch.float32
Expected layers.17.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.18.attn.output_proj.weight to be bf16, got torch.float32Expected layers.21.mlp_norm.scale to be bf16, got torch.float32
Expected layers.13.mlp.w3.weight to be bf16, got torch.float32
Expected layers.22.mlp.w1.weight to be bf16, got torch.float32

Expected layers.7.mlp.w3.weight to be bf16, got torch.float32

Expected layers.8.attn.q_proj.weight to be bf16, got torch.float32Expected layers.18.sa_norm.scale to be bf16, got torch.float32
Expected layers.17.attn.output_proj.weight to be bf16, got torch.float32Expected layers.18.mlp_norm.scale to be bf16, got torch.float32
Expected layers.21.mlp.w1.weight to be bf16, got torch.float32
Expected layers.14.sa_norm.scale to be bf16, got torch.float32Expected layers.22.mlp.w2.weight to be bf16, got torch.float32


Expected layers.8.sa_norm.scale to be bf16, got torch.float32

Expected layers.8.attn.k_proj.weight to be bf16, got torch.float32Expected layers.18.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.17.mlp_norm.scale to be bf16, got torch.float32Expected layers.18.mlp.w1.weight to be bf16, got torch.float32
Expected layers.21.mlp.w2.weight to be bf16, got torch.float32
Expected layers.14.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.22.mlp.w3.weight to be bf16, got torch.float32

Expected layers.8.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.8.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.18.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.17.mlp.w1.weight to be bf16, got torch.float32
Expected layers.18.mlp.w2.weight to be bf16, got torch.float32Expected layers.21.mlp.w3.weight to be bf16, got torch.float32
Expected layers.8.attn.k_proj.weight to be bf16, got torch.float32Expected layers.14.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.23.sa_norm.scale to be bf16, got torch.float32


Expected layers.8.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.18.attn.v_proj.weight to be bf16, got torch.float32Expected layers.17.mlp.w2.weight to be bf16, got torch.float32
Expected layers.18.mlp.w3.weight to be bf16, got torch.float32
Expected layers.22.sa_norm.scale to be bf16, got torch.float32
Expected layers.8.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.14.attn.v_proj.weight to be bf16, got torch.float32Expected layers.23.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.8.mlp_norm.scale to be bf16, got torch.float32

Expected layers.18.attn.output_proj.weight to be bf16, got torch.float32Expected layers.17.mlp.w3.weight to be bf16, got torch.float32
Expected layers.19.sa_norm.scale to be bf16, got torch.float32
Expected layers.22.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.8.attn.output_proj.weight to be bf16, got torch.float32Expected layers.14.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.23.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.8.mlp.w1.weight to be bf16, got torch.float32
Expected layers.18.mlp_norm.scale to be bf16, got torch.float32
Expected layers.18.sa_norm.scale to be bf16, got torch.float32
Expected layers.19.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.22.attn.k_proj.weight to be bf16, got torch.float32Expected layers.8.mlp_norm.scale to be bf16, got torch.float32
Expected layers.14.mlp_norm.scale to be bf16, got torch.float32
Expected layers.23.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.8.mlp.w2.weight to be bf16, got torch.float32
Expected layers.18.mlp.w1.weight to be bf16, got torch.float32
Expected layers.18.attn.q_proj.weight to be bf16, got torch.float32Expected layers.19.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.22.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.8.mlp.w1.weight to be bf16, got torch.float32

Expected layers.14.mlp.w1.weight to be bf16, got torch.float32
Expected layers.23.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.8.mlp.w3.weight to be bf16, got torch.float32

Expected layers.18.mlp.w2.weight to be bf16, got torch.float32Expected layers.18.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.19.attn.v_proj.weight to be bf16, got torch.float32Expected layers.22.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.8.mlp.w2.weight to be bf16, got torch.float32
Expected layers.14.mlp.w2.weight to be bf16, got torch.float32
Expected layers.23.mlp_norm.scale to be bf16, got torch.float32

Expected layers.9.sa_norm.scale to be bf16, got torch.float32

Expected layers.18.mlp.w3.weight to be bf16, got torch.float32Expected layers.18.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.19.attn.output_proj.weight to be bf16, got torch.float32Expected layers.22.mlp_norm.scale to be bf16, got torch.float32

Expected layers.8.mlp.w3.weight to be bf16, got torch.float32Expected layers.14.mlp.w3.weight to be bf16, got torch.float32
Expected layers.23.mlp.w1.weight to be bf16, got torch.float32
Expected layers.9.attn.q_proj.weight to be bf16, got torch.float32


Expected layers.19.sa_norm.scale to be bf16, got torch.float32Expected layers.18.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.19.mlp_norm.scale to be bf16, got torch.float32Expected layers.22.mlp.w1.weight to be bf16, got torch.float32

Expected layers.9.sa_norm.scale to be bf16, got torch.float32
Expected layers.15.sa_norm.scale to be bf16, got torch.float32
Expected layers.23.mlp.w2.weight to be bf16, got torch.float32
Expected layers.9.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.18.mlp_norm.scale to be bf16, got torch.float32
Expected layers.19.attn.q_proj.weight to be bf16, got torch.float32Expected layers.19.mlp.w1.weight to be bf16, got torch.float32
Expected layers.22.mlp.w2.weight to be bf16, got torch.float32

Expected layers.9.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.15.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.23.mlp.w3.weight to be bf16, got torch.float32
Expected layers.9.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.18.mlp.w1.weight to be bf16, got torch.float32Expected layers.19.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.19.mlp.w2.weight to be bf16, got torch.float32
Expected layers.22.mlp.w3.weight to be bf16, got torch.float32

Expected layers.9.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.15.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.24.sa_norm.scale to be bf16, got torch.float32
Expected layers.9.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.18.mlp.w2.weight to be bf16, got torch.float32
Expected layers.19.attn.v_proj.weight to be bf16, got torch.float32Expected layers.19.mlp.w3.weight to be bf16, got torch.float32
Expected layers.23.sa_norm.scale to be bf16, got torch.float32
Expected layers.9.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.15.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.24.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.9.mlp_norm.scale to be bf16, got torch.float32
Expected layers.18.mlp.w3.weight to be bf16, got torch.float32
Expected layers.20.sa_norm.scale to be bf16, got torch.float32Expected layers.19.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.23.attn.q_proj.weight to be bf16, got torch.float32Expected layers.9.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.15.attn.output_proj.weight to be bf16, got torch.float32Expected layers.24.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.9.mlp.w1.weight to be bf16, got torch.float32Expected layers.19.sa_norm.scale to be bf16, got torch.float32

Expected layers.20.attn.q_proj.weight to be bf16, got torch.float32Expected layers.19.mlp_norm.scale to be bf16, got torch.float32

Expected layers.23.attn.k_proj.weight to be bf16, got torch.float32Expected layers.9.mlp_norm.scale to be bf16, got torch.float32

Expected layers.15.mlp_norm.scale to be bf16, got torch.float32Expected layers.24.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.9.mlp.w2.weight to be bf16, got torch.float32Expected layers.19.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.20.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.19.mlp.w1.weight to be bf16, got torch.float32
Expected layers.23.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.9.mlp.w1.weight to be bf16, got torch.float32
Expected layers.15.mlp.w1.weight to be bf16, got torch.float32Expected layers.19.attn.k_proj.weight to be bf16, got torch.float32Expected layers.24.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.9.mlp.w3.weight to be bf16, got torch.float32


Expected layers.20.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.19.mlp.w2.weight to be bf16, got torch.float32
Expected layers.23.attn.output_proj.weight to be bf16, got torch.float32Expected layers.9.mlp.w2.weight to be bf16, got torch.float32Expected layers.15.mlp.w2.weight to be bf16, got torch.float32
Expected layers.19.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.24.mlp_norm.scale to be bf16, got torch.float32
Expected layers.10.sa_norm.scale to be bf16, got torch.float32
Expected layers.20.attn.output_proj.weight to be bf16, got torch.float32


Expected layers.19.mlp.w3.weight to be bf16, got torch.float32Expected layers.23.mlp_norm.scale to be bf16, got torch.float32
Expected layers.9.mlp.w3.weight to be bf16, got torch.float32Expected layers.15.mlp.w3.weight to be bf16, got torch.float32

Expected layers.19.attn.output_proj.weight to be bf16, got torch.float32Expected layers.24.mlp.w1.weight to be bf16, got torch.float32

Expected layers.10.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.20.mlp_norm.scale to be bf16, got torch.float32
Expected layers.23.mlp.w1.weight to be bf16, got torch.float32
Expected layers.20.sa_norm.scale to be bf16, got torch.float32
Expected layers.10.sa_norm.scale to be bf16, got torch.float32
Expected layers.16.sa_norm.scale to be bf16, got torch.float32Expected layers.19.mlp_norm.scale to be bf16, got torch.float32
Expected layers.24.mlp.w2.weight to be bf16, got torch.float32
Expected layers.10.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.20.mlp.w1.weight to be bf16, got torch.float32
Expected layers.23.mlp.w2.weight to be bf16, got torch.float32
Expected layers.20.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.10.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.16.attn.q_proj.weight to be bf16, got torch.float32Expected layers.19.mlp.w1.weight to be bf16, got torch.float32
Expected layers.24.mlp.w3.weight to be bf16, got torch.float32


Expected layers.20.mlp.w2.weight to be bf16, got torch.float32Expected layers.10.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.23.mlp.w3.weight to be bf16, got torch.float32
Expected layers.20.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.10.attn.k_proj.weight to be bf16, got torch.float32Expected layers.16.attn.k_proj.weight to be bf16, got torch.float32Expected layers.19.mlp.w2.weight to be bf16, got torch.float32

Expected layers.25.sa_norm.scale to be bf16, got torch.float32

Expected layers.20.mlp.w3.weight to be bf16, got torch.float32


Expected layers.24.sa_norm.scale to be bf16, got torch.float32Expected layers.20.attn.v_proj.weight to be bf16, got torch.float32Expected layers.10.attn.output_proj.weight to be bf16, got torch.float32Expected layers.10.attn.v_proj.weight to be bf16, got torch.float32Expected layers.16.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.25.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.19.mlp.w3.weight to be bf16, got torch.float32Expected layers.21.sa_norm.scale to be bf16, got torch.float32



Expected layers.24.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.20.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.10.attn.output_proj.weight to be bf16, got torch.float32Expected layers.10.mlp_norm.scale to be bf16, got torch.float32Expected layers.16.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.25.attn.k_proj.weight to be bf16, got torch.float32Expected layers.20.sa_norm.scale to be bf16, got torch.float32

Expected layers.21.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.24.attn.k_proj.weight to be bf16, got torch.float32


Expected layers.20.mlp_norm.scale to be bf16, got torch.float32
Expected layers.10.mlp_norm.scale to be bf16, got torch.float32Expected layers.16.mlp_norm.scale to be bf16, got torch.float32
Expected layers.10.mlp.w1.weight to be bf16, got torch.float32Expected layers.25.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.20.attn.q_proj.weight to be bf16, got torch.float32Expected layers.21.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.24.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.20.mlp.w1.weight to be bf16, got torch.float32

Expected layers.10.mlp.w1.weight to be bf16, got torch.float32
Expected layers.16.mlp.w1.weight to be bf16, got torch.float32

Expected layers.25.attn.output_proj.weight to be bf16, got torch.float32Expected layers.10.mlp.w2.weight to be bf16, got torch.float32
Expected layers.20.attn.k_proj.weight to be bf16, got torch.float32Expected layers.21.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.24.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.20.mlp.w2.weight to be bf16, got torch.float32Expected layers.10.mlp.w2.weight to be bf16, got torch.float32

Expected layers.16.mlp.w2.weight to be bf16, got torch.float32
Expected layers.25.mlp_norm.scale to be bf16, got torch.float32
Expected layers.20.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.10.mlp.w3.weight to be bf16, got torch.float32
Expected layers.21.attn.output_proj.weight to be bf16, got torch.float32Expected layers.24.mlp_norm.scale to be bf16, got torch.float32
Expected layers.20.mlp.w3.weight to be bf16, got torch.float32
Expected layers.10.mlp.w3.weight to be bf16, got torch.float32

Expected layers.16.mlp.w3.weight to be bf16, got torch.float32
Expected layers.25.mlp.w1.weight to be bf16, got torch.float32
Expected layers.20.attn.output_proj.weight to be bf16, got torch.float32Expected layers.24.mlp.w1.weight to be bf16, got torch.float32
Expected layers.11.sa_norm.scale to be bf16, got torch.float32
Expected layers.21.mlp_norm.scale to be bf16, got torch.float32


Expected layers.21.sa_norm.scale to be bf16, got torch.float32Expected layers.11.sa_norm.scale to be bf16, got torch.float32
Expected layers.17.sa_norm.scale to be bf16, got torch.float32Expected layers.25.mlp.w2.weight to be bf16, got torch.float32

Expected layers.20.mlp_norm.scale to be bf16, got torch.float32Expected layers.24.mlp.w2.weight to be bf16, got torch.float32
Expected layers.11.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.21.mlp.w1.weight to be bf16, got torch.float32


Expected layers.11.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.21.attn.q_proj.weight to be bf16, got torch.float32Expected layers.17.attn.q_proj.weight to be bf16, got torch.float32Expected layers.25.mlp.w3.weight to be bf16, got torch.float32
Expected layers.20.mlp.w1.weight to be bf16, got torch.float32
Expected layers.24.mlp.w3.weight to be bf16, got torch.float32
Expected layers.11.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.21.mlp.w2.weight to be bf16, got torch.float32


Expected layers.11.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.21.attn.k_proj.weight to be bf16, got torch.float32Expected layers.17.attn.k_proj.weight to be bf16, got torch.float32Expected layers.26.sa_norm.scale to be bf16, got torch.float32
Expected layers.20.mlp.w2.weight to be bf16, got torch.float32Expected layers.25.sa_norm.scale to be bf16, got torch.float32
Expected layers.11.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.21.mlp.w3.weight to be bf16, got torch.float32


Expected layers.11.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.21.attn.v_proj.weight to be bf16, got torch.float32Expected layers.17.attn.v_proj.weight to be bf16, got torch.float32Expected layers.26.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.20.mlp.w3.weight to be bf16, got torch.float32
Expected layers.25.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.11.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.22.sa_norm.scale to be bf16, got torch.float32


Expected layers.11.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.21.attn.output_proj.weight to be bf16, got torch.float32Expected layers.17.attn.output_proj.weight to be bf16, got torch.float32Expected layers.26.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.21.sa_norm.scale to be bf16, got torch.float32
Expected layers.25.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.11.mlp_norm.scale to be bf16, got torch.float32

Expected layers.22.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.11.mlp_norm.scale to be bf16, got torch.float32
Expected layers.21.mlp_norm.scale to be bf16, got torch.float32
Expected layers.17.mlp_norm.scale to be bf16, got torch.float32
Expected layers.26.attn.v_proj.weight to be bf16, got torch.float32Expected layers.21.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.25.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.11.mlp.w1.weight to be bf16, got torch.float32

Expected layers.22.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.11.mlp.w1.weight to be bf16, got torch.float32
Expected layers.21.mlp.w1.weight to be bf16, got torch.float32
Expected layers.17.mlp.w1.weight to be bf16, got torch.float32
Expected layers.26.attn.output_proj.weight to be bf16, got torch.float32Expected layers.21.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.25.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.11.mlp.w2.weight to be bf16, got torch.float32
Expected layers.22.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.11.mlp.w2.weight to be bf16, got torch.float32
Expected layers.21.mlp.w2.weight to be bf16, got torch.float32
Expected layers.17.mlp.w2.weight to be bf16, got torch.float32Expected layers.26.mlp_norm.scale to be bf16, got torch.float32
Expected layers.21.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.25.mlp_norm.scale to be bf16, got torch.float32

Expected layers.11.mlp.w3.weight to be bf16, got torch.float32
Expected layers.22.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.11.mlp.w3.weight to be bf16, got torch.float32

Expected layers.21.mlp.w3.weight to be bf16, got torch.float32Expected layers.17.mlp.w3.weight to be bf16, got torch.float32Expected layers.26.mlp.w1.weight to be bf16, got torch.float32
Expected layers.21.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.25.mlp.w1.weight to be bf16, got torch.float32

Expected layers.12.sa_norm.scale to be bf16, got torch.float32Expected layers.22.mlp_norm.scale to be bf16, got torch.float32

Expected layers.12.sa_norm.scale to be bf16, got torch.float32

Expected layers.22.sa_norm.scale to be bf16, got torch.float32Expected layers.18.sa_norm.scale to be bf16, got torch.float32Expected layers.26.mlp.w2.weight to be bf16, got torch.float32

Expected layers.25.mlp.w2.weight to be bf16, got torch.float32
Expected layers.21.mlp_norm.scale to be bf16, got torch.float32
Expected layers.12.attn.q_proj.weight to be bf16, got torch.float32Expected layers.22.mlp.w1.weight to be bf16, got torch.float32

Expected layers.12.attn.q_proj.weight to be bf16, got torch.float32Expected layers.22.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.18.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.26.mlp.w3.weight to be bf16, got torch.float32


Expected layers.25.mlp.w3.weight to be bf16, got torch.float32Expected layers.21.mlp.w1.weight to be bf16, got torch.float32
Expected layers.12.attn.k_proj.weight to be bf16, got torch.float32Expected layers.22.mlp.w2.weight to be bf16, got torch.float32

Expected layers.12.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.22.attn.k_proj.weight to be bf16, got torch.float32Expected layers.18.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.27.sa_norm.scale to be bf16, got torch.float32

Expected layers.26.sa_norm.scale to be bf16, got torch.float32
Expected layers.21.mlp.w2.weight to be bf16, got torch.float32
Expected layers.12.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.22.mlp.w3.weight to be bf16, got torch.float32
Expected layers.12.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.22.attn.v_proj.weight to be bf16, got torch.float32Expected layers.18.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.27.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.26.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.21.mlp.w3.weight to be bf16, got torch.float32
Expected layers.12.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.23.sa_norm.scale to be bf16, got torch.float32
Expected layers.12.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.22.attn.output_proj.weight to be bf16, got torch.float32Expected layers.18.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.27.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.26.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.22.sa_norm.scale to be bf16, got torch.float32
Expected layers.12.mlp_norm.scale to be bf16, got torch.float32
Expected layers.23.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.12.mlp_norm.scale to be bf16, got torch.float32
Expected layers.22.mlp_norm.scale to be bf16, got torch.float32Expected layers.18.mlp_norm.scale to be bf16, got torch.float32
Expected layers.27.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.26.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.23.attn.k_proj.weight to be bf16, got torch.float32Expected layers.22.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.12.mlp.w1.weight to be bf16, got torch.float32


Expected layers.12.mlp.w1.weight to be bf16, got torch.float32
Expected layers.22.mlp.w1.weight to be bf16, got torch.float32Expected layers.18.mlp.w1.weight to be bf16, got torch.float32
Expected layers.27.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.26.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.23.attn.v_proj.weight to be bf16, got torch.float32Expected layers.22.attn.k_proj.weight to be bf16, got torch.float32Expected layers.12.mlp.w2.weight to be bf16, got torch.float32

Expected layers.18.mlp.w2.weight to be bf16, got torch.float32

Expected layers.12.mlp.w2.weight to be bf16, got torch.float32
Expected layers.22.mlp.w2.weight to be bf16, got torch.float32

Expected layers.27.mlp_norm.scale to be bf16, got torch.float32Expected layers.26.mlp_norm.scale to be bf16, got torch.float32
Expected layers.23.attn.output_proj.weight to be bf16, got torch.float32Expected layers.22.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.12.mlp.w3.weight to be bf16, got torch.float32

Expected layers.18.mlp.w3.weight to be bf16, got torch.float32Expected layers.12.mlp.w3.weight to be bf16, got torch.float32

Expected layers.22.mlp.w3.weight to be bf16, got torch.float32
Expected layers.27.mlp.w1.weight to be bf16, got torch.float32
Expected layers.26.mlp.w1.weight to be bf16, got torch.float32
Expected layers.23.mlp_norm.scale to be bf16, got torch.float32
Expected layers.22.attn.output_proj.weight to be bf16, got torch.float32Expected layers.13.sa_norm.scale to be bf16, got torch.float32
Expected layers.19.sa_norm.scale to be bf16, got torch.float32
Expected layers.13.sa_norm.scale to be bf16, got torch.float32
Expected layers.23.sa_norm.scale to be bf16, got torch.float32
Expected layers.27.mlp.w2.weight to be bf16, got torch.float32

Expected layers.26.mlp.w2.weight to be bf16, got torch.float32
Expected layers.23.mlp.w1.weight to be bf16, got torch.float32
Expected layers.22.mlp_norm.scale to be bf16, got torch.float32Expected layers.13.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.13.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.19.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.23.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.27.mlp.w3.weight to be bf16, got torch.float32
Expected layers.26.mlp.w3.weight to be bf16, got torch.float32
Expected layers.23.mlp.w2.weight to be bf16, got torch.float32
Expected layers.22.mlp.w1.weight to be bf16, got torch.float32Expected layers.13.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.13.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.19.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.23.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.28.sa_norm.scale to be bf16, got torch.float32
Expected layers.27.sa_norm.scale to be bf16, got torch.float32
Expected layers.23.mlp.w3.weight to be bf16, got torch.float32
Expected layers.22.mlp.w2.weight to be bf16, got torch.float32Expected layers.13.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.13.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.19.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.23.attn.v_proj.weight to be bf16, got torch.float32Expected layers.28.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.22.mlp.w3.weight to be bf16, got torch.float32
Expected layers.27.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.24.sa_norm.scale to be bf16, got torch.float32

Expected layers.13.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.13.attn.output_proj.weight to be bf16, got torch.float32Expected layers.19.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.23.attn.output_proj.weight to be bf16, got torch.float32Expected layers.28.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.23.sa_norm.scale to be bf16, got torch.float32
Expected layers.27.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.24.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.13.mlp_norm.scale to be bf16, got torch.float32

Expected layers.13.mlp_norm.scale to be bf16, got torch.float32Expected layers.19.mlp_norm.scale to be bf16, got torch.float32
Expected layers.23.mlp_norm.scale to be bf16, got torch.float32Expected layers.28.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.23.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.27.attn.v_proj.weight to be bf16, got torch.float32Expected layers.24.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.13.mlp.w1.weight to be bf16, got torch.float32

Expected layers.13.mlp.w1.weight to be bf16, got torch.float32Expected layers.19.mlp.w1.weight to be bf16, got torch.float32Expected layers.23.mlp.w1.weight to be bf16, got torch.float32
Expected layers.28.attn.output_proj.weight to be bf16, got torch.float32


Expected layers.27.attn.output_proj.weight to be bf16, got torch.float32Expected layers.23.attn.k_proj.weight to be bf16, got torch.float32Expected layers.24.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.13.mlp.w2.weight to be bf16, got torch.float32
Expected layers.13.mlp.w2.weight to be bf16, got torch.float32
Expected layers.19.mlp.w2.weight to be bf16, got torch.float32Expected layers.23.mlp.w2.weight to be bf16, got torch.float32
Expected layers.28.mlp_norm.scale to be bf16, got torch.float32
Expected layers.27.mlp_norm.scale to be bf16, got torch.float32

Expected layers.23.attn.v_proj.weight to be bf16, got torch.float32Expected layers.24.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.13.mlp.w3.weight to be bf16, got torch.float32
Expected layers.13.mlp.w3.weight to be bf16, got torch.float32
Expected layers.19.mlp.w3.weight to be bf16, got torch.float32Expected layers.23.mlp.w3.weight to be bf16, got torch.float32

Expected layers.28.mlp.w1.weight to be bf16, got torch.float32Expected layers.27.mlp.w1.weight to be bf16, got torch.float32

Expected layers.23.attn.output_proj.weight to be bf16, got torch.float32Expected layers.24.mlp_norm.scale to be bf16, got torch.float32

Expected layers.14.sa_norm.scale to be bf16, got torch.float32Expected layers.14.sa_norm.scale to be bf16, got torch.float32

Expected layers.20.sa_norm.scale to be bf16, got torch.float32Expected layers.24.sa_norm.scale to be bf16, got torch.float32

Expected layers.28.mlp.w2.weight to be bf16, got torch.float32
Expected layers.27.mlp.w2.weight to be bf16, got torch.float32
Expected layers.23.mlp_norm.scale to be bf16, got torch.float32Expected layers.24.mlp.w1.weight to be bf16, got torch.float32

Expected layers.14.attn.q_proj.weight to be bf16, got torch.float32Expected layers.14.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.20.attn.q_proj.weight to be bf16, got torch.float32Expected layers.24.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.28.mlp.w3.weight to be bf16, got torch.float32
Expected layers.27.mlp.w3.weight to be bf16, got torch.float32
Expected layers.23.mlp.w1.weight to be bf16, got torch.float32Expected layers.24.mlp.w2.weight to be bf16, got torch.float32

Expected layers.14.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.14.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.20.attn.k_proj.weight to be bf16, got torch.float32Expected layers.24.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.29.sa_norm.scale to be bf16, got torch.float32Expected layers.28.sa_norm.scale to be bf16, got torch.float32

Expected layers.23.mlp.w2.weight to be bf16, got torch.float32Expected layers.24.mlp.w3.weight to be bf16, got torch.float32

Expected layers.14.attn.v_proj.weight to be bf16, got torch.float32Expected layers.14.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.20.attn.v_proj.weight to be bf16, got torch.float32Expected layers.24.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.29.attn.q_proj.weight to be bf16, got torch.float32Expected layers.28.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.23.mlp.w3.weight to be bf16, got torch.float32Expected layers.25.sa_norm.scale to be bf16, got torch.float32

Expected layers.14.attn.output_proj.weight to be bf16, got torch.float32Expected layers.24.attn.output_proj.weight to be bf16, got torch.float32Expected layers.14.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.20.attn.output_proj.weight to be bf16, got torch.float32


Expected layers.29.attn.k_proj.weight to be bf16, got torch.float32Expected layers.28.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.24.sa_norm.scale to be bf16, got torch.float32Expected layers.25.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.14.mlp_norm.scale to be bf16, got torch.float32Expected layers.24.mlp_norm.scale to be bf16, got torch.float32
Expected layers.14.mlp_norm.scale to be bf16, got torch.float32
Expected layers.20.mlp_norm.scale to be bf16, got torch.float32

Expected layers.29.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.28.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.24.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.25.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.14.mlp.w1.weight to be bf16, got torch.float32Expected layers.24.mlp.w1.weight to be bf16, got torch.float32
Expected layers.14.mlp.w1.weight to be bf16, got torch.float32
Expected layers.20.mlp.w1.weight to be bf16, got torch.float32

Expected layers.29.attn.output_proj.weight to be bf16, got torch.float32Expected layers.28.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.24.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.25.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.14.mlp.w2.weight to be bf16, got torch.float32Expected layers.24.mlp.w2.weight to be bf16, got torch.float32

Expected layers.14.mlp.w2.weight to be bf16, got torch.float32
Expected layers.20.mlp.w2.weight to be bf16, got torch.float32
Expected layers.29.mlp_norm.scale to be bf16, got torch.float32Expected layers.28.mlp_norm.scale to be bf16, got torch.float32

Expected layers.24.attn.v_proj.weight to be bf16, got torch.float32Expected layers.25.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.14.mlp.w3.weight to be bf16, got torch.float32
Expected layers.24.mlp.w3.weight to be bf16, got torch.float32
Expected layers.14.mlp.w3.weight to be bf16, got torch.float32Expected layers.20.mlp.w3.weight to be bf16, got torch.float32

Expected layers.29.mlp.w1.weight to be bf16, got torch.float32Expected layers.28.mlp.w1.weight to be bf16, got torch.float32

Expected layers.24.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.25.mlp_norm.scale to be bf16, got torch.float32
Expected layers.15.sa_norm.scale to be bf16, got torch.float32Expected layers.25.sa_norm.scale to be bf16, got torch.float32

Expected layers.15.sa_norm.scale to be bf16, got torch.float32Expected layers.21.sa_norm.scale to be bf16, got torch.float32

Expected layers.29.mlp.w2.weight to be bf16, got torch.float32Expected layers.28.mlp.w2.weight to be bf16, got torch.float32

Expected layers.24.mlp_norm.scale to be bf16, got torch.float32Expected layers.25.mlp.w1.weight to be bf16, got torch.float32

Expected layers.15.attn.q_proj.weight to be bf16, got torch.float32Expected layers.25.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.15.attn.q_proj.weight to be bf16, got torch.float32Expected layers.21.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.29.mlp.w3.weight to be bf16, got torch.float32Expected layers.28.mlp.w3.weight to be bf16, got torch.float32

Expected layers.24.mlp.w1.weight to be bf16, got torch.float32Expected layers.25.mlp.w2.weight to be bf16, got torch.float32

Expected layers.15.attn.k_proj.weight to be bf16, got torch.float32Expected layers.25.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.30.sa_norm.scale to be bf16, got torch.float32Expected layers.15.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.21.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.29.sa_norm.scale to be bf16, got torch.float32

Expected layers.24.mlp.w2.weight to be bf16, got torch.float32
Expected layers.25.mlp.w3.weight to be bf16, got torch.float32
Expected layers.15.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.25.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.30.attn.q_proj.weight to be bf16, got torch.float32Expected layers.15.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.24.mlp.w3.weight to be bf16, got torch.float32Expected layers.21.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.29.attn.q_proj.weight to be bf16, got torch.float32



Expected layers.26.sa_norm.scale to be bf16, got torch.float32
Expected layers.15.attn.output_proj.weight to be bf16, got torch.float32Expected layers.25.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.30.attn.k_proj.weight to be bf16, got torch.float32Expected layers.15.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.25.sa_norm.scale to be bf16, got torch.float32
Expected layers.21.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.29.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.26.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.15.mlp_norm.scale to be bf16, got torch.float32
Expected layers.25.mlp_norm.scale to be bf16, got torch.float32
Expected layers.30.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.15.mlp_norm.scale to be bf16, got torch.float32Expected layers.25.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.21.mlp_norm.scale to be bf16, got torch.float32
Expected layers.29.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.26.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.15.mlp.w1.weight to be bf16, got torch.float32
Expected layers.25.mlp.w1.weight to be bf16, got torch.float32
Expected layers.30.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.15.mlp.w1.weight to be bf16, got torch.float32
Expected layers.25.attn.k_proj.weight to be bf16, got torch.float32Expected layers.21.mlp.w1.weight to be bf16, got torch.float32
Expected layers.29.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.26.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.15.mlp.w2.weight to be bf16, got torch.float32
Expected layers.25.mlp.w2.weight to be bf16, got torch.float32
Expected layers.30.mlp_norm.scale to be bf16, got torch.float32
Expected layers.15.mlp.w2.weight to be bf16, got torch.float32
Expected layers.25.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.21.mlp.w2.weight to be bf16, got torch.float32Expected layers.29.mlp_norm.scale to be bf16, got torch.float32
Expected layers.26.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.15.mlp.w3.weight to be bf16, got torch.float32

Expected layers.25.mlp.w3.weight to be bf16, got torch.float32
Expected layers.30.mlp.w1.weight to be bf16, got torch.float32
Expected layers.15.mlp.w3.weight to be bf16, got torch.float32
Expected layers.25.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.16.sa_norm.scale to be bf16, got torch.float32Expected layers.21.mlp.w3.weight to be bf16, got torch.float32
Expected layers.29.mlp.w1.weight to be bf16, got torch.float32Expected layers.26.mlp_norm.scale to be bf16, got torch.float32



Expected layers.26.sa_norm.scale to be bf16, got torch.float32Expected layers.30.mlp.w2.weight to be bf16, got torch.float32
Expected layers.16.sa_norm.scale to be bf16, got torch.float32

Expected layers.25.mlp_norm.scale to be bf16, got torch.float32Expected layers.16.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.22.sa_norm.scale to be bf16, got torch.float32Expected layers.29.mlp.w2.weight to be bf16, got torch.float32
Expected layers.26.mlp.w1.weight to be bf16, got torch.float32
Expected layers.26.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.30.mlp.w3.weight to be bf16, got torch.float32

Expected layers.16.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.25.mlp.w1.weight to be bf16, got torch.float32
Expected layers.16.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.22.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.29.mlp.w3.weight to be bf16, got torch.float32Expected layers.26.mlp.w2.weight to be bf16, got torch.float32

Expected layers.26.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.31.sa_norm.scale to be bf16, got torch.float32
Expected layers.16.attn.k_proj.weight to be bf16, got torch.float32Expected layers.25.mlp.w2.weight to be bf16, got torch.float32
Expected layers.30.sa_norm.scale to be bf16, got torch.float32
Expected layers.16.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.22.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.26.mlp.w3.weight to be bf16, got torch.float32

Expected layers.26.attn.v_proj.weight to be bf16, got torch.float32Expected layers.31.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.16.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.25.mlp.w3.weight to be bf16, got torch.float32
Expected layers.30.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.16.attn.output_proj.weight to be bf16, got torch.float32Expected layers.22.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.27.sa_norm.scale to be bf16, got torch.float32
Expected layers.26.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.31.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.16.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.26.sa_norm.scale to be bf16, got torch.float32Expected layers.30.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.16.mlp_norm.scale to be bf16, got torch.float32Expected layers.22.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.27.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.26.mlp_norm.scale to be bf16, got torch.float32
Expected layers.31.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.16.mlp_norm.scale to be bf16, got torch.float32

Expected layers.26.attn.q_proj.weight to be bf16, got torch.float32Expected layers.30.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.16.mlp.w1.weight to be bf16, got torch.float32
Expected layers.22.mlp_norm.scale to be bf16, got torch.float32Expected layers.27.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.26.mlp.w1.weight to be bf16, got torch.float32
Expected layers.31.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.30.attn.output_proj.weight to be bf16, got torch.float32Expected layers.16.mlp.w1.weight to be bf16, got torch.float32
Expected layers.26.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.16.mlp.w2.weight to be bf16, got torch.float32
Expected layers.22.mlp.w1.weight to be bf16, got torch.float32
Expected layers.27.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.26.mlp.w2.weight to be bf16, got torch.float32
Expected layers.31.mlp_norm.scale to be bf16, got torch.float32
Expected layers.30.mlp_norm.scale to be bf16, got torch.float32
Expected layers.16.mlp.w2.weight to be bf16, got torch.float32Expected layers.26.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.16.mlp.w3.weight to be bf16, got torch.float32
Expected layers.22.mlp.w2.weight to be bf16, got torch.float32

Expected layers.27.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.26.mlp.w3.weight to be bf16, got torch.float32
Expected layers.31.mlp.w1.weight to be bf16, got torch.float32
Expected layers.30.mlp.w1.weight to be bf16, got torch.float32
Expected layers.16.mlp.w3.weight to be bf16, got torch.float32
Expected layers.26.attn.output_proj.weight to be bf16, got torch.float32Expected layers.17.sa_norm.scale to be bf16, got torch.float32
Expected layers.22.mlp.w3.weight to be bf16, got torch.float32
Expected layers.27.mlp_norm.scale to be bf16, got torch.float32

Expected layers.27.sa_norm.scale to be bf16, got torch.float32
Expected layers.31.mlp.w2.weight to be bf16, got torch.float32
Expected layers.30.mlp.w2.weight to be bf16, got torch.float32
Expected layers.17.sa_norm.scale to be bf16, got torch.float32
Expected layers.26.mlp_norm.scale to be bf16, got torch.float32
Expected layers.17.attn.q_proj.weight to be bf16, got torch.float32Expected layers.23.sa_norm.scale to be bf16, got torch.float32
Expected layers.27.mlp.w1.weight to be bf16, got torch.float32
Expected layers.27.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.31.mlp.w3.weight to be bf16, got torch.float32
Expected layers.30.mlp.w3.weight to be bf16, got torch.float32
Expected layers.17.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.26.mlp.w1.weight to be bf16, got torch.float32
Expected layers.17.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.23.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.27.mlp.w2.weight to be bf16, got torch.float32
Expected layers.27.attn.k_proj.weight to be bf16, got torch.float32Expected layers.32.sa_norm.scale to be bf16, got torch.float32

Expected layers.31.sa_norm.scale to be bf16, got torch.float32
Expected layers.17.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.17.attn.k_proj.weight to be bf16, got torch.float32Expected layers.26.mlp.w2.weight to be bf16, got torch.float32

Expected layers.23.attn.k_proj.weight to be bf16, got torch.float32Expected layers.27.mlp.w3.weight to be bf16, got torch.float32
Expected layers.27.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.32.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.31.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.17.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.17.attn.v_proj.weight to be bf16, got torch.float32Expected layers.28.sa_norm.scale to be bf16, got torch.float32
Expected layers.26.mlp.w3.weight to be bf16, got torch.float32
Expected layers.23.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.27.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.32.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.31.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.17.mlp_norm.scale to be bf16, got torch.float32
Expected layers.17.attn.output_proj.weight to be bf16, got torch.float32Expected layers.28.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.27.mlp_norm.scale to be bf16, got torch.float32Expected layers.27.sa_norm.scale to be bf16, got torch.float32
Expected layers.23.attn.output_proj.weight to be bf16, got torch.float32


Expected layers.32.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.31.attn.v_proj.weight to be bf16, got torch.float32Expected layers.17.mlp.w1.weight to be bf16, got torch.float32
Expected layers.17.mlp_norm.scale to be bf16, got torch.float32Expected layers.28.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.27.mlp.w1.weight to be bf16, got torch.float32
Expected layers.32.attn.output_proj.weight to be bf16, got torch.float32Expected layers.27.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.23.mlp_norm.scale to be bf16, got torch.float32



Expected layers.31.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.17.mlp.w2.weight to be bf16, got torch.float32Expected layers.17.mlp.w1.weight to be bf16, got torch.float32
Expected layers.28.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.27.mlp.w2.weight to be bf16, got torch.float32Expected layers.32.mlp_norm.scale to be bf16, got torch.float32
Expected layers.27.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.23.mlp.w1.weight to be bf16, got torch.float32

Expected layers.31.mlp_norm.scale to be bf16, got torch.float32
Expected layers.17.mlp.w3.weight to be bf16, got torch.float32
Expected layers.17.mlp.w2.weight to be bf16, got torch.float32
Expected layers.28.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.27.mlp.w3.weight to be bf16, got torch.float32Expected layers.32.mlp.w1.weight to be bf16, got torch.float32

Expected layers.27.attn.v_proj.weight to be bf16, got torch.float32Expected layers.23.mlp.w2.weight to be bf16, got torch.float32

Expected layers.31.mlp.w1.weight to be bf16, got torch.float32Expected layers.18.sa_norm.scale to be bf16, got torch.float32

Expected layers.17.mlp.w3.weight to be bf16, got torch.float32Expected layers.28.mlp_norm.scale to be bf16, got torch.float32

Expected layers.28.sa_norm.scale to be bf16, got torch.float32Expected layers.32.mlp.w2.weight to be bf16, got torch.float32

Expected layers.27.attn.output_proj.weight to be bf16, got torch.float32Expected layers.23.mlp.w3.weight to be bf16, got torch.float32

Expected layers.31.mlp.w2.weight to be bf16, got torch.float32Expected layers.18.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.18.sa_norm.scale to be bf16, got torch.float32Expected layers.28.mlp.w1.weight to be bf16, got torch.float32

Expected layers.28.attn.q_proj.weight to be bf16, got torch.float32Expected layers.32.mlp.w3.weight to be bf16, got torch.float32

Expected layers.27.mlp_norm.scale to be bf16, got torch.float32Expected layers.24.sa_norm.scale to be bf16, got torch.float32

Expected layers.31.mlp.w3.weight to be bf16, got torch.float32
Expected layers.18.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.18.attn.q_proj.weight to be bf16, got torch.float32Expected layers.28.mlp.w2.weight to be bf16, got torch.float32
Expected layers.28.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.33.sa_norm.scale to be bf16, got torch.float32

Expected layers.27.mlp.w1.weight to be bf16, got torch.float32
Expected layers.24.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.32.sa_norm.scale to be bf16, got torch.float32
Expected layers.18.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.18.attn.k_proj.weight to be bf16, got torch.float32Expected layers.28.mlp.w3.weight to be bf16, got torch.float32
Expected layers.28.attn.v_proj.weight to be bf16, got torch.float32


Expected layers.33.attn.q_proj.weight to be bf16, got torch.float32Expected layers.27.mlp.w2.weight to be bf16, got torch.float32
Expected layers.24.attn.k_proj.weight to be bf16, got torch.float32Expected layers.32.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.18.attn.output_proj.weight to be bf16, got torch.float32Expected layers.18.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.29.sa_norm.scale to be bf16, got torch.float32
Expected layers.28.attn.output_proj.weight to be bf16, got torch.float32


Expected layers.33.attn.k_proj.weight to be bf16, got torch.float32Expected layers.27.mlp.w3.weight to be bf16, got torch.float32
Expected layers.24.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.32.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.18.mlp_norm.scale to be bf16, got torch.float32Expected layers.18.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.29.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.28.mlp_norm.scale to be bf16, got torch.float32

Expected layers.33.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.28.sa_norm.scale to be bf16, got torch.float32
Expected layers.24.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.32.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.18.mlp.w1.weight to be bf16, got torch.float32Expected layers.18.mlp_norm.scale to be bf16, got torch.float32
Expected layers.29.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.28.mlp.w1.weight to be bf16, got torch.float32

Expected layers.33.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.28.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.24.mlp_norm.scale to be bf16, got torch.float32
Expected layers.32.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.18.mlp.w2.weight to be bf16, got torch.float32Expected layers.18.mlp.w1.weight to be bf16, got torch.float32
Expected layers.29.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.28.mlp.w2.weight to be bf16, got torch.float32

Expected layers.33.mlp_norm.scale to be bf16, got torch.float32
Expected layers.28.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.24.mlp.w1.weight to be bf16, got torch.float32
Expected layers.32.mlp_norm.scale to be bf16, got torch.float32
Expected layers.18.mlp.w3.weight to be bf16, got torch.float32Expected layers.18.mlp.w2.weight to be bf16, got torch.float32
Expected layers.29.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.28.mlp.w3.weight to be bf16, got torch.float32

Expected layers.33.mlp.w1.weight to be bf16, got torch.float32
Expected layers.28.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.24.mlp.w2.weight to be bf16, got torch.float32
Expected layers.32.mlp.w1.weight to be bf16, got torch.float32
Expected layers.19.sa_norm.scale to be bf16, got torch.float32Expected layers.18.mlp.w3.weight to be bf16, got torch.float32
Expected layers.29.mlp_norm.scale to be bf16, got torch.float32
Expected layers.29.sa_norm.scale to be bf16, got torch.float32

Expected layers.33.mlp.w2.weight to be bf16, got torch.float32
Expected layers.28.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.24.mlp.w3.weight to be bf16, got torch.float32
Expected layers.32.mlp.w2.weight to be bf16, got torch.float32Expected layers.19.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.19.sa_norm.scale to be bf16, got torch.float32Expected layers.29.mlp.w1.weight to be bf16, got torch.float32


Expected layers.29.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.33.mlp.w3.weight to be bf16, got torch.float32
Expected layers.28.mlp_norm.scale to be bf16, got torch.float32
Expected layers.25.sa_norm.scale to be bf16, got torch.float32
Expected layers.32.mlp.w3.weight to be bf16, got torch.float32Expected layers.19.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.19.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.29.mlp.w2.weight to be bf16, got torch.float32

Expected layers.29.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.34.sa_norm.scale to be bf16, got torch.float32
Expected layers.28.mlp.w1.weight to be bf16, got torch.float32
Expected layers.25.attn.q_proj.weight to be bf16, got torch.float32Expected layers.33.sa_norm.scale to be bf16, got torch.float32
Expected layers.19.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.19.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.29.mlp.w3.weight to be bf16, got torch.float32
Expected layers.29.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.34.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.28.mlp.w2.weight to be bf16, got torch.float32
Expected layers.25.attn.k_proj.weight to be bf16, got torch.float32Expected layers.33.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.19.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.19.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.30.sa_norm.scale to be bf16, got torch.float32
Expected layers.29.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.34.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.28.mlp.w3.weight to be bf16, got torch.float32
Expected layers.25.attn.v_proj.weight to be bf16, got torch.float32Expected layers.33.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.19.mlp_norm.scale to be bf16, got torch.float32
Expected layers.19.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.30.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.29.mlp_norm.scale to be bf16, got torch.float32
Expected layers.34.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.29.sa_norm.scale to be bf16, got torch.float32
Expected layers.33.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.19.mlp.w1.weight to be bf16, got torch.float32
Expected layers.25.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.19.mlp_norm.scale to be bf16, got torch.float32
Expected layers.30.attn.k_proj.weight to be bf16, got torch.float32Expected layers.29.mlp.w1.weight to be bf16, got torch.float32
Expected layers.34.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.29.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.33.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.19.mlp.w2.weight to be bf16, got torch.float32

Expected layers.19.mlp.w1.weight to be bf16, got torch.float32Expected layers.25.mlp_norm.scale to be bf16, got torch.float32
Expected layers.30.attn.v_proj.weight to be bf16, got torch.float32Expected layers.29.mlp.w2.weight to be bf16, got torch.float32
Expected layers.34.mlp_norm.scale to be bf16, got torch.float32

Expected layers.33.mlp_norm.scale to be bf16, got torch.float32Expected layers.29.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.19.mlp.w3.weight to be bf16, got torch.float32
Expected layers.19.mlp.w2.weight to be bf16, got torch.float32
Expected layers.30.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.25.mlp.w1.weight to be bf16, got torch.float32
Expected layers.29.mlp.w3.weight to be bf16, got torch.float32Expected layers.34.mlp.w1.weight to be bf16, got torch.float32
Expected layers.33.mlp.w1.weight to be bf16, got torch.float32
Expected layers.29.attn.v_proj.weight to be bf16, got torch.float32Expected layers.20.sa_norm.scale to be bf16, got torch.float32

Expected layers.19.mlp.w3.weight to be bf16, got torch.float32


Expected layers.30.mlp_norm.scale to be bf16, got torch.float32Expected layers.25.mlp.w2.weight to be bf16, got torch.float32
Expected layers.30.sa_norm.scale to be bf16, got torch.float32Expected layers.34.mlp.w2.weight to be bf16, got torch.float32

Expected layers.33.mlp.w2.weight to be bf16, got torch.float32Expected layers.29.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.20.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.20.sa_norm.scale to be bf16, got torch.float32

Expected layers.30.mlp.w1.weight to be bf16, got torch.float32
Expected layers.25.mlp.w3.weight to be bf16, got torch.float32
Expected layers.30.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.34.mlp.w3.weight to be bf16, got torch.float32
Expected layers.33.mlp.w3.weight to be bf16, got torch.float32Expected layers.29.mlp_norm.scale to be bf16, got torch.float32
Expected layers.20.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.20.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.30.mlp.w2.weight to be bf16, got torch.float32
Expected layers.26.sa_norm.scale to be bf16, got torch.float32
Expected layers.30.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.35.sa_norm.scale to be bf16, got torch.float32
Expected layers.34.sa_norm.scale to be bf16, got torch.float32Expected layers.29.mlp.w1.weight to be bf16, got torch.float32
Expected layers.20.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.20.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.30.mlp.w3.weight to be bf16, got torch.float32
Expected layers.26.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.30.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.35.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.34.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.29.mlp.w2.weight to be bf16, got torch.float32Expected layers.20.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.20.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.31.sa_norm.scale to be bf16, got torch.float32

Expected layers.26.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.30.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.35.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.34.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.29.mlp.w3.weight to be bf16, got torch.float32Expected layers.20.mlp_norm.scale to be bf16, got torch.float32
Expected layers.20.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.31.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.26.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.30.mlp_norm.scale to be bf16, got torch.float32
Expected layers.35.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.34.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.30.sa_norm.scale to be bf16, got torch.float32Expected layers.20.mlp.w1.weight to be bf16, got torch.float32
Expected layers.20.mlp_norm.scale to be bf16, got torch.float32
Expected layers.31.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.26.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.30.mlp.w1.weight to be bf16, got torch.float32
Expected layers.35.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.34.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.30.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.20.mlp.w2.weight to be bf16, got torch.float32Expected layers.20.mlp.w1.weight to be bf16, got torch.float32
Expected layers.31.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.26.mlp_norm.scale to be bf16, got torch.float32
Expected layers.30.mlp.w2.weight to be bf16, got torch.float32Expected layers.35.mlp_norm.scale to be bf16, got torch.float32
Expected layers.34.mlp_norm.scale to be bf16, got torch.float32

Expected layers.30.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.20.mlp.w3.weight to be bf16, got torch.float32
Expected layers.20.mlp.w2.weight to be bf16, got torch.float32
Expected layers.31.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.30.mlp.w3.weight to be bf16, got torch.float32
Expected layers.35.mlp.w1.weight to be bf16, got torch.float32
Expected layers.26.mlp.w1.weight to be bf16, got torch.float32Expected layers.34.mlp.w1.weight to be bf16, got torch.float32
Expected layers.30.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.21.sa_norm.scale to be bf16, got torch.float32

Expected layers.20.mlp.w3.weight to be bf16, got torch.float32
Expected layers.31.mlp_norm.scale to be bf16, got torch.float32
Expected layers.31.sa_norm.scale to be bf16, got torch.float32
Expected layers.35.mlp.w2.weight to be bf16, got torch.float32
Expected layers.26.mlp.w2.weight to be bf16, got torch.float32
Expected layers.34.mlp.w2.weight to be bf16, got torch.float32
Expected layers.30.attn.output_proj.weight to be bf16, got torch.float32Expected layers.21.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.21.sa_norm.scale to be bf16, got torch.float32Expected layers.31.mlp.w1.weight to be bf16, got torch.float32

Expected layers.31.attn.q_proj.weight to be bf16, got torch.float32Expected layers.35.mlp.w3.weight to be bf16, got torch.float32

Expected layers.30.mlp_norm.scale to be bf16, got torch.float32Expected layers.34.mlp.w3.weight to be bf16, got torch.float32

Expected layers.26.mlp.w3.weight to be bf16, got torch.float32
Expected layers.21.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.21.attn.q_proj.weight to be bf16, got torch.float32Expected layers.31.mlp.w2.weight to be bf16, got torch.float32

Expected layers.31.attn.k_proj.weight to be bf16, got torch.float32Expected layers.36.sa_norm.scale to be bf16, got torch.float32

Expected layers.30.mlp.w1.weight to be bf16, got torch.float32Expected layers.21.attn.v_proj.weight to be bf16, got torch.float32Expected layers.35.sa_norm.scale to be bf16, got torch.float32

Expected layers.27.sa_norm.scale to be bf16, got torch.float32


Expected layers.21.attn.k_proj.weight to be bf16, got torch.float32Expected layers.31.mlp.w3.weight to be bf16, got torch.float32
Expected layers.31.attn.v_proj.weight to be bf16, got torch.float32Expected layers.36.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.30.mlp.w2.weight to be bf16, got torch.float32Expected layers.21.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.35.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.27.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.21.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.32.sa_norm.scale to be bf16, got torch.float32Expected layers.31.attn.output_proj.weight to be bf16, got torch.float32Expected layers.36.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.30.mlp.w3.weight to be bf16, got torch.float32
Expected layers.21.mlp_norm.scale to be bf16, got torch.float32
Expected layers.35.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.27.attn.k_proj.weight to be bf16, got torch.float32


Expected layers.21.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.32.attn.q_proj.weight to be bf16, got torch.float32Expected layers.31.mlp_norm.scale to be bf16, got torch.float32Expected layers.36.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.31.sa_norm.scale to be bf16, got torch.float32
Expected layers.21.mlp.w1.weight to be bf16, got torch.float32
Expected layers.35.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.27.attn.v_proj.weight to be bf16, got torch.float32


Expected layers.21.mlp_norm.scale to be bf16, got torch.float32Expected layers.32.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.31.mlp.w1.weight to be bf16, got torch.float32Expected layers.36.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.31.attn.q_proj.weight to be bf16, got torch.float32Expected layers.21.mlp.w2.weight to be bf16, got torch.float32
Expected layers.35.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.27.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.21.mlp.w1.weight to be bf16, got torch.float32
Expected layers.32.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.31.mlp.w2.weight to be bf16, got torch.float32
Expected layers.36.mlp_norm.scale to be bf16, got torch.float32
Expected layers.31.attn.k_proj.weight to be bf16, got torch.float32Expected layers.21.mlp.w3.weight to be bf16, got torch.float32
Expected layers.35.mlp_norm.scale to be bf16, got torch.float32
Expected layers.27.mlp_norm.scale to be bf16, got torch.float32

Expected layers.21.mlp.w2.weight to be bf16, got torch.float32
Expected layers.32.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.31.mlp.w3.weight to be bf16, got torch.float32
Expected layers.36.mlp.w1.weight to be bf16, got torch.float32
Expected layers.22.sa_norm.scale to be bf16, got torch.float32
Expected layers.35.mlp.w1.weight to be bf16, got torch.float32Expected layers.31.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.27.mlp.w1.weight to be bf16, got torch.float32

Expected layers.21.mlp.w3.weight to be bf16, got torch.float32
Expected layers.32.mlp_norm.scale to be bf16, got torch.float32
Expected layers.32.sa_norm.scale to be bf16, got torch.float32
Expected layers.36.mlp.w2.weight to be bf16, got torch.float32Expected layers.31.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.22.attn.q_proj.weight to be bf16, got torch.float32Expected layers.35.mlp.w2.weight to be bf16, got torch.float32


Expected layers.32.mlp.w1.weight to be bf16, got torch.float32
Expected layers.22.sa_norm.scale to be bf16, got torch.float32
Expected layers.27.mlp.w2.weight to be bf16, got torch.float32
Expected layers.32.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.36.mlp.w3.weight to be bf16, got torch.float32Expected layers.31.mlp_norm.scale to be bf16, got torch.float32
Expected layers.22.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.35.mlp.w3.weight to be bf16, got torch.float32

Expected layers.32.mlp.w2.weight to be bf16, got torch.float32
Expected layers.22.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.32.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.37.sa_norm.scale to be bf16, got torch.float32Expected layers.27.mlp.w3.weight to be bf16, got torch.float32Expected layers.31.mlp.w1.weight to be bf16, got torch.float32
Expected layers.22.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.36.sa_norm.scale to be bf16, got torch.float32Expected layers.32.mlp.w3.weight to be bf16, got torch.float32

Expected layers.22.attn.k_proj.weight to be bf16, got torch.float32


Expected layers.32.attn.v_proj.weight to be bf16, got torch.float32Expected layers.37.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.31.mlp.w2.weight to be bf16, got torch.float32

Expected layers.22.attn.output_proj.weight to be bf16, got torch.float32Expected layers.28.sa_norm.scale to be bf16, got torch.float32Expected layers.36.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.33.sa_norm.scale to be bf16, got torch.float32
Expected layers.22.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.32.attn.output_proj.weight to be bf16, got torch.float32Expected layers.37.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.31.mlp.w3.weight to be bf16, got torch.float32

Expected layers.22.mlp_norm.scale to be bf16, got torch.float32Expected layers.36.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.33.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.28.attn.q_proj.weight to be bf16, got torch.float32Expected layers.22.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.32.mlp_norm.scale to be bf16, got torch.float32Expected layers.37.attn.v_proj.weight to be bf16, got torch.float32


Expected layers.32.sa_norm.scale to be bf16, got torch.float32
Expected layers.22.mlp.w1.weight to be bf16, got torch.float32
Expected layers.36.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.33.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.22.mlp_norm.scale to be bf16, got torch.float32
Expected layers.32.mlp.w1.weight to be bf16, got torch.float32
Expected layers.28.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.37.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.32.attn.q_proj.weight to be bf16, got torch.float32Expected layers.22.mlp.w2.weight to be bf16, got torch.float32
Expected layers.36.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.33.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.22.mlp.w1.weight to be bf16, got torch.float32Expected layers.28.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.32.mlp.w2.weight to be bf16, got torch.float32

Expected layers.37.mlp_norm.scale to be bf16, got torch.float32

Expected layers.32.attn.k_proj.weight to be bf16, got torch.float32Expected layers.22.mlp.w3.weight to be bf16, got torch.float32
Expected layers.36.mlp_norm.scale to be bf16, got torch.float32Expected layers.33.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.22.mlp.w2.weight to be bf16, got torch.float32
Expected layers.28.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.32.mlp.w3.weight to be bf16, got torch.float32

Expected layers.37.mlp.w1.weight to be bf16, got torch.float32

Expected layers.32.attn.v_proj.weight to be bf16, got torch.float32Expected layers.23.sa_norm.scale to be bf16, got torch.float32
Expected layers.36.mlp.w1.weight to be bf16, got torch.float32Expected layers.33.mlp_norm.scale to be bf16, got torch.float32
Expected layers.22.mlp.w3.weight to be bf16, got torch.float32
Expected layers.28.mlp_norm.scale to be bf16, got torch.float32
Expected layers.33.sa_norm.scale to be bf16, got torch.float32

Expected layers.37.mlp.w2.weight to be bf16, got torch.float32

Expected layers.32.attn.output_proj.weight to be bf16, got torch.float32Expected layers.23.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.36.mlp.w2.weight to be bf16, got torch.float32Expected layers.33.mlp.w1.weight to be bf16, got torch.float32
Expected layers.23.sa_norm.scale to be bf16, got torch.float32
Expected layers.28.mlp.w1.weight to be bf16, got torch.float32
Expected layers.33.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.37.mlp.w3.weight to be bf16, got torch.float32

Expected layers.32.mlp_norm.scale to be bf16, got torch.float32Expected layers.23.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.36.mlp.w3.weight to be bf16, got torch.float32Expected layers.33.mlp.w2.weight to be bf16, got torch.float32
Expected layers.23.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.33.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.28.mlp.w2.weight to be bf16, got torch.float32
Expected layers.23.attn.v_proj.weight to be bf16, got torch.float32Expected layers.38.sa_norm.scale to be bf16, got torch.float32
Expected layers.32.mlp.w1.weight to be bf16, got torch.float32
Expected layers.37.sa_norm.scale to be bf16, got torch.float32

Expected layers.33.mlp.w3.weight to be bf16, got torch.float32
Expected layers.23.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.33.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.28.mlp.w3.weight to be bf16, got torch.float32
Expected layers.23.attn.output_proj.weight to be bf16, got torch.float32Expected layers.38.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.32.mlp.w2.weight to be bf16, got torch.float32

Expected layers.37.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.34.sa_norm.scale to be bf16, got torch.float32Expected layers.23.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.33.attn.output_proj.weight to be bf16, got torch.float32


Expected layers.29.sa_norm.scale to be bf16, got torch.float32Expected layers.23.mlp_norm.scale to be bf16, got torch.float32
Expected layers.38.attn.k_proj.weight to be bf16, got torch.float32Expected layers.32.mlp.w3.weight to be bf16, got torch.float32

Expected layers.37.attn.k_proj.weight to be bf16, got torch.float32Expected layers.34.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.23.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.33.mlp_norm.scale to be bf16, got torch.float32


Expected layers.23.mlp.w1.weight to be bf16, got torch.float32Expected layers.29.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.38.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.33.sa_norm.scale to be bf16, got torch.float32
Expected layers.37.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.34.attn.k_proj.weight to be bf16, got torch.float32Expected layers.23.mlp_norm.scale to be bf16, got torch.float32
Expected layers.33.mlp.w1.weight to be bf16, got torch.float32
Expected layers.23.mlp.w2.weight to be bf16, got torch.float32

Expected layers.29.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.38.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.33.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.37.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.34.attn.v_proj.weight to be bf16, got torch.float32Expected layers.23.mlp.w1.weight to be bf16, got torch.float32
Expected layers.33.mlp.w2.weight to be bf16, got torch.float32
Expected layers.23.mlp.w3.weight to be bf16, got torch.float32

Expected layers.29.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.38.mlp_norm.scale to be bf16, got torch.float32
Expected layers.33.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.37.mlp_norm.scale to be bf16, got torch.float32
Expected layers.34.attn.output_proj.weight to be bf16, got torch.float32Expected layers.23.mlp.w2.weight to be bf16, got torch.float32
Expected layers.33.mlp.w3.weight to be bf16, got torch.float32
Expected layers.24.sa_norm.scale to be bf16, got torch.float32

Expected layers.29.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.38.mlp.w1.weight to be bf16, got torch.float32
Expected layers.33.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.37.mlp.w1.weight to be bf16, got torch.float32
Expected layers.34.mlp_norm.scale to be bf16, got torch.float32Expected layers.23.mlp.w3.weight to be bf16, got torch.float32
Expected layers.34.sa_norm.scale to be bf16, got torch.float32
Expected layers.24.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.29.mlp_norm.scale to be bf16, got torch.float32
Expected layers.38.mlp.w2.weight to be bf16, got torch.float32
Expected layers.33.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.37.mlp.w2.weight to be bf16, got torch.float32
Expected layers.34.mlp.w1.weight to be bf16, got torch.float32Expected layers.24.sa_norm.scale to be bf16, got torch.float32

Expected layers.34.attn.q_proj.weight to be bf16, got torch.float32Expected layers.24.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.29.mlp.w1.weight to be bf16, got torch.float32
Expected layers.38.mlp.w3.weight to be bf16, got torch.float32
Expected layers.33.mlp_norm.scale to be bf16, got torch.float32Expected layers.37.mlp.w3.weight to be bf16, got torch.float32

Expected layers.34.mlp.w2.weight to be bf16, got torch.float32Expected layers.24.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.34.attn.k_proj.weight to be bf16, got torch.float32Expected layers.39.sa_norm.scale to be bf16, got torch.float32Expected layers.24.attn.v_proj.weight to be bf16, got torch.float32


Expected layers.29.mlp.w2.weight to be bf16, got torch.float32
Expected layers.33.mlp.w1.weight to be bf16, got torch.float32
Expected layers.38.sa_norm.scale to be bf16, got torch.float32
Expected layers.34.mlp.w3.weight to be bf16, got torch.float32
Expected layers.24.attn.k_proj.weight to be bf16, got torch.float32Expected layers.34.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.39.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.24.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.29.mlp.w3.weight to be bf16, got torch.float32
Expected layers.33.mlp.w2.weight to be bf16, got torch.float32
Expected layers.38.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.35.sa_norm.scale to be bf16, got torch.float32
Expected layers.24.attn.v_proj.weight to be bf16, got torch.float32Expected layers.34.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.39.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.24.mlp_norm.scale to be bf16, got torch.float32

Expected layers.30.sa_norm.scale to be bf16, got torch.float32
Expected layers.33.mlp.w3.weight to be bf16, got torch.float32
Expected layers.38.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.35.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.24.attn.output_proj.weight to be bf16, got torch.float32Expected layers.34.mlp_norm.scale to be bf16, got torch.float32
Expected layers.39.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.24.mlp.w1.weight to be bf16, got torch.float32

Expected layers.30.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.34.sa_norm.scale to be bf16, got torch.float32
Expected layers.38.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.35.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.24.mlp_norm.scale to be bf16, got torch.float32Expected layers.34.mlp.w1.weight to be bf16, got torch.float32
Expected layers.39.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.24.mlp.w2.weight to be bf16, got torch.float32

Expected layers.30.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.34.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.38.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.35.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.24.mlp.w1.weight to be bf16, got torch.float32Expected layers.34.mlp.w2.weight to be bf16, got torch.float32
Expected layers.39.mlp_norm.scale to be bf16, got torch.float32
Expected layers.24.mlp.w3.weight to be bf16, got torch.float32

Expected layers.38.mlp_norm.scale to be bf16, got torch.float32Expected layers.30.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.34.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.35.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.24.mlp.w2.weight to be bf16, got torch.float32
Expected layers.34.mlp.w3.weight to be bf16, got torch.float32
Expected layers.39.mlp.w1.weight to be bf16, got torch.float32
Expected layers.25.sa_norm.scale to be bf16, got torch.float32Expected layers.38.mlp.w1.weight to be bf16, got torch.float32

Expected layers.34.attn.v_proj.weight to be bf16, got torch.float32Expected layers.30.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.35.mlp_norm.scale to be bf16, got torch.float32
Expected layers.24.mlp.w3.weight to be bf16, got torch.float32
Expected layers.35.sa_norm.scale to be bf16, got torch.float32Expected layers.39.mlp.w2.weight to be bf16, got torch.float32

Expected layers.25.attn.q_proj.weight to be bf16, got torch.float32Expected layers.38.mlp.w2.weight to be bf16, got torch.float32

Expected layers.34.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.30.mlp_norm.scale to be bf16, got torch.float32Expected layers.35.mlp.w1.weight to be bf16, got torch.float32Expected layers.25.sa_norm.scale to be bf16, got torch.float32
Expected layers.35.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.39.mlp.w3.weight to be bf16, got torch.float32

Expected layers.25.attn.k_proj.weight to be bf16, got torch.float32Expected layers.38.mlp.w3.weight to be bf16, got torch.float32

Expected layers.34.mlp_norm.scale to be bf16, got torch.float32

Expected layers.30.mlp.w1.weight to be bf16, got torch.float32Expected layers.35.mlp.w2.weight to be bf16, got torch.float32Expected layers.25.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.35.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.40.sa_norm.scale to be bf16, got torch.float32

Expected layers.25.attn.v_proj.weight to be bf16, got torch.float32Expected layers.39.sa_norm.scale to be bf16, got torch.float32

Expected layers.34.mlp.w1.weight to be bf16, got torch.float32

Expected layers.35.mlp.w3.weight to be bf16, got torch.float32Expected layers.25.attn.k_proj.weight to be bf16, got torch.float32Expected layers.30.mlp.w2.weight to be bf16, got torch.float32

Expected layers.40.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.25.attn.output_proj.weight to be bf16, got torch.float32Expected layers.35.attn.v_proj.weight to be bf16, got torch.float32Expected layers.39.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.34.mlp.w2.weight to be bf16, got torch.float32
Expected layers.36.sa_norm.scale to be bf16, got torch.float32

Expected layers.25.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.30.mlp.w3.weight to be bf16, got torch.float32
Expected layers.40.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.25.mlp_norm.scale to be bf16, got torch.float32
Expected layers.39.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.35.attn.output_proj.weight to be bf16, got torch.float32Expected layers.34.mlp.w3.weight to be bf16, got torch.float32
Expected layers.36.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.25.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.31.sa_norm.scale to be bf16, got torch.float32
Expected layers.40.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.25.mlp.w1.weight to be bf16, got torch.float32
Expected layers.39.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.35.mlp_norm.scale to be bf16, got torch.float32Expected layers.35.sa_norm.scale to be bf16, got torch.float32
Expected layers.36.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.25.mlp_norm.scale to be bf16, got torch.float32


Expected layers.40.attn.output_proj.weight to be bf16, got torch.float32Expected layers.31.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.25.mlp.w2.weight to be bf16, got torch.float32Expected layers.39.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.35.attn.q_proj.weight to be bf16, got torch.float32Expected layers.35.mlp.w1.weight to be bf16, got torch.float32Expected layers.36.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.25.mlp.w1.weight to be bf16, got torch.float32

Expected layers.40.mlp_norm.scale to be bf16, got torch.float32

Expected layers.31.attn.k_proj.weight to be bf16, got torch.float32Expected layers.25.mlp.w3.weight to be bf16, got torch.float32
Expected layers.39.mlp_norm.scale to be bf16, got torch.float32Expected layers.35.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.35.mlp.w2.weight to be bf16, got torch.float32Expected layers.36.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.25.mlp.w2.weight to be bf16, got torch.float32
Expected layers.40.mlp.w1.weight to be bf16, got torch.float32

Expected layers.31.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.26.sa_norm.scale to be bf16, got torch.float32
Expected layers.39.mlp.w1.weight to be bf16, got torch.float32
Expected layers.35.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.36.mlp_norm.scale to be bf16, got torch.float32Expected layers.35.mlp.w3.weight to be bf16, got torch.float32Expected layers.25.mlp.w3.weight to be bf16, got torch.float32
Expected layers.40.mlp.w2.weight to be bf16, got torch.float32
Expected layers.31.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.26.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.36.mlp.w1.weight to be bf16, got torch.float32Expected layers.39.mlp.w2.weight to be bf16, got torch.float32

Expected layers.35.attn.output_proj.weight to be bf16, got torch.float32


Expected layers.26.sa_norm.scale to be bf16, got torch.float32
Expected layers.36.sa_norm.scale to be bf16, got torch.float32Expected layers.40.mlp.w3.weight to be bf16, got torch.float32
Expected layers.26.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.36.mlp.w2.weight to be bf16, got torch.float32Expected layers.31.mlp_norm.scale to be bf16, got torch.float32
Expected layers.39.mlp.w3.weight to be bf16, got torch.float32
Expected layers.35.mlp_norm.scale to be bf16, got torch.float32

Expected layers.26.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.36.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.41.sa_norm.scale to be bf16, got torch.float32
Expected layers.26.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.31.mlp.w1.weight to be bf16, got torch.float32
Expected layers.40.sa_norm.scale to be bf16, got torch.float32Expected layers.36.mlp.w3.weight to be bf16, got torch.float32
Expected layers.35.mlp.w1.weight to be bf16, got torch.float32
Expected layers.26.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.36.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.41.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.26.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.31.mlp.w2.weight to be bf16, got torch.float32Expected layers.37.sa_norm.scale to be bf16, got torch.float32Expected layers.40.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.35.mlp.w2.weight to be bf16, got torch.float32Expected layers.26.attn.v_proj.weight to be bf16, got torch.float32


Expected layers.36.attn.v_proj.weight to be bf16, got torch.float32Expected layers.41.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.26.mlp_norm.scale to be bf16, got torch.float32

Expected layers.31.mlp.w3.weight to be bf16, got torch.float32Expected layers.37.attn.q_proj.weight to be bf16, got torch.float32Expected layers.40.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.35.mlp.w3.weight to be bf16, got torch.float32
Expected layers.26.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.41.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.36.attn.output_proj.weight to be bf16, got torch.float32Expected layers.26.mlp.w1.weight to be bf16, got torch.float32
Expected layers.32.sa_norm.scale to be bf16, got torch.float32Expected layers.37.attn.k_proj.weight to be bf16, got torch.float32Expected layers.40.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.36.sa_norm.scale to be bf16, got torch.float32Expected layers.26.mlp_norm.scale to be bf16, got torch.float32


Expected layers.41.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.36.mlp_norm.scale to be bf16, got torch.float32Expected layers.26.mlp.w2.weight to be bf16, got torch.float32
Expected layers.32.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.40.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.36.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.26.mlp.w1.weight to be bf16, got torch.float32
Expected layers.37.attn.v_proj.weight to be bf16, got torch.float32Expected layers.41.mlp_norm.scale to be bf16, got torch.float32

Expected layers.36.mlp.w1.weight to be bf16, got torch.float32Expected layers.26.mlp.w3.weight to be bf16, got torch.float32

Expected layers.32.attn.k_proj.weight to be bf16, got torch.float32Expected layers.40.mlp_norm.scale to be bf16, got torch.float32

Expected layers.36.attn.k_proj.weight to be bf16, got torch.float32Expected layers.26.mlp.w2.weight to be bf16, got torch.float32

Expected layers.41.mlp.w1.weight to be bf16, got torch.float32
Expected layers.37.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.36.mlp.w2.weight to be bf16, got torch.float32Expected layers.27.sa_norm.scale to be bf16, got torch.float32

Expected layers.32.attn.v_proj.weight to be bf16, got torch.float32Expected layers.40.mlp.w1.weight to be bf16, got torch.float32

Expected layers.36.attn.v_proj.weight to be bf16, got torch.float32Expected layers.26.mlp.w3.weight to be bf16, got torch.float32

Expected layers.41.mlp.w2.weight to be bf16, got torch.float32Expected layers.37.mlp_norm.scale to be bf16, got torch.float32

Expected layers.36.mlp.w3.weight to be bf16, got torch.float32Expected layers.27.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.32.attn.output_proj.weight to be bf16, got torch.float32Expected layers.40.mlp.w2.weight to be bf16, got torch.float32

Expected layers.36.attn.output_proj.weight to be bf16, got torch.float32Expected layers.27.sa_norm.scale to be bf16, got torch.float32

Expected layers.41.mlp.w3.weight to be bf16, got torch.float32
Expected layers.37.mlp.w1.weight to be bf16, got torch.float32
Expected layers.37.sa_norm.scale to be bf16, got torch.float32
Expected layers.27.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.32.mlp_norm.scale to be bf16, got torch.float32Expected layers.40.mlp.w3.weight to be bf16, got torch.float32
Expected layers.36.mlp_norm.scale to be bf16, got torch.float32
Expected layers.27.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.42.sa_norm.scale to be bf16, got torch.float32
Expected layers.37.mlp.w2.weight to be bf16, got torch.float32

Expected layers.37.attn.q_proj.weight to be bf16, got torch.float32Expected layers.27.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.32.mlp.w1.weight to be bf16, got torch.float32Expected layers.41.sa_norm.scale to be bf16, got torch.float32
Expected layers.36.mlp.w1.weight to be bf16, got torch.float32
Expected layers.27.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.42.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.37.mlp.w3.weight to be bf16, got torch.float32

Expected layers.37.attn.k_proj.weight to be bf16, got torch.float32Expected layers.27.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.32.mlp.w2.weight to be bf16, got torch.float32Expected layers.41.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.36.mlp.w2.weight to be bf16, got torch.float32
Expected layers.27.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.42.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.38.sa_norm.scale to be bf16, got torch.float32

Expected layers.37.attn.v_proj.weight to be bf16, got torch.float32Expected layers.27.mlp_norm.scale to be bf16, got torch.float32
Expected layers.41.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.36.mlp.w3.weight to be bf16, got torch.float32Expected layers.32.mlp.w3.weight to be bf16, got torch.float32
Expected layers.27.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.42.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.38.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.37.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.27.mlp.w1.weight to be bf16, got torch.float32
Expected layers.41.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.37.sa_norm.scale to be bf16, got torch.float32
Expected layers.27.mlp_norm.scale to be bf16, got torch.float32
Expected layers.33.sa_norm.scale to be bf16, got torch.float32Expected layers.42.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.38.attn.k_proj.weight to be bf16, got torch.float32Expected layers.37.mlp_norm.scale to be bf16, got torch.float32

Expected layers.27.mlp.w2.weight to be bf16, got torch.float32Expected layers.41.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.37.attn.q_proj.weight to be bf16, got torch.float32Expected layers.27.mlp.w1.weight to be bf16, got torch.float32

Expected layers.42.mlp_norm.scale to be bf16, got torch.float32
Expected layers.33.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.38.attn.v_proj.weight to be bf16, got torch.float32Expected layers.37.mlp.w1.weight to be bf16, got torch.float32
Expected layers.27.mlp.w3.weight to be bf16, got torch.float32
Expected layers.41.mlp_norm.scale to be bf16, got torch.float32

Expected layers.37.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.27.mlp.w2.weight to be bf16, got torch.float32
Expected layers.42.mlp.w1.weight to be bf16, got torch.float32

Expected layers.38.attn.output_proj.weight to be bf16, got torch.float32Expected layers.33.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.37.mlp.w2.weight to be bf16, got torch.float32Expected layers.28.sa_norm.scale to be bf16, got torch.float32
Expected layers.41.mlp.w1.weight to be bf16, got torch.float32
Expected layers.37.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.27.mlp.w3.weight to be bf16, got torch.float32

Expected layers.42.mlp.w2.weight to be bf16, got torch.float32
Expected layers.38.mlp_norm.scale to be bf16, got torch.float32

Expected layers.37.mlp.w3.weight to be bf16, got torch.float32
Expected layers.33.attn.v_proj.weight to be bf16, got torch.float32Expected layers.28.attn.q_proj.weight to be bf16, got torch.float32Expected layers.41.mlp.w2.weight to be bf16, got torch.float32
Expected layers.37.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.28.sa_norm.scale to be bf16, got torch.float32
Expected layers.42.mlp.w3.weight to be bf16, got torch.float32
Expected layers.38.mlp.w1.weight to be bf16, got torch.float32


Expected layers.38.sa_norm.scale to be bf16, got torch.float32
Expected layers.28.attn.k_proj.weight to be bf16, got torch.float32Expected layers.33.attn.output_proj.weight to be bf16, got torch.float32Expected layers.41.mlp.w3.weight to be bf16, got torch.float32
Expected layers.37.mlp_norm.scale to be bf16, got torch.float32
Expected layers.28.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.43.sa_norm.scale to be bf16, got torch.float32
Expected layers.38.mlp.w2.weight to be bf16, got torch.float32


Expected layers.38.attn.q_proj.weight to be bf16, got torch.float32Expected layers.28.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.33.mlp_norm.scale to be bf16, got torch.float32Expected layers.42.sa_norm.scale to be bf16, got torch.float32

Expected layers.37.mlp.w1.weight to be bf16, got torch.float32Expected layers.28.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.43.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.38.mlp.w3.weight to be bf16, got torch.float32


Expected layers.38.attn.k_proj.weight to be bf16, got torch.float32Expected layers.28.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.33.mlp.w1.weight to be bf16, got torch.float32Expected layers.42.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.37.mlp.w2.weight to be bf16, got torch.float32Expected layers.28.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.43.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.39.sa_norm.scale to be bf16, got torch.float32


Expected layers.38.attn.v_proj.weight to be bf16, got torch.float32Expected layers.28.mlp_norm.scale to be bf16, got torch.float32
Expected layers.42.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.33.mlp.w2.weight to be bf16, got torch.float32
Expected layers.37.mlp.w3.weight to be bf16, got torch.float32Expected layers.28.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.43.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.39.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.38.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.28.mlp.w1.weight to be bf16, got torch.float32
Expected layers.42.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.38.sa_norm.scale to be bf16, got torch.float32Expected layers.28.mlp_norm.scale to be bf16, got torch.float32Expected layers.33.mlp.w3.weight to be bf16, got torch.float32
Expected layers.43.attn.output_proj.weight to be bf16, got torch.float32Expected layers.39.attn.k_proj.weight to be bf16, got torch.float32


Expected layers.38.mlp_norm.scale to be bf16, got torch.float32Expected layers.28.mlp.w2.weight to be bf16, got torch.float32

Expected layers.42.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.38.attn.q_proj.weight to be bf16, got torch.float32Expected layers.28.mlp.w1.weight to be bf16, got torch.float32Expected layers.34.sa_norm.scale to be bf16, got torch.float32
Expected layers.43.mlp_norm.scale to be bf16, got torch.float32

Expected layers.39.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.38.mlp.w1.weight to be bf16, got torch.float32Expected layers.28.mlp.w3.weight to be bf16, got torch.float32

Expected layers.42.mlp_norm.scale to be bf16, got torch.float32

Expected layers.38.attn.k_proj.weight to be bf16, got torch.float32Expected layers.28.mlp.w2.weight to be bf16, got torch.float32Expected layers.34.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.43.mlp.w1.weight to be bf16, got torch.float32

Expected layers.39.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.38.mlp.w2.weight to be bf16, got torch.float32Expected layers.29.sa_norm.scale to be bf16, got torch.float32

Expected layers.42.mlp.w1.weight to be bf16, got torch.float32

Expected layers.38.attn.v_proj.weight to be bf16, got torch.float32Expected layers.28.mlp.w3.weight to be bf16, got torch.float32Expected layers.34.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.43.mlp.w2.weight to be bf16, got torch.float32

Expected layers.39.mlp_norm.scale to be bf16, got torch.float32
Expected layers.38.mlp.w3.weight to be bf16, got torch.float32Expected layers.29.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.42.mlp.w2.weight to be bf16, got torch.float32
Expected layers.38.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.29.sa_norm.scale to be bf16, got torch.float32Expected layers.34.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.43.mlp.w3.weight to be bf16, got torch.float32Expected layers.29.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.39.mlp.w1.weight to be bf16, got torch.float32
Expected layers.39.sa_norm.scale to be bf16, got torch.float32

Expected layers.42.mlp.w3.weight to be bf16, got torch.float32
Expected layers.38.mlp_norm.scale to be bf16, got torch.float32
Expected layers.29.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.34.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.44.sa_norm.scale to be bf16, got torch.float32Expected layers.29.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.39.mlp.w2.weight to be bf16, got torch.float32
Expected layers.39.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.43.sa_norm.scale to be bf16, got torch.float32
Expected layers.38.mlp.w1.weight to be bf16, got torch.float32
Expected layers.29.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.34.mlp_norm.scale to be bf16, got torch.float32
Expected layers.44.attn.q_proj.weight to be bf16, got torch.float32Expected layers.29.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.39.mlp.w3.weight to be bf16, got torch.float32
Expected layers.39.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.43.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.38.mlp.w2.weight to be bf16, got torch.float32
Expected layers.29.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.34.mlp.w1.weight to be bf16, got torch.float32Expected layers.44.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.29.mlp_norm.scale to be bf16, got torch.float32
Expected layers.40.sa_norm.scale to be bf16, got torch.float32

Expected layers.39.attn.v_proj.weight to be bf16, got torch.float32Expected layers.43.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.34.mlp.w2.weight to be bf16, got torch.float32
Expected layers.38.mlp.w3.weight to be bf16, got torch.float32Expected layers.29.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.44.attn.v_proj.weight to be bf16, got torch.float32


Expected layers.29.mlp.w1.weight to be bf16, got torch.float32Expected layers.40.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.39.attn.output_proj.weight to be bf16, got torch.float32Expected layers.43.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.34.mlp.w3.weight to be bf16, got torch.float32
Expected layers.39.sa_norm.scale to be bf16, got torch.float32Expected layers.29.mlp_norm.scale to be bf16, got torch.float32

Expected layers.44.attn.output_proj.weight to be bf16, got torch.float32


Expected layers.29.mlp.w2.weight to be bf16, got torch.float32Expected layers.43.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.40.attn.k_proj.weight to be bf16, got torch.float32Expected layers.39.mlp_norm.scale to be bf16, got torch.float32

Expected layers.39.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.29.mlp.w1.weight to be bf16, got torch.float32
Expected layers.35.sa_norm.scale to be bf16, got torch.float32Expected layers.44.mlp_norm.scale to be bf16, got torch.float32

Expected layers.29.mlp.w3.weight to be bf16, got torch.float32Expected layers.43.mlp_norm.scale to be bf16, got torch.float32

Expected layers.40.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.39.mlp.w1.weight to be bf16, got torch.float32Expected layers.39.attn.k_proj.weight to be bf16, got torch.float32Expected layers.29.mlp.w2.weight to be bf16, got torch.float32


Expected layers.35.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.44.mlp.w1.weight to be bf16, got torch.float32
Expected layers.30.sa_norm.scale to be bf16, got torch.float32
Expected layers.43.mlp.w1.weight to be bf16, got torch.float32
Expected layers.40.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.39.mlp.w2.weight to be bf16, got torch.float32Expected layers.39.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.29.mlp.w3.weight to be bf16, got torch.float32
Expected layers.35.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.44.mlp.w2.weight to be bf16, got torch.float32
Expected layers.30.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.43.mlp.w2.weight to be bf16, got torch.float32
Expected layers.40.mlp_norm.scale to be bf16, got torch.float32
Expected layers.39.mlp.w3.weight to be bf16, got torch.float32Expected layers.39.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.30.sa_norm.scale to be bf16, got torch.float32
Expected layers.35.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.44.mlp.w3.weight to be bf16, got torch.float32
Expected layers.30.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.43.mlp.w3.weight to be bf16, got torch.float32
Expected layers.40.mlp.w1.weight to be bf16, got torch.float32
Expected layers.40.sa_norm.scale to be bf16, got torch.float32Expected layers.39.mlp_norm.scale to be bf16, got torch.float32
Expected layers.30.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.35.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.45.sa_norm.scale to be bf16, got torch.float32
Expected layers.30.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.44.sa_norm.scale to be bf16, got torch.float32
Expected layers.40.mlp.w2.weight to be bf16, got torch.float32
Expected layers.39.mlp.w1.weight to be bf16, got torch.float32Expected layers.40.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.30.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.35.mlp_norm.scale to be bf16, got torch.float32

Expected layers.45.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.30.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.44.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.40.mlp.w3.weight to be bf16, got torch.float32Expected layers.39.mlp.w2.weight to be bf16, got torch.float32
Expected layers.40.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.30.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.35.mlp.w1.weight to be bf16, got torch.float32
Expected layers.45.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.30.mlp_norm.scale to be bf16, got torch.float32
Expected layers.44.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.41.sa_norm.scale to be bf16, got torch.float32Expected layers.39.mlp.w3.weight to be bf16, got torch.float32

Expected layers.40.attn.v_proj.weight to be bf16, got torch.float32Expected layers.30.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.35.mlp.w2.weight to be bf16, got torch.float32Expected layers.45.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.30.mlp.w1.weight to be bf16, got torch.float32Expected layers.44.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.41.attn.q_proj.weight to be bf16, got torch.float32Expected layers.40.sa_norm.scale to be bf16, got torch.float32

Expected layers.40.attn.output_proj.weight to be bf16, got torch.float32Expected layers.30.mlp_norm.scale to be bf16, got torch.float32

Expected layers.35.mlp.w3.weight to be bf16, got torch.float32Expected layers.45.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.30.mlp.w2.weight to be bf16, got torch.float32Expected layers.44.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.41.attn.k_proj.weight to be bf16, got torch.float32Expected layers.40.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.30.mlp.w1.weight to be bf16, got torch.float32Expected layers.40.mlp_norm.scale to be bf16, got torch.float32

Expected layers.45.mlp_norm.scale to be bf16, got torch.float32

Expected layers.36.sa_norm.scale to be bf16, got torch.float32Expected layers.30.mlp.w3.weight to be bf16, got torch.float32Expected layers.44.mlp_norm.scale to be bf16, got torch.float32
Expected layers.41.attn.v_proj.weight to be bf16, got torch.float32Expected layers.40.attn.k_proj.weight to be bf16, got torch.float32


Expected layers.30.mlp.w2.weight to be bf16, got torch.float32
Expected layers.40.mlp.w1.weight to be bf16, got torch.float32
Expected layers.45.mlp.w1.weight to be bf16, got torch.float32

Expected layers.31.sa_norm.scale to be bf16, got torch.float32Expected layers.44.mlp.w1.weight to be bf16, got torch.float32
Expected layers.36.attn.q_proj.weight to be bf16, got torch.float32Expected layers.41.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.40.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.30.mlp.w3.weight to be bf16, got torch.float32

Expected layers.40.mlp.w2.weight to be bf16, got torch.float32
Expected layers.45.mlp.w2.weight to be bf16, got torch.float32
Expected layers.31.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.44.mlp.w2.weight to be bf16, got torch.float32
Expected layers.41.mlp_norm.scale to be bf16, got torch.float32

Expected layers.40.attn.output_proj.weight to be bf16, got torch.float32Expected layers.36.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.31.sa_norm.scale to be bf16, got torch.float32Expected layers.40.mlp.w3.weight to be bf16, got torch.float32
Expected layers.45.mlp.w3.weight to be bf16, got torch.float32
Expected layers.31.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.44.mlp.w3.weight to be bf16, got torch.float32

Expected layers.41.mlp.w1.weight to be bf16, got torch.float32
Expected layers.40.mlp_norm.scale to be bf16, got torch.float32

Expected layers.31.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.36.attn.v_proj.weight to be bf16, got torch.float32Expected layers.41.sa_norm.scale to be bf16, got torch.float32Expected layers.46.sa_norm.scale to be bf16, got torch.float32
Expected layers.31.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.45.sa_norm.scale to be bf16, got torch.float32
Expected layers.41.mlp.w2.weight to be bf16, got torch.float32Expected layers.40.mlp.w1.weight to be bf16, got torch.float32


Expected layers.31.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.36.attn.output_proj.weight to be bf16, got torch.float32Expected layers.41.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.46.attn.q_proj.weight to be bf16, got torch.float32Expected layers.31.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.45.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.41.mlp.w3.weight to be bf16, got torch.float32
Expected layers.40.mlp.w2.weight to be bf16, got torch.float32

Expected layers.31.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.36.mlp_norm.scale to be bf16, got torch.float32Expected layers.41.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.46.attn.k_proj.weight to be bf16, got torch.float32Expected layers.31.mlp_norm.scale to be bf16, got torch.float32
Expected layers.45.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.42.sa_norm.scale to be bf16, got torch.float32
Expected layers.40.mlp.w3.weight to be bf16, got torch.float32

Expected layers.31.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.36.mlp.w1.weight to be bf16, got torch.float32
Expected layers.41.attn.v_proj.weight to be bf16, got torch.float32Expected layers.46.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.31.mlp.w1.weight to be bf16, got torch.float32Expected layers.45.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.42.attn.q_proj.weight to be bf16, got torch.float32Expected layers.41.sa_norm.scale to be bf16, got torch.float32

Expected layers.31.mlp_norm.scale to be bf16, got torch.float32Expected layers.36.mlp.w2.weight to be bf16, got torch.float32

Expected layers.41.attn.output_proj.weight to be bf16, got torch.float32Expected layers.46.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.31.mlp.w2.weight to be bf16, got torch.float32Expected layers.41.attn.q_proj.weight to be bf16, got torch.float32Expected layers.45.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.42.attn.k_proj.weight to be bf16, got torch.float32


Expected layers.31.mlp.w1.weight to be bf16, got torch.float32Expected layers.36.mlp.w3.weight to be bf16, got torch.float32
Expected layers.46.mlp_norm.scale to be bf16, got torch.float32Expected layers.41.mlp_norm.scale to be bf16, got torch.float32

Expected layers.31.mlp.w3.weight to be bf16, got torch.float32Expected layers.42.attn.v_proj.weight to be bf16, got torch.float32Expected layers.41.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.45.mlp_norm.scale to be bf16, got torch.float32



Expected layers.31.mlp.w2.weight to be bf16, got torch.float32Expected layers.37.sa_norm.scale to be bf16, got torch.float32
Expected layers.46.mlp.w1.weight to be bf16, got torch.float32

Expected layers.41.mlp.w1.weight to be bf16, got torch.float32Expected layers.32.sa_norm.scale to be bf16, got torch.float32
Expected layers.42.attn.output_proj.weight to be bf16, got torch.float32Expected layers.41.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.45.mlp.w1.weight to be bf16, got torch.float32
Expected layers.31.mlp.w3.weight to be bf16, got torch.float32
Expected layers.37.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.46.mlp.w2.weight to be bf16, got torch.float32

Expected layers.41.mlp.w2.weight to be bf16, got torch.float32Expected layers.32.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.42.mlp_norm.scale to be bf16, got torch.float32Expected layers.41.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.45.mlp.w2.weight to be bf16, got torch.float32
Expected layers.32.sa_norm.scale to be bf16, got torch.float32
Expected layers.37.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.46.mlp.w3.weight to be bf16, got torch.float32
Expected layers.41.mlp.w3.weight to be bf16, got torch.float32Expected layers.32.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.42.mlp.w1.weight to be bf16, got torch.float32
Expected layers.41.mlp_norm.scale to be bf16, got torch.float32
Expected layers.45.mlp.w3.weight to be bf16, got torch.float32
Expected layers.32.attn.q_proj.weight to be bf16, got torch.float32


Expected layers.47.sa_norm.scale to be bf16, got torch.float32
Expected layers.37.attn.v_proj.weight to be bf16, got torch.float32Expected layers.42.sa_norm.scale to be bf16, got torch.float32Expected layers.32.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.42.mlp.w2.weight to be bf16, got torch.float32
Expected layers.41.mlp.w1.weight to be bf16, got torch.float32
Expected layers.46.sa_norm.scale to be bf16, got torch.float32
Expected layers.32.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.47.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.37.attn.output_proj.weight to be bf16, got torch.float32Expected layers.42.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.32.attn.output_proj.weight to be bf16, got torch.float32Expected layers.42.mlp.w3.weight to be bf16, got torch.float32
Expected layers.41.mlp.w2.weight to be bf16, got torch.float32
Expected layers.46.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.32.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.47.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.37.mlp_norm.scale to be bf16, got torch.float32Expected layers.42.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.32.mlp_norm.scale to be bf16, got torch.float32Expected layers.43.sa_norm.scale to be bf16, got torch.float32
Expected layers.41.mlp.w3.weight to be bf16, got torch.float32
Expected layers.46.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.32.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.47.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.37.mlp.w1.weight to be bf16, got torch.float32Expected layers.42.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.32.mlp.w1.weight to be bf16, got torch.float32Expected layers.43.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.42.sa_norm.scale to be bf16, got torch.float32
Expected layers.46.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.32.mlp_norm.scale to be bf16, got torch.float32

Expected layers.47.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.37.mlp.w2.weight to be bf16, got torch.float32Expected layers.42.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.32.mlp.w2.weight to be bf16, got torch.float32Expected layers.43.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.42.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.46.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.32.mlp.w1.weight to be bf16, got torch.float32

Expected layers.47.mlp_norm.scale to be bf16, got torch.float32

Expected layers.37.mlp.w3.weight to be bf16, got torch.float32
Expected layers.42.mlp_norm.scale to be bf16, got torch.float32Expected layers.32.mlp.w3.weight to be bf16, got torch.float32Expected layers.43.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.42.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.46.mlp_norm.scale to be bf16, got torch.float32
Expected layers.32.mlp.w2.weight to be bf16, got torch.float32
Expected layers.47.mlp.w1.weight to be bf16, got torch.float32


Expected layers.38.sa_norm.scale to be bf16, got torch.float32
Expected layers.42.mlp.w1.weight to be bf16, got torch.float32Expected layers.33.sa_norm.scale to be bf16, got torch.float32Expected layers.43.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.42.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.46.mlp.w1.weight to be bf16, got torch.float32
Expected layers.32.mlp.w3.weight to be bf16, got torch.float32
Expected layers.47.mlp.w2.weight to be bf16, got torch.float32

Expected layers.38.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.42.mlp.w2.weight to be bf16, got torch.float32
Expected layers.33.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.43.mlp_norm.scale to be bf16, got torch.float32
Expected layers.42.attn.output_proj.weight to be bf16, got torch.float32Expected layers.46.mlp.w2.weight to be bf16, got torch.float32

Expected layers.33.sa_norm.scale to be bf16, got torch.float32
Expected layers.47.mlp.w3.weight to be bf16, got torch.float32
Expected layers.38.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.42.mlp.w3.weight to be bf16, got torch.float32
Expected layers.46.mlp.w3.weight to be bf16, got torch.float32Expected layers.33.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.43.mlp.w1.weight to be bf16, got torch.float32
Expected layers.42.mlp_norm.scale to be bf16, got torch.float32

Expected layers.33.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.48.sa_norm.scale to be bf16, got torch.float32
Expected layers.38.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.43.sa_norm.scale to be bf16, got torch.float32
Expected layers.47.sa_norm.scale to be bf16, got torch.float32
Expected layers.33.attn.v_proj.weight to be bf16, got torch.float32Expected layers.43.mlp.w2.weight to be bf16, got torch.float32
Expected layers.42.mlp.w1.weight to be bf16, got torch.float32
Expected layers.33.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.48.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.38.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.43.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.47.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.33.attn.output_proj.weight to be bf16, got torch.float32Expected layers.43.mlp.w3.weight to be bf16, got torch.float32
Expected layers.42.mlp.w2.weight to be bf16, got torch.float32
Expected layers.33.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.48.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.38.mlp_norm.scale to be bf16, got torch.float32
Expected layers.43.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.47.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.33.mlp_norm.scale to be bf16, got torch.float32Expected layers.44.sa_norm.scale to be bf16, got torch.float32

Expected layers.42.mlp.w3.weight to be bf16, got torch.float32
Expected layers.33.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.48.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.38.mlp.w1.weight to be bf16, got torch.float32
Expected layers.43.attn.v_proj.weight to be bf16, got torch.float32Expected layers.47.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.33.mlp.w1.weight to be bf16, got torch.float32
Expected layers.44.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.43.sa_norm.scale to be bf16, got torch.float32
Expected layers.33.mlp_norm.scale to be bf16, got torch.float32
Expected layers.48.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.38.mlp.w2.weight to be bf16, got torch.float32
Expected layers.43.attn.output_proj.weight to be bf16, got torch.float32Expected layers.47.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.33.mlp.w2.weight to be bf16, got torch.float32
Expected layers.44.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.43.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.33.mlp.w1.weight to be bf16, got torch.float32
Expected layers.48.mlp_norm.scale to be bf16, got torch.float32

Expected layers.38.mlp.w3.weight to be bf16, got torch.float32
Expected layers.43.mlp_norm.scale to be bf16, got torch.float32Expected layers.47.mlp_norm.scale to be bf16, got torch.float32Expected layers.43.attn.k_proj.weight to be bf16, got torch.float32Expected layers.33.mlp.w3.weight to be bf16, got torch.float32
Expected layers.44.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.33.mlp.w2.weight to be bf16, got torch.float32
Expected layers.48.mlp.w1.weight to be bf16, got torch.float32


Expected layers.39.sa_norm.scale to be bf16, got torch.float32
Expected layers.43.mlp.w1.weight to be bf16, got torch.float32Expected layers.47.mlp.w1.weight to be bf16, got torch.float32Expected layers.43.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.34.sa_norm.scale to be bf16, got torch.float32
Expected layers.44.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.33.mlp.w3.weight to be bf16, got torch.float32
Expected layers.48.mlp.w2.weight to be bf16, got torch.float32

Expected layers.39.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.43.mlp.w2.weight to be bf16, got torch.float32Expected layers.34.attn.q_proj.weight to be bf16, got torch.float32Expected layers.47.mlp.w2.weight to be bf16, got torch.float32
Expected layers.43.attn.output_proj.weight to be bf16, got torch.float32


Expected layers.34.sa_norm.scale to be bf16, got torch.float32Expected layers.44.mlp_norm.scale to be bf16, got torch.float32Expected layers.48.mlp.w3.weight to be bf16, got torch.float32
Expected layers.39.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.43.mlp.w3.weight to be bf16, got torch.float32
Expected layers.34.attn.k_proj.weight to be bf16, got torch.float32Expected layers.47.mlp.w3.weight to be bf16, got torch.float32

Expected layers.43.mlp_norm.scale to be bf16, got torch.float32
Expected layers.34.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.44.mlp.w1.weight to be bf16, got torch.float32Expected layers.49.sa_norm.scale to be bf16, got torch.float32

Expected layers.39.attn.v_proj.weight to be bf16, got torch.float32Expected layers.44.sa_norm.scale to be bf16, got torch.float32

Expected layers.34.attn.v_proj.weight to be bf16, got torch.float32Expected layers.48.sa_norm.scale to be bf16, got torch.float32

Expected layers.49.attn.q_proj.weight to be bf16, got torch.float32Expected layers.43.mlp.w1.weight to be bf16, got torch.float32Expected layers.34.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.44.mlp.w2.weight to be bf16, got torch.float32


Expected layers.39.attn.output_proj.weight to be bf16, got torch.float32Expected layers.44.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.34.attn.output_proj.weight to be bf16, got torch.float32Expected layers.48.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.49.attn.k_proj.weight to be bf16, got torch.float32Expected layers.43.mlp.w2.weight to be bf16, got torch.float32
Expected layers.34.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.44.mlp.w3.weight to be bf16, got torch.float32

Expected layers.39.mlp_norm.scale to be bf16, got torch.float32
Expected layers.44.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.34.mlp_norm.scale to be bf16, got torch.float32
Expected layers.48.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.49.attn.v_proj.weight to be bf16, got torch.float32Expected layers.43.mlp.w3.weight to be bf16, got torch.float32
Expected layers.34.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.45.sa_norm.scale to be bf16, got torch.float32

Expected layers.39.mlp.w1.weight to be bf16, got torch.float32
Expected layers.44.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.34.mlp.w1.weight to be bf16, got torch.float32
Expected layers.48.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.49.attn.output_proj.weight to be bf16, got torch.float32Expected layers.44.sa_norm.scale to be bf16, got torch.float32
Expected layers.34.mlp_norm.scale to be bf16, got torch.float32
Expected layers.45.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.39.mlp.w2.weight to be bf16, got torch.float32
Expected layers.44.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.34.mlp.w2.weight to be bf16, got torch.float32
Expected layers.48.attn.output_proj.weight to be bf16, got torch.float32Expected layers.49.mlp_norm.scale to be bf16, got torch.float32
Expected layers.44.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.34.mlp.w1.weight to be bf16, got torch.float32

Expected layers.45.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.39.mlp.w3.weight to be bf16, got torch.float32
Expected layers.44.mlp_norm.scale to be bf16, got torch.float32
Expected layers.34.mlp.w3.weight to be bf16, got torch.float32
Expected layers.48.mlp_norm.scale to be bf16, got torch.float32
Expected layers.49.mlp.w1.weight to be bf16, got torch.float32Expected layers.45.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.44.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.34.mlp.w2.weight to be bf16, got torch.float32

Expected layers.40.sa_norm.scale to be bf16, got torch.float32Expected layers.48.mlp.w1.weight to be bf16, got torch.float32

Expected layers.44.mlp.w1.weight to be bf16, got torch.float32
Expected layers.35.sa_norm.scale to be bf16, got torch.float32
Expected layers.45.attn.output_proj.weight to be bf16, got torch.float32Expected layers.49.mlp.w2.weight to be bf16, got torch.float32

Expected layers.44.attn.v_proj.weight to be bf16, got torch.float32Expected layers.34.mlp.w3.weight to be bf16, got torch.float32

Expected layers.40.attn.q_proj.weight to be bf16, got torch.float32Expected layers.48.mlp.w2.weight to be bf16, got torch.float32

Expected layers.44.mlp.w2.weight to be bf16, got torch.float32Expected layers.49.mlp.w3.weight to be bf16, got torch.float32Expected layers.35.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.45.mlp_norm.scale to be bf16, got torch.float32


Expected layers.44.attn.output_proj.weight to be bf16, got torch.float32Expected layers.35.sa_norm.scale to be bf16, got torch.float32
Expected layers.40.attn.k_proj.weight to be bf16, got torch.float32Expected layers.48.mlp.w3.weight to be bf16, got torch.float32

Expected layers.50.sa_norm.scale to be bf16, got torch.float32Expected layers.44.mlp.w3.weight to be bf16, got torch.float32
Expected layers.35.attn.k_proj.weight to be bf16, got torch.float32


Expected layers.45.mlp.w1.weight to be bf16, got torch.float32Expected layers.44.mlp_norm.scale to be bf16, got torch.float32
Expected layers.35.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.40.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.50.attn.q_proj.weight to be bf16, got torch.float32Expected layers.49.sa_norm.scale to be bf16, got torch.float32
Expected layers.45.sa_norm.scale to be bf16, got torch.float32Expected layers.35.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.45.mlp.w2.weight to be bf16, got torch.float32
Expected layers.44.mlp.w1.weight to be bf16, got torch.float32Expected layers.35.attn.k_proj.weight to be bf16, got torch.float32


Expected layers.40.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.50.attn.k_proj.weight to be bf16, got torch.float32Expected layers.49.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.45.attn.q_proj.weight to be bf16, got torch.float32Expected layers.35.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.45.mlp.w3.weight to be bf16, got torch.float32
Expected layers.44.mlp.w2.weight to be bf16, got torch.float32
Expected layers.35.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.40.mlp_norm.scale to be bf16, got torch.float32
Expected layers.50.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.49.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.45.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.35.mlp_norm.scale to be bf16, got torch.float32Expected layers.46.sa_norm.scale to be bf16, got torch.float32
Expected layers.44.mlp.w3.weight to be bf16, got torch.float32
Expected layers.35.attn.output_proj.weight to be bf16, got torch.float32


Expected layers.50.attn.output_proj.weight to be bf16, got torch.float32Expected layers.40.mlp.w1.weight to be bf16, got torch.float32
Expected layers.49.attn.v_proj.weight to be bf16, got torch.float32Expected layers.45.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.35.mlp.w1.weight to be bf16, got torch.float32
Expected layers.46.attn.q_proj.weight to be bf16, got torch.float32Expected layers.45.sa_norm.scale to be bf16, got torch.float32


Expected layers.35.mlp_norm.scale to be bf16, got torch.float32

Expected layers.50.mlp_norm.scale to be bf16, got torch.float32
Expected layers.49.attn.output_proj.weight to be bf16, got torch.float32Expected layers.40.mlp.w2.weight to be bf16, got torch.float32
Expected layers.45.attn.output_proj.weight to be bf16, got torch.float32Expected layers.35.mlp.w2.weight to be bf16, got torch.float32
Expected layers.46.attn.k_proj.weight to be bf16, got torch.float32Expected layers.45.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.35.mlp.w1.weight to be bf16, got torch.float32


Expected layers.50.mlp.w1.weight to be bf16, got torch.float32

Expected layers.49.mlp_norm.scale to be bf16, got torch.float32Expected layers.40.mlp.w3.weight to be bf16, got torch.float32
Expected layers.45.mlp_norm.scale to be bf16, got torch.float32Expected layers.35.mlp.w3.weight to be bf16, got torch.float32
Expected layers.46.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.45.attn.k_proj.weight to be bf16, got torch.float32Expected layers.35.mlp.w2.weight to be bf16, got torch.float32


Expected layers.50.mlp.w2.weight to be bf16, got torch.float32

Expected layers.49.mlp.w1.weight to be bf16, got torch.float32
Expected layers.36.sa_norm.scale to be bf16, got torch.float32Expected layers.41.sa_norm.scale to be bf16, got torch.float32
Expected layers.46.attn.output_proj.weight to be bf16, got torch.float32Expected layers.45.mlp.w1.weight to be bf16, got torch.float32
Expected layers.45.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.35.mlp.w3.weight to be bf16, got torch.float32
Expected layers.50.mlp.w3.weight to be bf16, got torch.float32

Expected layers.49.mlp.w2.weight to be bf16, got torch.float32
Expected layers.36.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.46.mlp_norm.scale to be bf16, got torch.float32
Expected layers.41.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.45.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.45.mlp.w2.weight to be bf16, got torch.float32Expected layers.36.sa_norm.scale to be bf16, got torch.float32
Expected layers.51.sa_norm.scale to be bf16, got torch.float32
Expected layers.49.mlp.w3.weight to be bf16, got torch.float32

Expected layers.36.attn.k_proj.weight to be bf16, got torch.float32Expected layers.46.mlp.w1.weight to be bf16, got torch.float32


Expected layers.45.mlp_norm.scale to be bf16, got torch.float32
Expected layers.41.attn.k_proj.weight to be bf16, got torch.float32Expected layers.36.attn.q_proj.weight to be bf16, got torch.float32Expected layers.45.mlp.w3.weight to be bf16, got torch.float32
Expected layers.51.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.50.sa_norm.scale to be bf16, got torch.float32
Expected layers.36.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.46.mlp.w2.weight to be bf16, got torch.float32

Expected layers.45.mlp.w1.weight to be bf16, got torch.float32

Expected layers.36.attn.k_proj.weight to be bf16, got torch.float32Expected layers.41.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.51.attn.k_proj.weight to be bf16, got torch.float32Expected layers.46.sa_norm.scale to be bf16, got torch.float32
Expected layers.50.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.36.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.46.mlp.w3.weight to be bf16, got torch.float32

Expected layers.45.mlp.w2.weight to be bf16, got torch.float32
Expected layers.36.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.41.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.51.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.50.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.46.attn.q_proj.weight to be bf16, got torch.float32Expected layers.36.mlp_norm.scale to be bf16, got torch.float32
Expected layers.47.sa_norm.scale to be bf16, got torch.float32
Expected layers.45.mlp.w3.weight to be bf16, got torch.float32
Expected layers.36.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.41.mlp_norm.scale to be bf16, got torch.float32
Expected layers.51.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.50.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.46.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.36.mlp.w1.weight to be bf16, got torch.float32Expected layers.47.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.46.sa_norm.scale to be bf16, got torch.float32
Expected layers.36.mlp_norm.scale to be bf16, got torch.float32

Expected layers.41.mlp.w1.weight to be bf16, got torch.float32
Expected layers.46.attn.v_proj.weight to be bf16, got torch.float32Expected layers.51.mlp_norm.scale to be bf16, got torch.float32
Expected layers.50.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.36.mlp.w2.weight to be bf16, got torch.float32
Expected layers.47.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.46.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.36.mlp.w1.weight to be bf16, got torch.float32
Expected layers.46.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.41.mlp.w2.weight to be bf16, got torch.float32Expected layers.51.mlp.w1.weight to be bf16, got torch.float32
Expected layers.50.mlp_norm.scale to be bf16, got torch.float32
Expected layers.36.mlp.w3.weight to be bf16, got torch.float32

Expected layers.47.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.46.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.36.mlp.w2.weight to be bf16, got torch.float32
Expected layers.46.mlp_norm.scale to be bf16, got torch.float32
Expected layers.51.mlp.w2.weight to be bf16, got torch.float32
Expected layers.41.mlp.w3.weight to be bf16, got torch.float32Expected layers.50.mlp.w1.weight to be bf16, got torch.float32
Expected layers.37.sa_norm.scale to be bf16, got torch.float32
Expected layers.47.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.46.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.36.mlp.w3.weight to be bf16, got torch.float32
Expected layers.46.mlp.w1.weight to be bf16, got torch.float32
Expected layers.51.mlp.w3.weight to be bf16, got torch.float32Expected layers.37.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.42.sa_norm.scale to be bf16, got torch.float32
Expected layers.50.mlp.w2.weight to be bf16, got torch.float32

Expected layers.47.mlp_norm.scale to be bf16, got torch.float32
Expected layers.46.mlp.w2.weight to be bf16, got torch.float32Expected layers.46.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.37.sa_norm.scale to be bf16, got torch.float32

Expected layers.52.sa_norm.scale to be bf16, got torch.float32
Expected layers.37.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.42.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.50.mlp.w3.weight to be bf16, got torch.float32
Expected layers.47.mlp.w1.weight to be bf16, got torch.float32Expected layers.46.mlp.w3.weight to be bf16, got torch.float32
Expected layers.46.mlp_norm.scale to be bf16, got torch.float32
Expected layers.37.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.52.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.37.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.42.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.51.sa_norm.scale to be bf16, got torch.float32
Expected layers.47.mlp.w2.weight to be bf16, got torch.float32Expected layers.47.sa_norm.scale to be bf16, got torch.float32

Expected layers.46.mlp.w1.weight to be bf16, got torch.float32Expected layers.37.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.52.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.37.attn.output_proj.weight to be bf16, got torch.float32


Expected layers.42.attn.v_proj.weight to be bf16, got torch.float32Expected layers.51.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.47.mlp.w3.weight to be bf16, got torch.float32Expected layers.47.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.46.mlp.w2.weight to be bf16, got torch.float32Expected layers.37.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.52.attn.v_proj.weight to be bf16, got torch.float32Expected layers.42.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.37.mlp_norm.scale to be bf16, got torch.float32


Expected layers.51.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.48.sa_norm.scale to be bf16, got torch.float32Expected layers.47.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.46.mlp.w3.weight to be bf16, got torch.float32Expected layers.37.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.52.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.42.mlp_norm.scale to be bf16, got torch.float32
Expected layers.37.mlp.w1.weight to be bf16, got torch.float32
Expected layers.51.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.48.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.47.attn.v_proj.weight to be bf16, got torch.float32Expected layers.47.sa_norm.scale to be bf16, got torch.float32
Expected layers.37.mlp_norm.scale to be bf16, got torch.float32

Expected layers.52.mlp_norm.scale to be bf16, got torch.float32
Expected layers.42.mlp.w1.weight to be bf16, got torch.float32
Expected layers.37.mlp.w2.weight to be bf16, got torch.float32
Expected layers.51.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.48.attn.k_proj.weight to be bf16, got torch.float32Expected layers.47.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.47.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.37.mlp.w1.weight to be bf16, got torch.float32

Expected layers.52.mlp.w1.weight to be bf16, got torch.float32
Expected layers.42.mlp.w2.weight to be bf16, got torch.float32
Expected layers.37.mlp.w3.weight to be bf16, got torch.float32
Expected layers.51.mlp_norm.scale to be bf16, got torch.float32
Expected layers.48.attn.v_proj.weight to be bf16, got torch.float32Expected layers.47.mlp_norm.scale to be bf16, got torch.float32
Expected layers.47.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.37.mlp.w2.weight to be bf16, got torch.float32

Expected layers.52.mlp.w2.weight to be bf16, got torch.float32
Expected layers.42.mlp.w3.weight to be bf16, got torch.float32
Expected layers.48.attn.output_proj.weight to be bf16, got torch.float32Expected layers.38.sa_norm.scale to be bf16, got torch.float32
Expected layers.51.mlp.w1.weight to be bf16, got torch.float32

Expected layers.47.mlp.w1.weight to be bf16, got torch.float32
Expected layers.47.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.37.mlp.w3.weight to be bf16, got torch.float32
Expected layers.52.mlp.w3.weight to be bf16, got torch.float32
Expected layers.43.sa_norm.scale to be bf16, got torch.float32Expected layers.48.mlp_norm.scale to be bf16, got torch.float32
Expected layers.38.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.51.mlp.w2.weight to be bf16, got torch.float32

Expected layers.47.mlp.w2.weight to be bf16, got torch.float32
Expected layers.47.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.38.sa_norm.scale to be bf16, got torch.float32
Expected layers.53.sa_norm.scale to be bf16, got torch.float32
Expected layers.43.attn.q_proj.weight to be bf16, got torch.float32Expected layers.48.mlp.w1.weight to be bf16, got torch.float32
Expected layers.38.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.51.mlp.w3.weight to be bf16, got torch.float32
Expected layers.47.mlp.w3.weight to be bf16, got torch.float32
Expected layers.47.mlp_norm.scale to be bf16, got torch.float32
Expected layers.38.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.53.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.43.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.48.mlp.w2.weight to be bf16, got torch.float32
Expected layers.38.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.52.sa_norm.scale to be bf16, got torch.float32Expected layers.48.sa_norm.scale to be bf16, got torch.float32

Expected layers.47.mlp.w1.weight to be bf16, got torch.float32Expected layers.38.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.53.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.43.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.48.mlp.w3.weight to be bf16, got torch.float32Expected layers.48.attn.q_proj.weight to be bf16, got torch.float32Expected layers.38.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.52.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.47.mlp.w2.weight to be bf16, got torch.float32
Expected layers.38.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.53.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.43.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.49.sa_norm.scale to be bf16, got torch.float32
Expected layers.48.attn.k_proj.weight to be bf16, got torch.float32Expected layers.38.mlp_norm.scale to be bf16, got torch.float32
Expected layers.52.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.47.mlp.w3.weight to be bf16, got torch.float32

Expected layers.38.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.53.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.43.mlp_norm.scale to be bf16, got torch.float32
Expected layers.49.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.38.mlp.w1.weight to be bf16, got torch.float32Expected layers.48.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.52.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.48.sa_norm.scale to be bf16, got torch.float32

Expected layers.38.mlp_norm.scale to be bf16, got torch.float32
Expected layers.53.mlp_norm.scale to be bf16, got torch.float32
Expected layers.43.mlp.w1.weight to be bf16, got torch.float32
Expected layers.49.attn.k_proj.weight to be bf16, got torch.float32Expected layers.38.mlp.w2.weight to be bf16, got torch.float32

Expected layers.48.attn.output_proj.weight to be bf16, got torch.float32Expected layers.52.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.48.attn.q_proj.weight to be bf16, got torch.float32Expected layers.38.mlp.w1.weight to be bf16, got torch.float32

Expected layers.53.mlp.w1.weight to be bf16, got torch.float32Expected layers.43.mlp.w2.weight to be bf16, got torch.float32

Expected layers.49.attn.v_proj.weight to be bf16, got torch.float32Expected layers.38.mlp.w3.weight to be bf16, got torch.float32

Expected layers.52.mlp_norm.scale to be bf16, got torch.float32
Expected layers.48.mlp_norm.scale to be bf16, got torch.float32
Expected layers.48.attn.k_proj.weight to be bf16, got torch.float32Expected layers.38.mlp.w2.weight to be bf16, got torch.float32
Expected layers.53.mlp.w2.weight to be bf16, got torch.float32
Expected layers.43.mlp.w3.weight to be bf16, got torch.float32

Expected layers.49.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.39.sa_norm.scale to be bf16, got torch.float32
Expected layers.52.mlp.w1.weight to be bf16, got torch.float32
Expected layers.48.mlp.w1.weight to be bf16, got torch.float32
Expected layers.48.attn.v_proj.weight to be bf16, got torch.float32Expected layers.38.mlp.w3.weight to be bf16, got torch.float32
Expected layers.49.mlp_norm.scale to be bf16, got torch.float32Expected layers.53.mlp.w3.weight to be bf16, got torch.float32
Expected layers.44.sa_norm.scale to be bf16, got torch.float32Expected layers.52.mlp.w2.weight to be bf16, got torch.float32


Expected layers.39.attn.q_proj.weight to be bf16, got torch.float32


Expected layers.48.mlp.w2.weight to be bf16, got torch.float32
Expected layers.48.attn.output_proj.weight to be bf16, got torch.float32Expected layers.39.sa_norm.scale to be bf16, got torch.float32Expected layers.49.mlp.w1.weight to be bf16, got torch.float32
Expected layers.54.sa_norm.scale to be bf16, got torch.float32

Expected layers.44.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.52.mlp.w3.weight to be bf16, got torch.float32Expected layers.39.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.48.mlp.w3.weight to be bf16, got torch.float32Expected layers.48.mlp_norm.scale to be bf16, got torch.float32Expected layers.39.attn.q_proj.weight to be bf16, got torch.float32



Expected layers.49.mlp.w2.weight to be bf16, got torch.float32Expected layers.44.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.54.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.53.sa_norm.scale to be bf16, got torch.float32
Expected layers.39.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.49.sa_norm.scale to be bf16, got torch.float32Expected layers.48.mlp.w1.weight to be bf16, got torch.float32
Expected layers.39.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.49.mlp.w3.weight to be bf16, got torch.float32


Expected layers.54.attn.k_proj.weight to be bf16, got torch.float32Expected layers.44.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.53.attn.q_proj.weight to be bf16, got torch.float32Expected layers.39.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.49.attn.q_proj.weight to be bf16, got torch.float32Expected layers.48.mlp.w2.weight to be bf16, got torch.float32
Expected layers.39.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.50.sa_norm.scale to be bf16, got torch.float32

Expected layers.54.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.44.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.53.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.39.mlp_norm.scale to be bf16, got torch.float32

Expected layers.48.mlp.w3.weight to be bf16, got torch.float32Expected layers.49.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.39.attn.output_proj.weight to be bf16, got torch.float32Expected layers.50.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.54.attn.output_proj.weight to be bf16, got torch.float32


Expected layers.53.attn.v_proj.weight to be bf16, got torch.float32Expected layers.44.mlp_norm.scale to be bf16, got torch.float32
Expected layers.39.mlp.w1.weight to be bf16, got torch.float32Expected layers.49.sa_norm.scale to be bf16, got torch.float32

Expected layers.49.attn.v_proj.weight to be bf16, got torch.float32Expected layers.39.mlp_norm.scale to be bf16, got torch.float32
Expected layers.50.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.54.mlp_norm.scale to be bf16, got torch.float32

Expected layers.53.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.39.mlp.w2.weight to be bf16, got torch.float32Expected layers.44.mlp.w1.weight to be bf16, got torch.float32
Expected layers.49.attn.q_proj.weight to be bf16, got torch.float32Expected layers.49.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.39.mlp.w1.weight to be bf16, got torch.float32
Expected layers.50.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.54.mlp.w1.weight to be bf16, got torch.float32

Expected layers.53.mlp_norm.scale to be bf16, got torch.float32

Expected layers.39.mlp.w3.weight to be bf16, got torch.float32Expected layers.44.mlp.w2.weight to be bf16, got torch.float32
Expected layers.49.attn.k_proj.weight to be bf16, got torch.float32Expected layers.54.mlp.w2.weight to be bf16, got torch.float32Expected layers.49.mlp_norm.scale to be bf16, got torch.float32
Expected layers.39.mlp.w2.weight to be bf16, got torch.float32
Expected layers.50.attn.output_proj.weight to be bf16, got torch.float32


Expected layers.53.mlp.w1.weight to be bf16, got torch.float32

Expected layers.40.sa_norm.scale to be bf16, got torch.float32Expected layers.44.mlp.w3.weight to be bf16, got torch.float32
Expected layers.49.attn.v_proj.weight to be bf16, got torch.float32Expected layers.54.mlp.w3.weight to be bf16, got torch.float32

Expected layers.49.mlp.w1.weight to be bf16, got torch.float32Expected layers.39.mlp.w3.weight to be bf16, got torch.float32
Expected layers.50.mlp_norm.scale to be bf16, got torch.float32
Expected layers.53.mlp.w2.weight to be bf16, got torch.float32

Expected layers.40.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.55.sa_norm.scale to be bf16, got torch.float32Expected layers.45.sa_norm.scale to be bf16, got torch.float32
Expected layers.49.attn.output_proj.weight to be bf16, got torch.float32


Expected layers.49.mlp.w2.weight to be bf16, got torch.float32Expected layers.40.sa_norm.scale to be bf16, got torch.float32
Expected layers.50.mlp.w1.weight to be bf16, got torch.float32Expected layers.53.mlp.w3.weight to be bf16, got torch.float32

Expected layers.40.attn.k_proj.weight to be bf16, got torch.float32Expected layers.55.attn.q_proj.weight to be bf16, got torch.float32


Expected layers.49.mlp_norm.scale to be bf16, got torch.float32Expected layers.45.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.49.mlp.w3.weight to be bf16, got torch.float32Expected layers.40.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.50.mlp.w2.weight to be bf16, got torch.float32Expected layers.54.sa_norm.scale to be bf16, got torch.float32
Expected layers.40.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.55.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.49.mlp.w1.weight to be bf16, got torch.float32

Expected layers.45.attn.k_proj.weight to be bf16, got torch.float32Expected layers.50.sa_norm.scale to be bf16, got torch.float32
Expected layers.40.attn.k_proj.weight to be bf16, got torch.float32Expected layers.50.mlp.w3.weight to be bf16, got torch.float32

Expected layers.40.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.54.attn.q_proj.weight to be bf16, got torch.float32Expected layers.55.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.49.mlp.w2.weight to be bf16, got torch.float32
Expected layers.45.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.50.attn.q_proj.weight to be bf16, got torch.float32Expected layers.40.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.51.sa_norm.scale to be bf16, got torch.float32Expected layers.40.mlp_norm.scale to be bf16, got torch.float32

Expected layers.54.attn.k_proj.weight to be bf16, got torch.float32Expected layers.55.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.49.mlp.w3.weight to be bf16, got torch.float32
Expected layers.45.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.50.attn.k_proj.weight to be bf16, got torch.float32Expected layers.40.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.51.attn.q_proj.weight to be bf16, got torch.float32Expected layers.55.mlp_norm.scale to be bf16, got torch.float32Expected layers.40.mlp.w1.weight to be bf16, got torch.float32

Expected layers.54.attn.v_proj.weight to be bf16, got torch.float32


Expected layers.50.sa_norm.scale to be bf16, got torch.float32Expected layers.45.mlp_norm.scale to be bf16, got torch.float32
Expected layers.50.attn.v_proj.weight to be bf16, got torch.float32Expected layers.40.mlp_norm.scale to be bf16, got torch.float32

Expected layers.51.attn.k_proj.weight to be bf16, got torch.float32Expected layers.55.mlp.w1.weight to be bf16, got torch.float32
Expected layers.40.mlp.w2.weight to be bf16, got torch.float32
Expected layers.54.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.50.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.45.mlp.w1.weight to be bf16, got torch.float32Expected layers.50.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.40.mlp.w1.weight to be bf16, got torch.float32Expected layers.51.attn.v_proj.weight to be bf16, got torch.float32Expected layers.55.mlp.w2.weight to be bf16, got torch.float32

Expected layers.40.mlp.w3.weight to be bf16, got torch.float32Expected layers.54.mlp_norm.scale to be bf16, got torch.float32


Expected layers.50.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.45.mlp.w2.weight to be bf16, got torch.float32Expected layers.50.mlp_norm.scale to be bf16, got torch.float32
Expected layers.40.mlp.w2.weight to be bf16, got torch.float32Expected layers.51.attn.output_proj.weight to be bf16, got torch.float32Expected layers.55.mlp.w3.weight to be bf16, got torch.float32

Expected layers.41.sa_norm.scale to be bf16, got torch.float32
Expected layers.54.mlp.w1.weight to be bf16, got torch.float32

Expected layers.50.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.45.mlp.w3.weight to be bf16, got torch.float32Expected layers.50.mlp.w1.weight to be bf16, got torch.float32
Expected layers.40.mlp.w3.weight to be bf16, got torch.float32Expected layers.51.mlp_norm.scale to be bf16, got torch.float32
Expected layers.56.sa_norm.scale to be bf16, got torch.float32
Expected layers.41.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.54.mlp.w2.weight to be bf16, got torch.float32

Expected layers.50.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.46.sa_norm.scale to be bf16, got torch.float32
Expected layers.50.mlp.w2.weight to be bf16, got torch.float32
Expected layers.41.sa_norm.scale to be bf16, got torch.float32
Expected layers.51.mlp.w1.weight to be bf16, got torch.float32Expected layers.56.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.41.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.54.mlp.w3.weight to be bf16, got torch.float32
Expected layers.50.mlp_norm.scale to be bf16, got torch.float32
Expected layers.46.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.50.mlp.w3.weight to be bf16, got torch.float32
Expected layers.41.attn.q_proj.weight to be bf16, got torch.float32Expected layers.51.mlp.w2.weight to be bf16, got torch.float32
Expected layers.56.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.41.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.55.sa_norm.scale to be bf16, got torch.float32
Expected layers.50.mlp.w1.weight to be bf16, got torch.float32
Expected layers.46.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.51.sa_norm.scale to be bf16, got torch.float32
Expected layers.41.attn.k_proj.weight to be bf16, got torch.float32Expected layers.51.mlp.w3.weight to be bf16, got torch.float32
Expected layers.56.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.41.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.55.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.50.mlp.w2.weight to be bf16, got torch.float32
Expected layers.46.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.51.attn.q_proj.weight to be bf16, got torch.float32Expected layers.41.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.52.sa_norm.scale to be bf16, got torch.float32
Expected layers.56.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.41.mlp_norm.scale to be bf16, got torch.float32
Expected layers.55.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.50.mlp.w3.weight to be bf16, got torch.float32
Expected layers.46.attn.output_proj.weight to be bf16, got torch.float32Expected layers.51.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.41.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.52.attn.q_proj.weight to be bf16, got torch.float32


Expected layers.56.mlp_norm.scale to be bf16, got torch.float32Expected layers.41.mlp.w1.weight to be bf16, got torch.float32
Expected layers.55.attn.v_proj.weight to be bf16, got torch.float32Expected layers.51.sa_norm.scale to be bf16, got torch.float32

Expected layers.46.mlp_norm.scale to be bf16, got torch.float32Expected layers.51.attn.v_proj.weight to be bf16, got torch.float32Expected layers.41.mlp_norm.scale to be bf16, got torch.float32

Expected layers.52.attn.k_proj.weight to be bf16, got torch.float32


Expected layers.56.mlp.w1.weight to be bf16, got torch.float32Expected layers.41.mlp.w2.weight to be bf16, got torch.float32
Expected layers.55.attn.output_proj.weight to be bf16, got torch.float32Expected layers.51.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.46.mlp.w1.weight to be bf16, got torch.float32
Expected layers.51.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.41.mlp.w1.weight to be bf16, got torch.float32

Expected layers.52.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.56.mlp.w2.weight to be bf16, got torch.float32
Expected layers.41.mlp.w3.weight to be bf16, got torch.float32
Expected layers.55.mlp_norm.scale to be bf16, got torch.float32
Expected layers.51.attn.k_proj.weight to be bf16, got torch.float32Expected layers.46.mlp.w2.weight to be bf16, got torch.float32
Expected layers.51.mlp_norm.scale to be bf16, got torch.float32
Expected layers.41.mlp.w2.weight to be bf16, got torch.float32

Expected layers.52.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.56.mlp.w3.weight to be bf16, got torch.float32
Expected layers.42.sa_norm.scale to be bf16, got torch.float32
Expected layers.55.mlp.w1.weight to be bf16, got torch.float32
Expected layers.51.attn.v_proj.weight to be bf16, got torch.float32Expected layers.46.mlp.w3.weight to be bf16, got torch.float32
Expected layers.51.mlp.w1.weight to be bf16, got torch.float32
Expected layers.41.mlp.w3.weight to be bf16, got torch.float32

Expected layers.52.mlp_norm.scale to be bf16, got torch.float32Expected layers.57.sa_norm.scale to be bf16, got torch.float32

Expected layers.42.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.55.mlp.w2.weight to be bf16, got torch.float32
Expected layers.51.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.47.sa_norm.scale to be bf16, got torch.float32Expected layers.51.mlp.w2.weight to be bf16, got torch.float32Expected layers.42.sa_norm.scale to be bf16, got torch.float32
Expected layers.52.mlp.w1.weight to be bf16, got torch.float32
Expected layers.57.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.42.attn.k_proj.weight to be bf16, got torch.float32Expected layers.55.mlp.w3.weight to be bf16, got torch.float32

Expected layers.51.mlp_norm.scale to be bf16, got torch.float32

Expected layers.47.attn.q_proj.weight to be bf16, got torch.float32Expected layers.51.mlp.w3.weight to be bf16, got torch.float32Expected layers.42.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.52.mlp.w2.weight to be bf16, got torch.float32Expected layers.57.attn.k_proj.weight to be bf16, got torch.float32


Expected layers.42.attn.v_proj.weight to be bf16, got torch.float32Expected layers.56.sa_norm.scale to be bf16, got torch.float32

Expected layers.51.mlp.w1.weight to be bf16, got torch.float32

Expected layers.47.attn.k_proj.weight to be bf16, got torch.float32Expected layers.52.sa_norm.scale to be bf16, got torch.float32Expected layers.42.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.52.mlp.w3.weight to be bf16, got torch.float32
Expected layers.57.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.42.attn.output_proj.weight to be bf16, got torch.float32Expected layers.56.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.51.mlp.w2.weight to be bf16, got torch.float32
Expected layers.47.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.52.attn.q_proj.weight to be bf16, got torch.float32Expected layers.42.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.53.sa_norm.scale to be bf16, got torch.float32Expected layers.57.attn.output_proj.weight to be bf16, got torch.float32Expected layers.56.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.42.mlp_norm.scale to be bf16, got torch.float32

Expected layers.51.mlp.w3.weight to be bf16, got torch.float32

Expected layers.47.attn.output_proj.weight to be bf16, got torch.float32Expected layers.52.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.42.attn.output_proj.weight to be bf16, got torch.float32Expected layers.53.attn.q_proj.weight to be bf16, got torch.float32Expected layers.57.mlp_norm.scale to be bf16, got torch.float32

Expected layers.56.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.42.mlp.w1.weight to be bf16, got torch.float32

Expected layers.52.sa_norm.scale to be bf16, got torch.float32

Expected layers.47.mlp_norm.scale to be bf16, got torch.float32Expected layers.52.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.42.mlp_norm.scale to be bf16, got torch.float32Expected layers.53.attn.k_proj.weight to be bf16, got torch.float32Expected layers.57.mlp.w1.weight to be bf16, got torch.float32

Expected layers.56.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.42.mlp.w2.weight to be bf16, got torch.float32

Expected layers.52.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.47.mlp.w1.weight to be bf16, got torch.float32
Expected layers.52.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.42.mlp.w1.weight to be bf16, got torch.float32
Expected layers.53.attn.v_proj.weight to be bf16, got torch.float32Expected layers.57.mlp.w2.weight to be bf16, got torch.float32
Expected layers.56.mlp_norm.scale to be bf16, got torch.float32
Expected layers.42.mlp.w3.weight to be bf16, got torch.float32

Expected layers.52.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.47.mlp.w2.weight to be bf16, got torch.float32
Expected layers.52.mlp_norm.scale to be bf16, got torch.float32
Expected layers.42.mlp.w2.weight to be bf16, got torch.float32Expected layers.53.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.57.mlp.w3.weight to be bf16, got torch.float32
Expected layers.56.mlp.w1.weight to be bf16, got torch.float32

Expected layers.43.sa_norm.scale to be bf16, got torch.float32
Expected layers.52.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.52.mlp.w1.weight to be bf16, got torch.float32Expected layers.47.mlp.w3.weight to be bf16, got torch.float32
Expected layers.42.mlp.w3.weight to be bf16, got torch.float32Expected layers.53.mlp_norm.scale to be bf16, got torch.float32Expected layers.58.sa_norm.scale to be bf16, got torch.float32
Expected layers.56.mlp.w2.weight to be bf16, got torch.float32
Expected layers.43.attn.q_proj.weight to be bf16, got torch.float32



Expected layers.52.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.52.mlp.w2.weight to be bf16, got torch.float32
Expected layers.43.sa_norm.scale to be bf16, got torch.float32Expected layers.53.mlp.w1.weight to be bf16, got torch.float32
Expected layers.58.attn.q_proj.weight to be bf16, got torch.float32Expected layers.48.sa_norm.scale to be bf16, got torch.float32Expected layers.56.mlp.w3.weight to be bf16, got torch.float32

Expected layers.43.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.52.mlp_norm.scale to be bf16, got torch.float32

Expected layers.52.mlp.w3.weight to be bf16, got torch.float32

Expected layers.43.attn.q_proj.weight to be bf16, got torch.float32Expected layers.53.mlp.w2.weight to be bf16, got torch.float32
Expected layers.58.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.57.sa_norm.scale to be bf16, got torch.float32
Expected layers.43.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.48.attn.q_proj.weight to be bf16, got torch.float32Expected layers.52.mlp.w1.weight to be bf16, got torch.float32
Expected layers.53.sa_norm.scale to be bf16, got torch.float32
Expected layers.43.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.53.mlp.w3.weight to be bf16, got torch.float32
Expected layers.58.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.57.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.43.attn.output_proj.weight to be bf16, got torch.float32Expected layers.48.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.52.mlp.w2.weight to be bf16, got torch.float32

Expected layers.53.attn.q_proj.weight to be bf16, got torch.float32Expected layers.43.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.54.sa_norm.scale to be bf16, got torch.float32
Expected layers.58.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.57.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.43.mlp_norm.scale to be bf16, got torch.float32
Expected layers.48.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.52.mlp.w3.weight to be bf16, got torch.float32
Expected layers.53.attn.k_proj.weight to be bf16, got torch.float32Expected layers.43.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.54.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.58.mlp_norm.scale to be bf16, got torch.float32

Expected layers.57.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.43.mlp.w1.weight to be bf16, got torch.float32
Expected layers.48.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.53.sa_norm.scale to be bf16, got torch.float32
Expected layers.53.attn.v_proj.weight to be bf16, got torch.float32Expected layers.43.mlp_norm.scale to be bf16, got torch.float32
Expected layers.54.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.58.mlp.w1.weight to be bf16, got torch.float32

Expected layers.57.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.43.mlp.w2.weight to be bf16, got torch.float32
Expected layers.48.mlp_norm.scale to be bf16, got torch.float32
Expected layers.53.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.53.attn.output_proj.weight to be bf16, got torch.float32Expected layers.43.mlp.w1.weight to be bf16, got torch.float32
Expected layers.54.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.58.mlp.w2.weight to be bf16, got torch.float32

Expected layers.57.mlp_norm.scale to be bf16, got torch.float32
Expected layers.43.mlp.w3.weight to be bf16, got torch.float32
Expected layers.48.mlp.w1.weight to be bf16, got torch.float32
Expected layers.53.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.53.mlp_norm.scale to be bf16, got torch.float32Expected layers.43.mlp.w2.weight to be bf16, got torch.float32
Expected layers.54.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.58.mlp.w3.weight to be bf16, got torch.float32

Expected layers.57.mlp.w1.weight to be bf16, got torch.float32
Expected layers.44.sa_norm.scale to be bf16, got torch.float32
Expected layers.48.mlp.w2.weight to be bf16, got torch.float32Expected layers.53.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.53.mlp.w1.weight to be bf16, got torch.float32Expected layers.43.mlp.w3.weight to be bf16, got torch.float32

Expected layers.54.mlp_norm.scale to be bf16, got torch.float32Expected layers.59.sa_norm.scale to be bf16, got torch.float32

Expected layers.57.mlp.w2.weight to be bf16, got torch.float32Expected layers.44.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.53.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.48.mlp.w3.weight to be bf16, got torch.float32Expected layers.53.mlp.w2.weight to be bf16, got torch.float32Expected layers.44.sa_norm.scale to be bf16, got torch.float32
Expected layers.54.mlp.w1.weight to be bf16, got torch.float32Expected layers.59.attn.q_proj.weight to be bf16, got torch.float32


Expected layers.57.mlp.w3.weight to be bf16, got torch.float32Expected layers.44.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.53.mlp_norm.scale to be bf16, got torch.float32

Expected layers.53.mlp.w3.weight to be bf16, got torch.float32Expected layers.44.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.49.sa_norm.scale to be bf16, got torch.float32Expected layers.54.mlp.w2.weight to be bf16, got torch.float32
Expected layers.59.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.58.sa_norm.scale to be bf16, got torch.float32Expected layers.44.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.53.mlp.w1.weight to be bf16, got torch.float32


Expected layers.54.sa_norm.scale to be bf16, got torch.float32Expected layers.44.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.49.attn.q_proj.weight to be bf16, got torch.float32Expected layers.54.mlp.w3.weight to be bf16, got torch.float32

Expected layers.59.attn.v_proj.weight to be bf16, got torch.float32Expected layers.58.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.44.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.53.mlp.w2.weight to be bf16, got torch.float32


Expected layers.44.attn.v_proj.weight to be bf16, got torch.float32Expected layers.54.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.49.attn.k_proj.weight to be bf16, got torch.float32Expected layers.55.sa_norm.scale to be bf16, got torch.float32
Expected layers.59.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.58.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.44.mlp_norm.scale to be bf16, got torch.float32

Expected layers.53.mlp.w3.weight to be bf16, got torch.float32
Expected layers.44.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.54.attn.k_proj.weight to be bf16, got torch.float32Expected layers.49.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.55.attn.q_proj.weight to be bf16, got torch.float32Expected layers.59.mlp_norm.scale to be bf16, got torch.float32
Expected layers.58.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.44.mlp.w1.weight to be bf16, got torch.float32
Expected layers.54.sa_norm.scale to be bf16, got torch.float32

Expected layers.44.mlp_norm.scale to be bf16, got torch.float32
Expected layers.55.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.49.attn.output_proj.weight to be bf16, got torch.float32Expected layers.58.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.54.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.59.mlp.w1.weight to be bf16, got torch.float32

Expected layers.44.mlp.w2.weight to be bf16, got torch.float32Expected layers.54.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.44.mlp.w1.weight to be bf16, got torch.float32Expected layers.55.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.58.mlp_norm.scale to be bf16, got torch.float32

Expected layers.49.mlp_norm.scale to be bf16, got torch.float32Expected layers.59.mlp.w2.weight to be bf16, got torch.float32
Expected layers.44.mlp.w3.weight to be bf16, got torch.float32Expected layers.54.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.54.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.44.mlp.w2.weight to be bf16, got torch.float32Expected layers.55.attn.output_proj.weight to be bf16, got torch.float32


Expected layers.58.mlp.w1.weight to be bf16, got torch.float32

Expected layers.49.mlp.w1.weight to be bf16, got torch.float32Expected layers.59.mlp.w3.weight to be bf16, got torch.float32
Expected layers.45.sa_norm.scale to be bf16, got torch.float32
Expected layers.54.mlp_norm.scale to be bf16, got torch.float32Expected layers.54.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.44.mlp.w3.weight to be bf16, got torch.float32
Expected layers.55.mlp_norm.scale to be bf16, got torch.float32
Expected layers.58.mlp.w2.weight to be bf16, got torch.float32
Expected layers.49.mlp.w2.weight to be bf16, got torch.float32

Expected layers.60.sa_norm.scale to be bf16, got torch.float32
Expected layers.45.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.54.mlp.w1.weight to be bf16, got torch.float32
Expected layers.54.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.45.sa_norm.scale to be bf16, got torch.float32Expected layers.55.mlp.w1.weight to be bf16, got torch.float32
Expected layers.58.mlp.w3.weight to be bf16, got torch.float32Expected layers.45.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.49.mlp.w3.weight to be bf16, got torch.float32
Expected layers.60.attn.q_proj.weight to be bf16, got torch.float32




Expected layers.54.mlp.w2.weight to be bf16, got torch.float32Expected layers.54.mlp_norm.scale to be bf16, got torch.float32Expected layers.45.attn.q_proj.weight to be bf16, got torch.float32Expected layers.55.mlp.w2.weight to be bf16, got torch.float32
Expected layers.59.sa_norm.scale to be bf16, got torch.float32
Expected layers.45.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.50.sa_norm.scale to be bf16, got torch.float32
Expected layers.60.attn.k_proj.weight to be bf16, got torch.float32Expected layers.54.mlp.w1.weight to be bf16, got torch.float32


Expected layers.54.mlp.w3.weight to be bf16, got torch.float32

Expected layers.45.attn.k_proj.weight to be bf16, got torch.float32Expected layers.55.mlp.w3.weight to be bf16, got torch.float32
Expected layers.59.attn.q_proj.weight to be bf16, got torch.float32Expected layers.45.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.50.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.60.attn.v_proj.weight to be bf16, got torch.float32Expected layers.54.mlp.w2.weight to be bf16, got torch.float32

Expected layers.56.sa_norm.scale to be bf16, got torch.float32
Expected layers.55.sa_norm.scale to be bf16, got torch.float32Expected layers.45.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.59.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.45.mlp_norm.scale to be bf16, got torch.float32
Expected layers.50.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.60.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.54.mlp.w3.weight to be bf16, got torch.float32
Expected layers.56.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.55.attn.q_proj.weight to be bf16, got torch.float32Expected layers.45.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.59.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.45.mlp.w1.weight to be bf16, got torch.float32

Expected layers.50.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.60.mlp_norm.scale to be bf16, got torch.float32
Expected layers.55.sa_norm.scale to be bf16, got torch.float32
Expected layers.56.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.45.mlp_norm.scale to be bf16, got torch.float32Expected layers.55.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.59.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.45.mlp.w2.weight to be bf16, got torch.float32

Expected layers.50.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.60.mlp.w1.weight to be bf16, got torch.float32
Expected layers.55.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.56.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.45.mlp.w1.weight to be bf16, got torch.float32
Expected layers.55.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.59.mlp_norm.scale to be bf16, got torch.float32Expected layers.45.mlp.w3.weight to be bf16, got torch.float32

Expected layers.60.mlp.w2.weight to be bf16, got torch.float32Expected layers.50.mlp_norm.scale to be bf16, got torch.float32

Expected layers.55.attn.k_proj.weight to be bf16, got torch.float32Expected layers.56.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.45.mlp.w2.weight to be bf16, got torch.float32Expected layers.55.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.59.mlp.w1.weight to be bf16, got torch.float32Expected layers.46.sa_norm.scale to be bf16, got torch.float32

Expected layers.60.mlp.w3.weight to be bf16, got torch.float32Expected layers.50.mlp.w1.weight to be bf16, got torch.float32

Expected layers.55.attn.v_proj.weight to be bf16, got torch.float32Expected layers.56.mlp_norm.scale to be bf16, got torch.float32

Expected layers.45.mlp.w3.weight to be bf16, got torch.float32Expected layers.55.mlp_norm.scale to be bf16, got torch.float32

Expected layers.59.mlp.w2.weight to be bf16, got torch.float32Expected layers.46.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.61.sa_norm.scale to be bf16, got torch.float32Expected layers.50.mlp.w2.weight to be bf16, got torch.float32

Expected layers.55.attn.output_proj.weight to be bf16, got torch.float32Expected layers.56.mlp.w1.weight to be bf16, got torch.float32

Expected layers.46.sa_norm.scale to be bf16, got torch.float32
Expected layers.55.mlp.w1.weight to be bf16, got torch.float32
Expected layers.59.mlp.w3.weight to be bf16, got torch.float32
Expected layers.46.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.61.attn.q_proj.weight to be bf16, got torch.float32Expected layers.50.mlp.w3.weight to be bf16, got torch.float32
Expected layers.55.mlp_norm.scale to be bf16, got torch.float32
Expected layers.56.mlp.w2.weight to be bf16, got torch.float32

Expected layers.46.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.55.mlp.w2.weight to be bf16, got torch.float32
Expected layers.60.sa_norm.scale to be bf16, got torch.float32
Expected layers.46.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.61.attn.k_proj.weight to be bf16, got torch.float32Expected layers.51.sa_norm.scale to be bf16, got torch.float32
Expected layers.55.mlp.w1.weight to be bf16, got torch.float32
Expected layers.56.mlp.w3.weight to be bf16, got torch.float32

Expected layers.46.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.60.attn.q_proj.weight to be bf16, got torch.float32Expected layers.55.mlp.w3.weight to be bf16, got torch.float32
Expected layers.46.attn.output_proj.weight to be bf16, got torch.float32Expected layers.61.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.51.attn.q_proj.weight to be bf16, got torch.float32Expected layers.55.mlp.w2.weight to be bf16, got torch.float32
Expected layers.57.sa_norm.scale to be bf16, got torch.float32
Expected layers.46.attn.v_proj.weight to be bf16, got torch.float32


Expected layers.60.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.56.sa_norm.scale to be bf16, got torch.float32Expected layers.46.mlp_norm.scale to be bf16, got torch.float32Expected layers.61.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.51.attn.k_proj.weight to be bf16, got torch.float32Expected layers.55.mlp.w3.weight to be bf16, got torch.float32
Expected layers.57.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.46.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.60.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.56.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.46.mlp.w1.weight to be bf16, got torch.float32
Expected layers.61.mlp_norm.scale to be bf16, got torch.float32
Expected layers.51.attn.v_proj.weight to be bf16, got torch.float32Expected layers.56.sa_norm.scale to be bf16, got torch.float32
Expected layers.57.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.46.mlp_norm.scale to be bf16, got torch.float32

Expected layers.60.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.56.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.46.mlp.w2.weight to be bf16, got torch.float32
Expected layers.57.attn.v_proj.weight to be bf16, got torch.float32Expected layers.61.mlp.w1.weight to be bf16, got torch.float32
Expected layers.51.attn.output_proj.weight to be bf16, got torch.float32Expected layers.56.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.46.mlp.w1.weight to be bf16, got torch.float32
Expected layers.60.mlp_norm.scale to be bf16, got torch.float32


Expected layers.56.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.46.mlp.w3.weight to be bf16, got torch.float32Expected layers.57.attn.output_proj.weight to be bf16, got torch.float32Expected layers.61.mlp.w2.weight to be bf16, got torch.float32
Expected layers.51.mlp_norm.scale to be bf16, got torch.float32
Expected layers.56.attn.k_proj.weight to be bf16, got torch.float32Expected layers.46.mlp.w2.weight to be bf16, got torch.float32
Expected layers.60.mlp.w1.weight to be bf16, got torch.float32




Expected layers.56.attn.output_proj.weight to be bf16, got torch.float32Expected layers.47.sa_norm.scale to be bf16, got torch.float32Expected layers.57.mlp_norm.scale to be bf16, got torch.float32Expected layers.61.mlp.w3.weight to be bf16, got torch.float32

Expected layers.51.mlp.w1.weight to be bf16, got torch.float32
Expected layers.56.attn.v_proj.weight to be bf16, got torch.float32Expected layers.46.mlp.w3.weight to be bf16, got torch.float32
Expected layers.60.mlp.w2.weight to be bf16, got torch.float32


Expected layers.56.mlp_norm.scale to be bf16, got torch.float32Expected layers.47.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.57.mlp.w1.weight to be bf16, got torch.float32Expected layers.62.sa_norm.scale to be bf16, got torch.float32

Expected layers.51.mlp.w2.weight to be bf16, got torch.float32Expected layers.56.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.47.sa_norm.scale to be bf16, got torch.float32
Expected layers.60.mlp.w3.weight to be bf16, got torch.float32


Expected layers.47.attn.k_proj.weight to be bf16, got torch.float32Expected layers.56.mlp.w1.weight to be bf16, got torch.float32
Expected layers.57.mlp.w2.weight to be bf16, got torch.float32Expected layers.62.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.51.mlp.w3.weight to be bf16, got torch.float32Expected layers.56.mlp_norm.scale to be bf16, got torch.float32
Expected layers.47.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.61.sa_norm.scale to be bf16, got torch.float32

Expected layers.47.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.56.mlp.w2.weight to be bf16, got torch.float32
Expected layers.57.mlp.w3.weight to be bf16, got torch.float32
Expected layers.62.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.56.mlp.w1.weight to be bf16, got torch.float32
Expected layers.52.sa_norm.scale to be bf16, got torch.float32Expected layers.47.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.61.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.47.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.56.mlp.w3.weight to be bf16, got torch.float32
Expected layers.58.sa_norm.scale to be bf16, got torch.float32
Expected layers.62.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.56.mlp.w2.weight to be bf16, got torch.float32
Expected layers.52.attn.q_proj.weight to be bf16, got torch.float32Expected layers.47.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.61.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.47.mlp_norm.scale to be bf16, got torch.float32


Expected layers.57.sa_norm.scale to be bf16, got torch.float32Expected layers.58.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.62.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.56.mlp.w3.weight to be bf16, got torch.float32
Expected layers.52.attn.k_proj.weight to be bf16, got torch.float32Expected layers.47.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.61.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.47.mlp.w1.weight to be bf16, got torch.float32

Expected layers.57.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.58.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.62.mlp_norm.scale to be bf16, got torch.float32
Expected layers.57.sa_norm.scale to be bf16, got torch.float32

Expected layers.47.mlp_norm.scale to be bf16, got torch.float32Expected layers.52.attn.v_proj.weight to be bf16, got torch.float32Expected layers.61.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.47.mlp.w2.weight to be bf16, got torch.float32

Expected layers.57.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.58.attn.v_proj.weight to be bf16, got torch.float32Expected layers.62.mlp.w1.weight to be bf16, got torch.float32
Expected layers.57.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.47.mlp.w1.weight to be bf16, got torch.float32

Expected layers.61.mlp_norm.scale to be bf16, got torch.float32
Expected layers.52.attn.output_proj.weight to be bf16, got torch.float32Expected layers.47.mlp.w3.weight to be bf16, got torch.float32
Expected layers.57.attn.v_proj.weight to be bf16, got torch.float32Expected layers.58.attn.output_proj.weight to be bf16, got torch.float32Expected layers.62.mlp.w2.weight to be bf16, got torch.float32

Expected layers.57.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.47.mlp.w2.weight to be bf16, got torch.float32

Expected layers.61.mlp.w1.weight to be bf16, got torch.float32

Expected layers.48.sa_norm.scale to be bf16, got torch.float32
Expected layers.52.mlp_norm.scale to be bf16, got torch.float32Expected layers.57.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.58.mlp_norm.scale to be bf16, got torch.float32Expected layers.62.mlp.w3.weight to be bf16, got torch.float32
Expected layers.57.attn.v_proj.weight to be bf16, got torch.float32Expected layers.47.mlp.w3.weight to be bf16, got torch.float32
Expected layers.61.mlp.w2.weight to be bf16, got torch.float32


Expected layers.48.attn.q_proj.weight to be bf16, got torch.float32


Expected layers.57.mlp_norm.scale to be bf16, got torch.float32Expected layers.58.mlp.w1.weight to be bf16, got torch.float32Expected layers.63.sa_norm.scale to be bf16, got torch.float32Expected layers.52.mlp.w1.weight to be bf16, got torch.float32Expected layers.57.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.48.sa_norm.scale to be bf16, got torch.float32
Expected layers.61.mlp.w3.weight to be bf16, got torch.float32Expected layers.48.attn.k_proj.weight to be bf16, got torch.float32




Expected layers.57.mlp.w1.weight to be bf16, got torch.float32Expected layers.58.mlp.w2.weight to be bf16, got torch.float32Expected layers.63.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.57.mlp_norm.scale to be bf16, got torch.float32
Expected layers.48.attn.q_proj.weight to be bf16, got torch.float32Expected layers.62.sa_norm.scale to be bf16, got torch.float32Expected layers.52.mlp.w2.weight to be bf16, got torch.float32Expected layers.48.attn.v_proj.weight to be bf16, got torch.float32


Expected layers.57.mlp.w2.weight to be bf16, got torch.float32Expected layers.58.mlp.w3.weight to be bf16, got torch.float32

Expected layers.63.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.57.mlp.w1.weight to be bf16, got torch.float32Expected layers.48.attn.k_proj.weight to be bf16, got torch.float32Expected layers.62.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.52.mlp.w3.weight to be bf16, got torch.float32
Expected layers.48.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.57.mlp.w3.weight to be bf16, got torch.float32Expected layers.59.sa_norm.scale to be bf16, got torch.float32

Expected layers.63.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.57.mlp.w2.weight to be bf16, got torch.float32
Expected layers.48.attn.v_proj.weight to be bf16, got torch.float32Expected layers.62.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.53.sa_norm.scale to be bf16, got torch.float32Expected layers.48.mlp_norm.scale to be bf16, got torch.float32

Expected layers.58.sa_norm.scale to be bf16, got torch.float32Expected layers.59.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.63.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.57.mlp.w3.weight to be bf16, got torch.float32
Expected layers.48.attn.output_proj.weight to be bf16, got torch.float32Expected layers.62.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.53.attn.q_proj.weight to be bf16, got torch.float32Expected layers.48.mlp.w1.weight to be bf16, got torch.float32

Expected layers.58.attn.q_proj.weight to be bf16, got torch.float32Expected layers.59.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.63.mlp_norm.scale to be bf16, got torch.float32Expected layers.58.sa_norm.scale to be bf16, got torch.float32

Expected layers.48.mlp_norm.scale to be bf16, got torch.float32Expected layers.62.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.48.mlp.w2.weight to be bf16, got torch.float32Expected layers.53.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.58.attn.k_proj.weight to be bf16, got torch.float32Expected layers.59.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.63.mlp.w1.weight to be bf16, got torch.float32Expected layers.58.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.48.mlp.w1.weight to be bf16, got torch.float32Expected layers.62.mlp_norm.scale to be bf16, got torch.float32

Expected layers.48.mlp.w3.weight to be bf16, got torch.float32Expected layers.53.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.59.attn.output_proj.weight to be bf16, got torch.float32Expected layers.58.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.63.mlp.w2.weight to be bf16, got torch.float32Expected layers.58.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.48.mlp.w2.weight to be bf16, got torch.float32Expected layers.62.mlp.w1.weight to be bf16, got torch.float32

Expected layers.49.sa_norm.scale to be bf16, got torch.float32Expected layers.53.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.59.mlp_norm.scale to be bf16, got torch.float32Expected layers.58.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.63.mlp.w3.weight to be bf16, got torch.float32Expected layers.58.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.48.mlp.w3.weight to be bf16, got torch.float32Expected layers.62.mlp.w2.weight to be bf16, got torch.float32

Expected layers.49.attn.q_proj.weight to be bf16, got torch.float32Expected layers.53.mlp_norm.scale to be bf16, got torch.float32

Expected layers.59.mlp.w1.weight to be bf16, got torch.float32Expected layers.58.mlp_norm.scale to be bf16, got torch.float32

Expected layers.64.sa_norm.scale to be bf16, got torch.float32Expected layers.58.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.49.sa_norm.scale to be bf16, got torch.float32Expected layers.62.mlp.w3.weight to be bf16, got torch.float32

Expected layers.49.attn.k_proj.weight to be bf16, got torch.float32Expected layers.53.mlp.w1.weight to be bf16, got torch.float32

Expected layers.59.mlp.w2.weight to be bf16, got torch.float32
Expected layers.58.mlp.w1.weight to be bf16, got torch.float32
Expected layers.64.attn.q_proj.weight to be bf16, got torch.float32Expected layers.58.mlp_norm.scale to be bf16, got torch.float32

Expected layers.49.attn.q_proj.weight to be bf16, got torch.float32Expected layers.63.sa_norm.scale to be bf16, got torch.float32

Expected layers.49.attn.v_proj.weight to be bf16, got torch.float32Expected layers.53.mlp.w2.weight to be bf16, got torch.float32

Expected layers.59.mlp.w3.weight to be bf16, got torch.float32
Expected layers.58.mlp.w2.weight to be bf16, got torch.float32
Expected layers.64.attn.k_proj.weight to be bf16, got torch.float32Expected layers.58.mlp.w1.weight to be bf16, got torch.float32

Expected layers.49.attn.k_proj.weight to be bf16, got torch.float32Expected layers.63.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.49.attn.output_proj.weight to be bf16, got torch.float32Expected layers.53.mlp.w3.weight to be bf16, got torch.float32

Expected layers.60.sa_norm.scale to be bf16, got torch.float32Expected layers.58.mlp.w3.weight to be bf16, got torch.float32

Expected layers.64.attn.v_proj.weight to be bf16, got torch.float32Expected layers.58.mlp.w2.weight to be bf16, got torch.float32

Expected layers.49.attn.v_proj.weight to be bf16, got torch.float32Expected layers.63.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.49.mlp_norm.scale to be bf16, got torch.float32Expected layers.54.sa_norm.scale to be bf16, got torch.float32

Expected layers.60.attn.q_proj.weight to be bf16, got torch.float32Expected layers.59.sa_norm.scale to be bf16, got torch.float32

Expected layers.64.attn.output_proj.weight to be bf16, got torch.float32Expected layers.58.mlp.w3.weight to be bf16, got torch.float32

Expected layers.49.attn.output_proj.weight to be bf16, got torch.float32Expected layers.63.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.49.mlp.w1.weight to be bf16, got torch.float32Expected layers.54.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.60.attn.k_proj.weight to be bf16, got torch.float32Expected layers.59.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.64.mlp_norm.scale to be bf16, got torch.float32
Expected layers.59.sa_norm.scale to be bf16, got torch.float32
Expected layers.49.mlp_norm.scale to be bf16, got torch.float32
Expected layers.63.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.49.mlp.w2.weight to be bf16, got torch.float32
Expected layers.54.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.60.attn.v_proj.weight to be bf16, got torch.float32Expected layers.59.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.64.mlp.w1.weight to be bf16, got torch.float32
Expected layers.59.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.49.mlp.w1.weight to be bf16, got torch.float32Expected layers.63.mlp_norm.scale to be bf16, got torch.float32

Expected layers.49.mlp.w3.weight to be bf16, got torch.float32
Expected layers.54.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.60.attn.output_proj.weight to be bf16, got torch.float32Expected layers.59.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.64.mlp.w2.weight to be bf16, got torch.float32Expected layers.59.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.49.mlp.w2.weight to be bf16, got torch.float32Expected layers.63.mlp.w1.weight to be bf16, got torch.float32

Expected layers.50.sa_norm.scale to be bf16, got torch.float32Expected layers.54.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.60.mlp_norm.scale to be bf16, got torch.float32Expected layers.59.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.64.mlp.w3.weight to be bf16, got torch.float32
Expected layers.59.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.49.mlp.w3.weight to be bf16, got torch.float32Expected layers.63.mlp.w2.weight to be bf16, got torch.float32

Expected layers.50.attn.q_proj.weight to be bf16, got torch.float32Expected layers.54.mlp_norm.scale to be bf16, got torch.float32

Expected layers.60.mlp.w1.weight to be bf16, got torch.float32Expected layers.59.mlp_norm.scale to be bf16, got torch.float32

Expected layers.65.sa_norm.scale to be bf16, got torch.float32

Expected layers.59.attn.output_proj.weight to be bf16, got torch.float32Expected layers.50.sa_norm.scale to be bf16, got torch.float32Expected layers.63.mlp.w3.weight to be bf16, got torch.float32

Expected layers.54.mlp.w1.weight to be bf16, got torch.float32
Expected layers.50.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.60.mlp.w2.weight to be bf16, got torch.float32Expected layers.59.mlp.w1.weight to be bf16, got torch.float32

Expected layers.65.attn.q_proj.weight to be bf16, got torch.float32Expected layers.59.mlp_norm.scale to be bf16, got torch.float32

Expected layers.50.attn.q_proj.weight to be bf16, got torch.float32Expected layers.64.sa_norm.scale to be bf16, got torch.float32

Expected layers.54.mlp.w2.weight to be bf16, got torch.float32Expected layers.50.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.60.mlp.w3.weight to be bf16, got torch.float32Expected layers.59.mlp.w2.weight to be bf16, got torch.float32

Expected layers.65.attn.k_proj.weight to be bf16, got torch.float32Expected layers.59.mlp.w1.weight to be bf16, got torch.float32

Expected layers.50.attn.k_proj.weight to be bf16, got torch.float32Expected layers.64.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.54.mlp.w3.weight to be bf16, got torch.float32Expected layers.50.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.61.sa_norm.scale to be bf16, got torch.float32
Expected layers.59.mlp.w3.weight to be bf16, got torch.float32
Expected layers.65.attn.v_proj.weight to be bf16, got torch.float32Expected layers.59.mlp.w2.weight to be bf16, got torch.float32

Expected layers.50.attn.v_proj.weight to be bf16, got torch.float32Expected layers.64.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.55.sa_norm.scale to be bf16, got torch.float32Expected layers.50.mlp_norm.scale to be bf16, got torch.float32

Expected layers.61.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.60.sa_norm.scale to be bf16, got torch.float32
Expected layers.65.attn.output_proj.weight to be bf16, got torch.float32Expected layers.59.mlp.w3.weight to be bf16, got torch.float32

Expected layers.50.attn.output_proj.weight to be bf16, got torch.float32Expected layers.64.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.55.attn.q_proj.weight to be bf16, got torch.float32Expected layers.50.mlp.w1.weight to be bf16, got torch.float32

Expected layers.61.attn.k_proj.weight to be bf16, got torch.float32Expected layers.60.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.65.mlp_norm.scale to be bf16, got torch.float32

Expected layers.60.sa_norm.scale to be bf16, got torch.float32Expected layers.50.mlp_norm.scale to be bf16, got torch.float32Expected layers.64.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.55.attn.k_proj.weight to be bf16, got torch.float32Expected layers.50.mlp.w2.weight to be bf16, got torch.float32


Expected layers.61.attn.v_proj.weight to be bf16, got torch.float32Expected layers.60.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.60.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.64.mlp_norm.scale to be bf16, got torch.float32Expected layers.65.mlp.w1.weight to be bf16, got torch.float32

Expected layers.50.mlp.w1.weight to be bf16, got torch.float32


Expected layers.55.attn.v_proj.weight to be bf16, got torch.float32Expected layers.50.mlp.w3.weight to be bf16, got torch.float32
Expected layers.61.attn.output_proj.weight to be bf16, got torch.float32Expected layers.60.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.60.attn.k_proj.weight to be bf16, got torch.float32Expected layers.64.mlp.w1.weight to be bf16, got torch.float32
Expected layers.65.mlp.w2.weight to be bf16, got torch.float32
Expected layers.50.mlp.w2.weight to be bf16, got torch.float32

Expected layers.55.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.51.sa_norm.scale to be bf16, got torch.float32
Expected layers.61.mlp_norm.scale to be bf16, got torch.float32
Expected layers.60.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.60.attn.v_proj.weight to be bf16, got torch.float32Expected layers.64.mlp.w2.weight to be bf16, got torch.float32
Expected layers.65.mlp.w3.weight to be bf16, got torch.float32
Expected layers.50.mlp.w3.weight to be bf16, got torch.float32

Expected layers.55.mlp_norm.scale to be bf16, got torch.float32
Expected layers.51.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.61.mlp.w1.weight to be bf16, got torch.float32
Expected layers.60.mlp_norm.scale to be bf16, got torch.float32
Expected layers.60.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.64.mlp.w3.weight to be bf16, got torch.float32Expected layers.66.sa_norm.scale to be bf16, got torch.float32
Expected layers.51.sa_norm.scale to be bf16, got torch.float32
Expected layers.55.mlp.w1.weight to be bf16, got torch.float32

Expected layers.51.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.61.mlp.w2.weight to be bf16, got torch.float32
Expected layers.60.mlp.w1.weight to be bf16, got torch.float32
Expected layers.60.mlp_norm.scale to be bf16, got torch.float32
Expected layers.65.sa_norm.scale to be bf16, got torch.float32Expected layers.66.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.51.attn.q_proj.weight to be bf16, got torch.float32Expected layers.55.mlp.w2.weight to be bf16, got torch.float32


Expected layers.51.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.61.mlp.w3.weight to be bf16, got torch.float32
Expected layers.60.mlp.w2.weight to be bf16, got torch.float32
Expected layers.60.mlp.w1.weight to be bf16, got torch.float32
Expected layers.65.attn.q_proj.weight to be bf16, got torch.float32Expected layers.66.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.51.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.55.mlp.w3.weight to be bf16, got torch.float32

Expected layers.51.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.62.sa_norm.scale to be bf16, got torch.float32
Expected layers.60.mlp.w3.weight to be bf16, got torch.float32
Expected layers.60.mlp.w2.weight to be bf16, got torch.float32
Expected layers.65.attn.k_proj.weight to be bf16, got torch.float32Expected layers.66.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.51.attn.v_proj.weight to be bf16, got torch.float32


Expected layers.51.mlp_norm.scale to be bf16, got torch.float32Expected layers.56.sa_norm.scale to be bf16, got torch.float32
Expected layers.62.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.61.sa_norm.scale to be bf16, got torch.float32
Expected layers.60.mlp.w3.weight to be bf16, got torch.float32Expected layers.65.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.66.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.51.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.51.mlp.w1.weight to be bf16, got torch.float32

Expected layers.62.attn.k_proj.weight to be bf16, got torch.float32Expected layers.56.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.61.attn.q_proj.weight to be bf16, got torch.float32Expected layers.61.sa_norm.scale to be bf16, got torch.float32
Expected layers.65.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.66.mlp_norm.scale to be bf16, got torch.float32
Expected layers.51.mlp_norm.scale to be bf16, got torch.float32

Expected layers.51.mlp.w2.weight to be bf16, got torch.float32
Expected layers.62.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.56.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.61.attn.k_proj.weight to be bf16, got torch.float32Expected layers.61.attn.q_proj.weight to be bf16, got torch.float32Expected layers.65.mlp_norm.scale to be bf16, got torch.float32
Expected layers.66.mlp.w1.weight to be bf16, got torch.float32Expected layers.51.mlp.w1.weight to be bf16, got torch.float32
Expected layers.51.mlp.w3.weight to be bf16, got torch.float32

Expected layers.62.attn.output_proj.weight to be bf16, got torch.float32


Expected layers.56.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.61.attn.v_proj.weight to be bf16, got torch.float32Expected layers.61.attn.k_proj.weight to be bf16, got torch.float32Expected layers.65.mlp.w1.weight to be bf16, got torch.float32
Expected layers.66.mlp.w2.weight to be bf16, got torch.float32
Expected layers.51.mlp.w2.weight to be bf16, got torch.float32
Expected layers.52.sa_norm.scale to be bf16, got torch.float32
Expected layers.62.mlp_norm.scale to be bf16, got torch.float32


Expected layers.61.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.56.attn.output_proj.weight to be bf16, got torch.float32Expected layers.61.attn.v_proj.weight to be bf16, got torch.float32Expected layers.65.mlp.w2.weight to be bf16, got torch.float32
Expected layers.66.mlp.w3.weight to be bf16, got torch.float32
Expected layers.51.mlp.w3.weight to be bf16, got torch.float32
Expected layers.52.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.62.mlp.w1.weight to be bf16, got torch.float32

Expected layers.61.mlp_norm.scale to be bf16, got torch.float32
Expected layers.56.mlp_norm.scale to be bf16, got torch.float32Expected layers.67.sa_norm.scale to be bf16, got torch.float32
Expected layers.61.attn.output_proj.weight to be bf16, got torch.float32Expected layers.65.mlp.w3.weight to be bf16, got torch.float32


Expected layers.52.sa_norm.scale to be bf16, got torch.float32
Expected layers.52.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.62.mlp.w2.weight to be bf16, got torch.float32
Expected layers.61.mlp.w1.weight to be bf16, got torch.float32
Expected layers.67.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.56.mlp.w1.weight to be bf16, got torch.float32Expected layers.61.mlp_norm.scale to be bf16, got torch.float32
Expected layers.66.sa_norm.scale to be bf16, got torch.float32
Expected layers.52.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.52.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.62.mlp.w3.weight to be bf16, got torch.float32
Expected layers.61.mlp.w2.weight to be bf16, got torch.float32
Expected layers.67.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.61.mlp.w1.weight to be bf16, got torch.float32Expected layers.66.attn.q_proj.weight to be bf16, got torch.float32Expected layers.52.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.56.mlp.w2.weight to be bf16, got torch.float32Expected layers.52.attn.k_proj.weight to be bf16, got torch.float32


Expected layers.63.sa_norm.scale to be bf16, got torch.float32Expected layers.61.mlp.w3.weight to be bf16, got torch.float32

Expected layers.67.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.61.mlp.w2.weight to be bf16, got torch.float32Expected layers.66.attn.k_proj.weight to be bf16, got torch.float32Expected layers.52.mlp_norm.scale to be bf16, got torch.float32
Expected layers.52.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.56.mlp.w3.weight to be bf16, got torch.float32
Expected layers.63.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.62.sa_norm.scale to be bf16, got torch.float32

Expected layers.67.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.61.mlp.w3.weight to be bf16, got torch.float32
Expected layers.66.attn.v_proj.weight to be bf16, got torch.float32Expected layers.52.mlp.w1.weight to be bf16, got torch.float32
Expected layers.52.attn.output_proj.weight to be bf16, got torch.float32


Expected layers.63.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.62.attn.q_proj.weight to be bf16, got torch.float32Expected layers.57.sa_norm.scale to be bf16, got torch.float32Expected layers.67.mlp_norm.scale to be bf16, got torch.float32
Expected layers.62.sa_norm.scale to be bf16, got torch.float32
Expected layers.66.attn.output_proj.weight to be bf16, got torch.float32Expected layers.52.mlp.w2.weight to be bf16, got torch.float32

Expected layers.52.mlp_norm.scale to be bf16, got torch.float32Expected layers.62.attn.k_proj.weight to be bf16, got torch.float32


Expected layers.63.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.67.mlp.w1.weight to be bf16, got torch.float32
Expected layers.57.attn.q_proj.weight to be bf16, got torch.float32Expected layers.62.attn.q_proj.weight to be bf16, got torch.float32Expected layers.66.mlp_norm.scale to be bf16, got torch.float32
Expected layers.52.mlp.w3.weight to be bf16, got torch.float32
Expected layers.52.mlp.w1.weight to be bf16, got torch.float32
Expected layers.62.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.63.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.67.mlp.w2.weight to be bf16, got torch.float32

Expected layers.53.sa_norm.scale to be bf16, got torch.float32Expected layers.57.attn.k_proj.weight to be bf16, got torch.float32Expected layers.62.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.66.mlp.w1.weight to be bf16, got torch.float32Expected layers.52.mlp.w2.weight to be bf16, got torch.float32


Expected layers.62.attn.output_proj.weight to be bf16, got torch.float32Expected layers.63.mlp_norm.scale to be bf16, got torch.float32

Expected layers.67.mlp.w3.weight to be bf16, got torch.float32

Expected layers.53.attn.q_proj.weight to be bf16, got torch.float32Expected layers.57.attn.v_proj.weight to be bf16, got torch.float32Expected layers.62.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.66.mlp.w2.weight to be bf16, got torch.float32
Expected layers.52.mlp.w3.weight to be bf16, got torch.float32

Expected layers.62.mlp_norm.scale to be bf16, got torch.float32Expected layers.63.mlp.w1.weight to be bf16, got torch.float32

Expected layers.68.sa_norm.scale to be bf16, got torch.float32
Expected layers.53.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.57.attn.output_proj.weight to be bf16, got torch.float32Expected layers.62.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.66.mlp.w3.weight to be bf16, got torch.float32Expected layers.53.sa_norm.scale to be bf16, got torch.float32
Expected layers.62.mlp.w1.weight to be bf16, got torch.float32
Expected layers.63.mlp.w2.weight to be bf16, got torch.float32

Expected layers.68.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.53.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.57.mlp_norm.scale to be bf16, got torch.float32
Expected layers.62.mlp_norm.scale to be bf16, got torch.float32
Expected layers.67.sa_norm.scale to be bf16, got torch.float32Expected layers.53.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.62.mlp.w2.weight to be bf16, got torch.float32
Expected layers.63.mlp.w3.weight to be bf16, got torch.float32

Expected layers.68.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.53.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.57.mlp.w1.weight to be bf16, got torch.float32
Expected layers.62.mlp.w1.weight to be bf16, got torch.float32
Expected layers.67.attn.q_proj.weight to be bf16, got torch.float32Expected layers.53.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.62.mlp.w3.weight to be bf16, got torch.float32
Expected layers.64.sa_norm.scale to be bf16, got torch.float32

Expected layers.68.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.53.mlp_norm.scale to be bf16, got torch.float32
Expected layers.57.mlp.w2.weight to be bf16, got torch.float32
Expected layers.62.mlp.w2.weight to be bf16, got torch.float32
Expected layers.67.attn.k_proj.weight to be bf16, got torch.float32Expected layers.53.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.63.sa_norm.scale to be bf16, got torch.float32
Expected layers.64.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.68.attn.output_proj.weight to be bf16, got torch.float32Expected layers.53.mlp.w1.weight to be bf16, got torch.float32


Expected layers.62.mlp.w3.weight to be bf16, got torch.float32
Expected layers.57.mlp.w3.weight to be bf16, got torch.float32Expected layers.67.attn.v_proj.weight to be bf16, got torch.float32Expected layers.53.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.63.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.64.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.68.mlp_norm.scale to be bf16, got torch.float32
Expected layers.53.mlp.w2.weight to be bf16, got torch.float32

Expected layers.63.sa_norm.scale to be bf16, got torch.float32
Expected layers.53.mlp_norm.scale to be bf16, got torch.float32
Expected layers.67.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.58.sa_norm.scale to be bf16, got torch.float32
Expected layers.63.attn.k_proj.weight to be bf16, got torch.float32Expected layers.53.mlp.w3.weight to be bf16, got torch.float32Expected layers.64.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.68.mlp.w1.weight to be bf16, got torch.float32


Expected layers.63.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.53.mlp.w1.weight to be bf16, got torch.float32
Expected layers.67.mlp_norm.scale to be bf16, got torch.float32
Expected layers.58.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.63.attn.v_proj.weight to be bf16, got torch.float32Expected layers.54.sa_norm.scale to be bf16, got torch.float32
Expected layers.64.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.68.mlp.w2.weight to be bf16, got torch.float32

Expected layers.63.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.53.mlp.w2.weight to be bf16, got torch.float32
Expected layers.67.mlp.w1.weight to be bf16, got torch.float32
Expected layers.58.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.63.attn.output_proj.weight to be bf16, got torch.float32Expected layers.54.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.64.mlp_norm.scale to be bf16, got torch.float32
Expected layers.68.mlp.w3.weight to be bf16, got torch.float32

Expected layers.63.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.53.mlp.w3.weight to be bf16, got torch.float32
Expected layers.67.mlp.w2.weight to be bf16, got torch.float32

Expected layers.63.mlp_norm.scale to be bf16, got torch.float32Expected layers.58.attn.v_proj.weight to be bf16, got torch.float32Expected layers.54.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.54.sa_norm.scale to be bf16, got torch.float32Expected layers.64.mlp.w1.weight to be bf16, got torch.float32
Expected layers.69.sa_norm.scale to be bf16, got torch.float32

Expected layers.63.attn.output_proj.weight to be bf16, got torch.float32


Expected layers.67.mlp.w3.weight to be bf16, got torch.float32
Expected layers.63.mlp.w1.weight to be bf16, got torch.float32
Expected layers.54.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.54.attn.v_proj.weight to be bf16, got torch.float32Expected layers.58.attn.output_proj.weight to be bf16, got torch.float32Expected layers.64.mlp.w2.weight to be bf16, got torch.float32Expected layers.69.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.63.mlp_norm.scale to be bf16, got torch.float32

Expected layers.68.sa_norm.scale to be bf16, got torch.float32

Expected layers.63.mlp.w2.weight to be bf16, got torch.float32
Expected layers.58.mlp_norm.scale to be bf16, got torch.float32
Expected layers.54.attn.k_proj.weight to be bf16, got torch.float32Expected layers.54.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.64.mlp.w3.weight to be bf16, got torch.float32Expected layers.69.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.63.mlp.w1.weight to be bf16, got torch.float32Expected layers.68.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.63.mlp.w3.weight to be bf16, got torch.float32Expected layers.58.mlp.w1.weight to be bf16, got torch.float32

Expected layers.54.attn.v_proj.weight to be bf16, got torch.float32Expected layers.54.mlp_norm.scale to be bf16, got torch.float32

Expected layers.65.sa_norm.scale to be bf16, got torch.float32Expected layers.69.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.63.mlp.w2.weight to be bf16, got torch.float32Expected layers.68.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.64.sa_norm.scale to be bf16, got torch.float32Expected layers.58.mlp.w2.weight to be bf16, got torch.float32

Expected layers.54.attn.output_proj.weight to be bf16, got torch.float32Expected layers.54.mlp.w1.weight to be bf16, got torch.float32

Expected layers.65.attn.q_proj.weight to be bf16, got torch.float32Expected layers.69.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.63.mlp.w3.weight to be bf16, got torch.float32Expected layers.68.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.64.attn.q_proj.weight to be bf16, got torch.float32Expected layers.58.mlp.w3.weight to be bf16, got torch.float32

Expected layers.54.mlp_norm.scale to be bf16, got torch.float32Expected layers.54.mlp.w2.weight to be bf16, got torch.float32

Expected layers.65.attn.k_proj.weight to be bf16, got torch.float32Expected layers.69.mlp_norm.scale to be bf16, got torch.float32
Expected layers.64.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.64.sa_norm.scale to be bf16, got torch.float32Expected layers.68.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.59.sa_norm.scale to be bf16, got torch.float32


Expected layers.54.mlp.w1.weight to be bf16, got torch.float32Expected layers.54.mlp.w3.weight to be bf16, got torch.float32
Expected layers.65.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.69.mlp.w1.weight to be bf16, got torch.float32
Expected layers.64.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.64.attn.q_proj.weight to be bf16, got torch.float32Expected layers.68.mlp_norm.scale to be bf16, got torch.float32
Expected layers.59.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.54.mlp.w2.weight to be bf16, got torch.float32

Expected layers.55.sa_norm.scale to be bf16, got torch.float32
Expected layers.65.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.69.mlp.w2.weight to be bf16, got torch.float32
Expected layers.64.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.64.attn.k_proj.weight to be bf16, got torch.float32Expected layers.68.mlp.w1.weight to be bf16, got torch.float32

Expected layers.59.attn.k_proj.weight to be bf16, got torch.float32Expected layers.54.mlp.w3.weight to be bf16, got torch.float32

Expected layers.55.attn.q_proj.weight to be bf16, got torch.float32Expected layers.65.mlp_norm.scale to be bf16, got torch.float32

Expected layers.69.mlp.w3.weight to be bf16, got torch.float32Expected layers.64.mlp_norm.scale to be bf16, got torch.float32

Expected layers.64.attn.v_proj.weight to be bf16, got torch.float32Expected layers.68.mlp.w2.weight to be bf16, got torch.float32

Expected layers.59.attn.v_proj.weight to be bf16, got torch.float32Expected layers.55.sa_norm.scale to be bf16, got torch.float32

Expected layers.55.attn.k_proj.weight to be bf16, got torch.float32Expected layers.65.mlp.w1.weight to be bf16, got torch.float32

Expected layers.70.sa_norm.scale to be bf16, got torch.float32Expected layers.64.mlp.w1.weight to be bf16, got torch.float32

Expected layers.64.attn.output_proj.weight to be bf16, got torch.float32Expected layers.68.mlp.w3.weight to be bf16, got torch.float32

Expected layers.59.attn.output_proj.weight to be bf16, got torch.float32Expected layers.55.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.55.attn.v_proj.weight to be bf16, got torch.float32Expected layers.65.mlp.w2.weight to be bf16, got torch.float32Expected layers.64.mlp.w2.weight to be bf16, got torch.float32

Expected layers.70.attn.q_proj.weight to be bf16, got torch.float32


Expected layers.64.mlp_norm.scale to be bf16, got torch.float32Expected layers.69.sa_norm.scale to be bf16, got torch.float32
Expected layers.59.mlp_norm.scale to be bf16, got torch.float32Expected layers.55.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.55.attn.output_proj.weight to be bf16, got torch.float32Expected layers.65.mlp.w3.weight to be bf16, got torch.float32
Expected layers.64.mlp.w3.weight to be bf16, got torch.float32
Expected layers.70.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.64.mlp.w1.weight to be bf16, got torch.float32
Expected layers.69.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.59.mlp.w1.weight to be bf16, got torch.float32
Expected layers.55.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.55.mlp_norm.scale to be bf16, got torch.float32
Expected layers.66.sa_norm.scale to be bf16, got torch.float32
Expected layers.65.sa_norm.scale to be bf16, got torch.float32Expected layers.70.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.64.mlp.w2.weight to be bf16, got torch.float32
Expected layers.69.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.59.mlp.w2.weight to be bf16, got torch.float32
Expected layers.55.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.55.mlp.w1.weight to be bf16, got torch.float32
Expected layers.66.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.65.attn.q_proj.weight to be bf16, got torch.float32Expected layers.70.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.64.mlp.w3.weight to be bf16, got torch.float32
Expected layers.69.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.59.mlp.w3.weight to be bf16, got torch.float32
Expected layers.55.mlp_norm.scale to be bf16, got torch.float32
Expected layers.55.mlp.w2.weight to be bf16, got torch.float32
Expected layers.66.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.65.attn.k_proj.weight to be bf16, got torch.float32Expected layers.70.mlp_norm.scale to be bf16, got torch.float32
Expected layers.65.sa_norm.scale to be bf16, got torch.float32
Expected layers.69.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.60.sa_norm.scale to be bf16, got torch.float32
Expected layers.55.mlp.w1.weight to be bf16, got torch.float32
Expected layers.55.mlp.w3.weight to be bf16, got torch.float32
Expected layers.66.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.65.attn.v_proj.weight to be bf16, got torch.float32Expected layers.70.mlp.w1.weight to be bf16, got torch.float32
Expected layers.65.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.69.mlp_norm.scale to be bf16, got torch.float32

Expected layers.60.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.55.mlp.w2.weight to be bf16, got torch.float32
Expected layers.56.sa_norm.scale to be bf16, got torch.float32
Expected layers.66.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.65.attn.output_proj.weight to be bf16, got torch.float32Expected layers.70.mlp.w2.weight to be bf16, got torch.float32
Expected layers.65.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.69.mlp.w1.weight to be bf16, got torch.float32
Expected layers.60.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.66.mlp_norm.scale to be bf16, got torch.float32Expected layers.55.mlp.w3.weight to be bf16, got torch.float32
Expected layers.56.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.65.mlp_norm.scale to be bf16, got torch.float32

Expected layers.70.mlp.w3.weight to be bf16, got torch.float32
Expected layers.65.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.69.mlp.w2.weight to be bf16, got torch.float32
Expected layers.60.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.66.mlp.w1.weight to be bf16, got torch.float32Expected layers.56.sa_norm.scale to be bf16, got torch.float32
Expected layers.56.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.65.mlp.w1.weight to be bf16, got torch.float32
Expected layers.71.sa_norm.scale to be bf16, got torch.float32
Expected layers.65.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.69.mlp.w3.weight to be bf16, got torch.float32
Expected layers.60.attn.output_proj.weight to be bf16, got torch.float32Expected layers.66.mlp.w2.weight to be bf16, got torch.float32

Expected layers.56.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.56.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.65.mlp.w2.weight to be bf16, got torch.float32Expected layers.71.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.65.mlp_norm.scale to be bf16, got torch.float32Expected layers.70.sa_norm.scale to be bf16, got torch.float32

Expected layers.60.mlp_norm.scale to be bf16, got torch.float32Expected layers.66.mlp.w3.weight to be bf16, got torch.float32

Expected layers.56.attn.k_proj.weight to be bf16, got torch.float32Expected layers.56.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.71.attn.k_proj.weight to be bf16, got torch.float32Expected layers.65.mlp.w3.weight to be bf16, got torch.float32

Expected layers.65.mlp.w1.weight to be bf16, got torch.float32Expected layers.70.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.60.mlp.w1.weight to be bf16, got torch.float32Expected layers.67.sa_norm.scale to be bf16, got torch.float32

Expected layers.56.attn.v_proj.weight to be bf16, got torch.float32Expected layers.56.mlp_norm.scale to be bf16, got torch.float32

Expected layers.71.attn.v_proj.weight to be bf16, got torch.float32Expected layers.66.sa_norm.scale to be bf16, got torch.float32

Expected layers.65.mlp.w2.weight to be bf16, got torch.float32Expected layers.70.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.60.mlp.w2.weight to be bf16, got torch.float32Expected layers.67.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.56.attn.output_proj.weight to be bf16, got torch.float32Expected layers.56.mlp.w1.weight to be bf16, got torch.float32

Expected layers.71.attn.output_proj.weight to be bf16, got torch.float32Expected layers.66.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.65.mlp.w3.weight to be bf16, got torch.float32Expected layers.70.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.56.mlp_norm.scale to be bf16, got torch.float32Expected layers.60.mlp.w3.weight to be bf16, got torch.float32Expected layers.67.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.56.mlp.w2.weight to be bf16, got torch.float32


Expected layers.71.mlp_norm.scale to be bf16, got torch.float32Expected layers.66.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.66.sa_norm.scale to be bf16, got torch.float32Expected layers.70.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.56.mlp.w1.weight to be bf16, got torch.float32Expected layers.61.sa_norm.scale to be bf16, got torch.float32Expected layers.67.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.56.mlp.w3.weight to be bf16, got torch.float32


Expected layers.71.mlp.w1.weight to be bf16, got torch.float32Expected layers.66.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.66.attn.q_proj.weight to be bf16, got torch.float32Expected layers.70.mlp_norm.scale to be bf16, got torch.float32

Expected layers.56.mlp.w2.weight to be bf16, got torch.float32Expected layers.61.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.67.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.57.sa_norm.scale to be bf16, got torch.float32

Expected layers.71.mlp.w2.weight to be bf16, got torch.float32
Expected layers.66.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.66.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.70.mlp.w1.weight to be bf16, got torch.float32
Expected layers.56.mlp.w3.weight to be bf16, got torch.float32Expected layers.61.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.67.mlp_norm.scale to be bf16, got torch.float32
Expected layers.57.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.71.mlp.w3.weight to be bf16, got torch.float32Expected layers.66.mlp_norm.scale to be bf16, got torch.float32

Expected layers.66.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.70.mlp.w2.weight to be bf16, got torch.float32
Expected layers.57.sa_norm.scale to be bf16, got torch.float32Expected layers.61.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.67.mlp.w1.weight to be bf16, got torch.float32Expected layers.57.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.72.sa_norm.scale to be bf16, got torch.float32Expected layers.70.mlp.w3.weight to be bf16, got torch.float32Expected layers.66.mlp.w1.weight to be bf16, got torch.float32

Expected layers.66.attn.output_proj.weight to be bf16, got torch.float32


Expected layers.57.attn.q_proj.weight to be bf16, got torch.float32Expected layers.61.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.67.mlp.w2.weight to be bf16, got torch.float32Expected layers.57.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.72.attn.q_proj.weight to be bf16, got torch.float32Expected layers.71.sa_norm.scale to be bf16, got torch.float32
Expected layers.66.mlp.w2.weight to be bf16, got torch.float32
Expected layers.66.mlp_norm.scale to be bf16, got torch.float32

Expected layers.57.attn.k_proj.weight to be bf16, got torch.float32Expected layers.61.mlp_norm.scale to be bf16, got torch.float32

Expected layers.67.mlp.w3.weight to be bf16, got torch.float32
Expected layers.57.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.72.attn.k_proj.weight to be bf16, got torch.float32Expected layers.71.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.66.mlp.w3.weight to be bf16, got torch.float32Expected layers.66.mlp.w1.weight to be bf16, got torch.float32

Expected layers.57.attn.v_proj.weight to be bf16, got torch.float32Expected layers.61.mlp.w1.weight to be bf16, got torch.float32

Expected layers.68.sa_norm.scale to be bf16, got torch.float32Expected layers.57.mlp_norm.scale to be bf16, got torch.float32

Expected layers.72.attn.v_proj.weight to be bf16, got torch.float32Expected layers.71.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.67.sa_norm.scale to be bf16, got torch.float32
Expected layers.66.mlp.w2.weight to be bf16, got torch.float32
Expected layers.57.attn.output_proj.weight to be bf16, got torch.float32Expected layers.61.mlp.w2.weight to be bf16, got torch.float32

Expected layers.68.attn.q_proj.weight to be bf16, got torch.float32Expected layers.57.mlp.w1.weight to be bf16, got torch.float32

Expected layers.72.attn.output_proj.weight to be bf16, got torch.float32Expected layers.71.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.67.attn.q_proj.weight to be bf16, got torch.float32Expected layers.66.mlp.w3.weight to be bf16, got torch.float32

Expected layers.57.mlp_norm.scale to be bf16, got torch.float32

Expected layers.61.mlp.w3.weight to be bf16, got torch.float32Expected layers.68.attn.k_proj.weight to be bf16, got torch.float32Expected layers.57.mlp.w2.weight to be bf16, got torch.float32
Expected layers.72.mlp_norm.scale to be bf16, got torch.float32Expected layers.71.attn.output_proj.weight to be bf16, got torch.float32


Expected layers.67.attn.k_proj.weight to be bf16, got torch.float32Expected layers.67.sa_norm.scale to be bf16, got torch.float32

Expected layers.57.mlp.w1.weight to be bf16, got torch.float32

Expected layers.62.sa_norm.scale to be bf16, got torch.float32Expected layers.68.attn.v_proj.weight to be bf16, got torch.float32Expected layers.57.mlp.w3.weight to be bf16, got torch.float32
Expected layers.72.mlp.w1.weight to be bf16, got torch.float32
Expected layers.71.mlp_norm.scale to be bf16, got torch.float32

Expected layers.67.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.67.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.57.mlp.w2.weight to be bf16, got torch.float32
Expected layers.62.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.68.attn.output_proj.weight to be bf16, got torch.float32Expected layers.58.sa_norm.scale to be bf16, got torch.float32
Expected layers.72.mlp.w2.weight to be bf16, got torch.float32
Expected layers.71.mlp.w1.weight to be bf16, got torch.float32

Expected layers.67.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.67.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.57.mlp.w3.weight to be bf16, got torch.float32
Expected layers.62.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.68.mlp_norm.scale to be bf16, got torch.float32Expected layers.58.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.72.mlp.w3.weight to be bf16, got torch.float32
Expected layers.71.mlp.w2.weight to be bf16, got torch.float32

Expected layers.67.mlp_norm.scale to be bf16, got torch.float32
Expected layers.67.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.58.attn.k_proj.weight to be bf16, got torch.float32Expected layers.58.sa_norm.scale to be bf16, got torch.float32
Expected layers.62.attn.v_proj.weight to be bf16, got torch.float32Expected layers.68.mlp.w1.weight to be bf16, got torch.float32

Expected layers.73.sa_norm.scale to be bf16, got torch.float32

Expected layers.71.mlp.w3.weight to be bf16, got torch.float32
Expected layers.67.mlp.w1.weight to be bf16, got torch.float32
Expected layers.67.attn.output_proj.weight to be bf16, got torch.float32Expected layers.58.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.58.attn.q_proj.weight to be bf16, got torch.float32Expected layers.62.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.68.mlp.w2.weight to be bf16, got torch.float32Expected layers.73.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.72.sa_norm.scale to be bf16, got torch.float32Expected layers.67.mlp.w2.weight to be bf16, got torch.float32

Expected layers.67.mlp_norm.scale to be bf16, got torch.float32Expected layers.58.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.58.attn.k_proj.weight to be bf16, got torch.float32Expected layers.62.mlp_norm.scale to be bf16, got torch.float32

Expected layers.68.mlp.w3.weight to be bf16, got torch.float32Expected layers.73.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.72.attn.q_proj.weight to be bf16, got torch.float32Expected layers.67.mlp.w3.weight to be bf16, got torch.float32

Expected layers.67.mlp.w1.weight to be bf16, got torch.float32Expected layers.58.mlp_norm.scale to be bf16, got torch.float32

Expected layers.69.sa_norm.scale to be bf16, got torch.float32Expected layers.58.attn.v_proj.weight to be bf16, got torch.float32Expected layers.62.mlp.w1.weight to be bf16, got torch.float32

Expected layers.73.attn.v_proj.weight to be bf16, got torch.float32


Expected layers.72.attn.k_proj.weight to be bf16, got torch.float32Expected layers.68.sa_norm.scale to be bf16, got torch.float32
Expected layers.67.mlp.w2.weight to be bf16, got torch.float32Expected layers.58.mlp.w1.weight to be bf16, got torch.float32

Expected layers.69.attn.q_proj.weight to be bf16, got torch.float32Expected layers.58.attn.output_proj.weight to be bf16, got torch.float32Expected layers.62.mlp.w2.weight to be bf16, got torch.float32

Expected layers.73.attn.output_proj.weight to be bf16, got torch.float32


Expected layers.72.attn.v_proj.weight to be bf16, got torch.float32Expected layers.68.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.67.mlp.w3.weight to be bf16, got torch.float32Expected layers.58.mlp.w2.weight to be bf16, got torch.float32

Expected layers.69.attn.k_proj.weight to be bf16, got torch.float32Expected layers.58.mlp_norm.scale to be bf16, got torch.float32
Expected layers.62.mlp.w3.weight to be bf16, got torch.float32Expected layers.72.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.73.mlp_norm.scale to be bf16, got torch.float32


Expected layers.68.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.68.sa_norm.scale to be bf16, got torch.float32Expected layers.58.mlp.w3.weight to be bf16, got torch.float32
Expected layers.69.attn.v_proj.weight to be bf16, got torch.float32Expected layers.58.mlp.w1.weight to be bf16, got torch.float32

Expected layers.63.sa_norm.scale to be bf16, got torch.float32Expected layers.72.mlp_norm.scale to be bf16, got torch.float32Expected layers.73.mlp.w1.weight to be bf16, got torch.float32

Expected layers.68.attn.v_proj.weight to be bf16, got torch.float32


Expected layers.68.attn.q_proj.weight to be bf16, got torch.float32Expected layers.59.sa_norm.scale to be bf16, got torch.float32
Expected layers.69.attn.output_proj.weight to be bf16, got torch.float32Expected layers.58.mlp.w2.weight to be bf16, got torch.float32

Expected layers.72.mlp.w1.weight to be bf16, got torch.float32
Expected layers.73.mlp.w2.weight to be bf16, got torch.float32
Expected layers.63.attn.q_proj.weight to be bf16, got torch.float32Expected layers.68.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.68.attn.k_proj.weight to be bf16, got torch.float32Expected layers.59.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.69.mlp_norm.scale to be bf16, got torch.float32Expected layers.58.mlp.w3.weight to be bf16, got torch.float32

Expected layers.72.mlp.w2.weight to be bf16, got torch.float32Expected layers.73.mlp.w3.weight to be bf16, got torch.float32

Expected layers.63.attn.k_proj.weight to be bf16, got torch.float32Expected layers.68.mlp_norm.scale to be bf16, got torch.float32

Expected layers.68.attn.v_proj.weight to be bf16, got torch.float32Expected layers.59.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.69.mlp.w1.weight to be bf16, got torch.float32Expected layers.59.sa_norm.scale to be bf16, got torch.float32

Expected layers.72.mlp.w3.weight to be bf16, got torch.float32
Expected layers.74.sa_norm.scale to be bf16, got torch.float32
Expected layers.63.attn.v_proj.weight to be bf16, got torch.float32Expected layers.68.mlp.w1.weight to be bf16, got torch.float32

Expected layers.68.attn.output_proj.weight to be bf16, got torch.float32Expected layers.59.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.69.mlp.w2.weight to be bf16, got torch.float32Expected layers.59.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.63.attn.output_proj.weight to be bf16, got torch.float32Expected layers.73.sa_norm.scale to be bf16, got torch.float32Expected layers.74.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.68.mlp.w2.weight to be bf16, got torch.float32


Expected layers.68.mlp_norm.scale to be bf16, got torch.float32Expected layers.59.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.69.mlp.w3.weight to be bf16, got torch.float32Expected layers.59.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.63.mlp_norm.scale to be bf16, got torch.float32Expected layers.73.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.74.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.68.mlp.w3.weight to be bf16, got torch.float32
Expected layers.68.mlp.w1.weight to be bf16, got torch.float32
Expected layers.59.mlp_norm.scale to be bf16, got torch.float32
Expected layers.70.sa_norm.scale to be bf16, got torch.float32
Expected layers.59.attn.v_proj.weight to be bf16, got torch.float32Expected layers.63.mlp.w1.weight to be bf16, got torch.float32
Expected layers.73.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.74.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.69.sa_norm.scale to be bf16, got torch.float32
Expected layers.68.mlp.w2.weight to be bf16, got torch.float32
Expected layers.59.mlp.w1.weight to be bf16, got torch.float32
Expected layers.70.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.59.attn.output_proj.weight to be bf16, got torch.float32Expected layers.63.mlp.w2.weight to be bf16, got torch.float32
Expected layers.73.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.74.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.69.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.68.mlp.w3.weight to be bf16, got torch.float32

Expected layers.59.mlp.w2.weight to be bf16, got torch.float32
Expected layers.70.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.59.mlp_norm.scale to be bf16, got torch.float32Expected layers.63.mlp.w3.weight to be bf16, got torch.float32
Expected layers.73.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.74.mlp_norm.scale to be bf16, got torch.float32

Expected layers.69.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.69.sa_norm.scale to be bf16, got torch.float32
Expected layers.59.mlp.w3.weight to be bf16, got torch.float32
Expected layers.70.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.59.mlp.w1.weight to be bf16, got torch.float32Expected layers.64.sa_norm.scale to be bf16, got torch.float32
Expected layers.73.mlp_norm.scale to be bf16, got torch.float32
Expected layers.74.mlp.w1.weight to be bf16, got torch.float32

Expected layers.69.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.69.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.60.sa_norm.scale to be bf16, got torch.float32
Expected layers.70.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.59.mlp.w2.weight to be bf16, got torch.float32Expected layers.64.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.73.mlp.w1.weight to be bf16, got torch.float32
Expected layers.74.mlp.w2.weight to be bf16, got torch.float32

Expected layers.69.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.69.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.60.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.70.mlp_norm.scale to be bf16, got torch.float32
Expected layers.59.mlp.w3.weight to be bf16, got torch.float32Expected layers.74.mlp.w3.weight to be bf16, got torch.float32Expected layers.64.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.73.mlp.w2.weight to be bf16, got torch.float32



Expected layers.69.mlp_norm.scale to be bf16, got torch.float32Expected layers.69.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.60.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.70.mlp.w1.weight to be bf16, got torch.float32Expected layers.60.sa_norm.scale to be bf16, got torch.float32
Expected layers.75.sa_norm.scale to be bf16, got torch.float32
Expected layers.64.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.73.mlp.w3.weight to be bf16, got torch.float32
Expected layers.69.mlp.w1.weight to be bf16, got torch.float32
Expected layers.69.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.60.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.70.mlp.w2.weight to be bf16, got torch.float32Expected layers.60.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.75.attn.q_proj.weight to be bf16, got torch.float32


Expected layers.74.sa_norm.scale to be bf16, got torch.float32Expected layers.64.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.69.mlp.w2.weight to be bf16, got torch.float32Expected layers.69.mlp_norm.scale to be bf16, got torch.float32

Expected layers.60.attn.output_proj.weight to be bf16, got torch.float32Expected layers.70.mlp.w3.weight to be bf16, got torch.float32Expected layers.60.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.75.attn.k_proj.weight to be bf16, got torch.float32


Expected layers.74.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.64.mlp_norm.scale to be bf16, got torch.float32Expected layers.69.mlp.w3.weight to be bf16, got torch.float32Expected layers.69.mlp.w1.weight to be bf16, got torch.float32
Expected layers.60.mlp_norm.scale to be bf16, got torch.float32
Expected layers.71.sa_norm.scale to be bf16, got torch.float32
Expected layers.60.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.75.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.74.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.70.sa_norm.scale to be bf16, got torch.float32Expected layers.69.mlp.w2.weight to be bf16, got torch.float32Expected layers.64.mlp.w1.weight to be bf16, got torch.float32
Expected layers.60.mlp.w1.weight to be bf16, got torch.float32
Expected layers.71.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.60.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.75.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.74.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.70.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.69.mlp.w3.weight to be bf16, got torch.float32

Expected layers.60.mlp.w2.weight to be bf16, got torch.float32Expected layers.64.mlp.w2.weight to be bf16, got torch.float32Expected layers.71.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.60.mlp_norm.scale to be bf16, got torch.float32
Expected layers.75.mlp_norm.scale to be bf16, got torch.float32

Expected layers.74.attn.output_proj.weight to be bf16, got torch.float32Expected layers.70.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.70.sa_norm.scale to be bf16, got torch.float32
Expected layers.60.mlp.w3.weight to be bf16, got torch.float32
Expected layers.71.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.74.mlp_norm.scale to be bf16, got torch.float32Expected layers.64.mlp.w3.weight to be bf16, got torch.float32Expected layers.60.mlp.w1.weight to be bf16, got torch.float32Expected layers.75.mlp.w1.weight to be bf16, got torch.float32



Expected layers.70.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.70.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.61.sa_norm.scale to be bf16, got torch.float32Expected layers.71.attn.output_proj.weight to be bf16, got torch.float32Expected layers.74.mlp.w1.weight to be bf16, got torch.float32
Expected layers.65.sa_norm.scale to be bf16, got torch.float32
Expected layers.60.mlp.w2.weight to be bf16, got torch.float32
Expected layers.75.mlp.w2.weight to be bf16, got torch.float32
Expected layers.70.attn.output_proj.weight to be bf16, got torch.float32


Expected layers.70.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.61.attn.q_proj.weight to be bf16, got torch.float32Expected layers.71.mlp_norm.scale to be bf16, got torch.float32Expected layers.74.mlp.w2.weight to be bf16, got torch.float32
Expected layers.65.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.60.mlp.w3.weight to be bf16, got torch.float32
Expected layers.75.mlp.w3.weight to be bf16, got torch.float32
Expected layers.70.mlp_norm.scale to be bf16, got torch.float32


Expected layers.70.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.61.attn.k_proj.weight to be bf16, got torch.float32Expected layers.71.mlp.w1.weight to be bf16, got torch.float32
Expected layers.74.mlp.w3.weight to be bf16, got torch.float32Expected layers.65.attn.k_proj.weight to be bf16, got torch.float32Expected layers.61.sa_norm.scale to be bf16, got torch.float32

Expected layers.76.sa_norm.scale to be bf16, got torch.float32
Expected layers.70.mlp.w1.weight to be bf16, got torch.float32

Expected layers.70.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.61.attn.v_proj.weight to be bf16, got torch.float32Expected layers.71.mlp.w2.weight to be bf16, got torch.float32

Expected layers.75.sa_norm.scale to be bf16, got torch.float32
Expected layers.65.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.61.attn.q_proj.weight to be bf16, got torch.float32Expected layers.76.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.70.mlp.w2.weight to be bf16, got torch.float32Expected layers.70.mlp_norm.scale to be bf16, got torch.float32

Expected layers.61.attn.output_proj.weight to be bf16, got torch.float32Expected layers.71.mlp.w3.weight to be bf16, got torch.float32

Expected layers.75.attn.q_proj.weight to be bf16, got torch.float32Expected layers.65.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.61.attn.k_proj.weight to be bf16, got torch.float32Expected layers.76.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.70.mlp.w3.weight to be bf16, got torch.float32Expected layers.70.mlp.w1.weight to be bf16, got torch.float32

Expected layers.61.mlp_norm.scale to be bf16, got torch.float32Expected layers.72.sa_norm.scale to be bf16, got torch.float32

Expected layers.75.attn.k_proj.weight to be bf16, got torch.float32Expected layers.65.mlp_norm.scale to be bf16, got torch.float32

Expected layers.61.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.76.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.71.sa_norm.scale to be bf16, got torch.float32
Expected layers.70.mlp.w2.weight to be bf16, got torch.float32
Expected layers.61.mlp.w1.weight to be bf16, got torch.float32Expected layers.72.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.75.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.65.mlp.w1.weight to be bf16, got torch.float32

Expected layers.61.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.76.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.71.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.70.mlp.w3.weight to be bf16, got torch.float32
Expected layers.61.mlp.w2.weight to be bf16, got torch.float32Expected layers.72.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.75.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.65.mlp.w2.weight to be bf16, got torch.float32
Expected layers.61.mlp_norm.scale to be bf16, got torch.float32

Expected layers.76.mlp_norm.scale to be bf16, got torch.float32

Expected layers.71.attn.k_proj.weight to be bf16, got torch.float32Expected layers.71.sa_norm.scale to be bf16, got torch.float32
Expected layers.61.mlp.w3.weight to be bf16, got torch.float32Expected layers.72.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.75.mlp_norm.scale to be bf16, got torch.float32

Expected layers.61.mlp.w1.weight to be bf16, got torch.float32
Expected layers.65.mlp.w3.weight to be bf16, got torch.float32
Expected layers.76.mlp.w1.weight to be bf16, got torch.float32
Expected layers.71.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.71.attn.q_proj.weight to be bf16, got torch.float32Expected layers.62.sa_norm.scale to be bf16, got torch.float32
Expected layers.72.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.75.mlp.w1.weight to be bf16, got torch.float32

Expected layers.61.mlp.w2.weight to be bf16, got torch.float32
Expected layers.71.attn.output_proj.weight to be bf16, got torch.float32Expected layers.66.sa_norm.scale to be bf16, got torch.float32
Expected layers.76.mlp.w2.weight to be bf16, got torch.float32

Expected layers.71.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.62.attn.q_proj.weight to be bf16, got torch.float32Expected layers.61.mlp.w3.weight to be bf16, got torch.float32
Expected layers.72.mlp_norm.scale to be bf16, got torch.float32Expected layers.75.mlp.w2.weight to be bf16, got torch.float32


Expected layers.71.mlp_norm.scale to be bf16, got torch.float32Expected layers.66.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.76.mlp.w3.weight to be bf16, got torch.float32

Expected layers.71.attn.v_proj.weight to be bf16, got torch.float32Expected layers.62.attn.k_proj.weight to be bf16, got torch.float32Expected layers.62.sa_norm.scale to be bf16, got torch.float32
Expected layers.72.mlp.w1.weight to be bf16, got torch.float32
Expected layers.75.mlp.w3.weight to be bf16, got torch.float32

Expected layers.71.mlp.w1.weight to be bf16, got torch.float32Expected layers.66.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.77.sa_norm.scale to be bf16, got torch.float32
Expected layers.71.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.62.attn.v_proj.weight to be bf16, got torch.float32Expected layers.62.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.72.mlp.w2.weight to be bf16, got torch.float32Expected layers.76.sa_norm.scale to be bf16, got torch.float32

Expected layers.71.mlp.w2.weight to be bf16, got torch.float32
Expected layers.66.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.77.attn.q_proj.weight to be bf16, got torch.float32Expected layers.71.mlp_norm.scale to be bf16, got torch.float32

Expected layers.62.attn.output_proj.weight to be bf16, got torch.float32Expected layers.62.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.72.mlp.w3.weight to be bf16, got torch.float32
Expected layers.76.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.71.mlp.w3.weight to be bf16, got torch.float32

Expected layers.77.attn.k_proj.weight to be bf16, got torch.float32Expected layers.66.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.71.mlp.w1.weight to be bf16, got torch.float32Expected layers.62.mlp_norm.scale to be bf16, got torch.float32
Expected layers.62.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.73.sa_norm.scale to be bf16, got torch.float32Expected layers.76.attn.k_proj.weight to be bf16, got torch.float32


Expected layers.72.sa_norm.scale to be bf16, got torch.float32

Expected layers.77.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.71.mlp.w2.weight to be bf16, got torch.float32Expected layers.66.mlp_norm.scale to be bf16, got torch.float32Expected layers.62.mlp.w1.weight to be bf16, got torch.float32Expected layers.62.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.73.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.76.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.72.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.77.attn.output_proj.weight to be bf16, got torch.float32Expected layers.71.mlp.w3.weight to be bf16, got torch.float32
Expected layers.66.mlp.w1.weight to be bf16, got torch.float32Expected layers.62.mlp.w2.weight to be bf16, got torch.float32
Expected layers.62.mlp_norm.scale to be bf16, got torch.float32
Expected layers.73.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.76.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.72.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.66.mlp.w2.weight to be bf16, got torch.float32
Expected layers.77.mlp_norm.scale to be bf16, got torch.float32Expected layers.72.sa_norm.scale to be bf16, got torch.float32

Expected layers.62.mlp.w3.weight to be bf16, got torch.float32
Expected layers.62.mlp.w1.weight to be bf16, got torch.float32
Expected layers.73.attn.v_proj.weight to be bf16, got torch.float32Expected layers.76.mlp_norm.scale to be bf16, got torch.float32

Expected layers.72.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.77.mlp.w1.weight to be bf16, got torch.float32Expected layers.66.mlp.w3.weight to be bf16, got torch.float32Expected layers.72.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.63.sa_norm.scale to be bf16, got torch.float32Expected layers.62.mlp.w2.weight to be bf16, got torch.float32

Expected layers.73.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.76.mlp.w1.weight to be bf16, got torch.float32

Expected layers.72.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.77.mlp.w2.weight to be bf16, got torch.float32
Expected layers.72.attn.k_proj.weight to be bf16, got torch.float32Expected layers.67.sa_norm.scale to be bf16, got torch.float32Expected layers.63.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.62.mlp.w3.weight to be bf16, got torch.float32
Expected layers.73.mlp_norm.scale to be bf16, got torch.float32
Expected layers.76.mlp.w2.weight to be bf16, got torch.float32
Expected layers.72.mlp_norm.scale to be bf16, got torch.float32

Expected layers.77.mlp.w3.weight to be bf16, got torch.float32
Expected layers.72.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.67.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.63.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.63.sa_norm.scale to be bf16, got torch.float32Expected layers.73.mlp.w1.weight to be bf16, got torch.float32
Expected layers.76.mlp.w3.weight to be bf16, got torch.float32
Expected layers.72.mlp.w1.weight to be bf16, got torch.float32

Expected layers.78.sa_norm.scale to be bf16, got torch.float32
Expected layers.72.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.67.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.63.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.63.attn.q_proj.weight to be bf16, got torch.float32Expected layers.73.mlp.w2.weight to be bf16, got torch.float32
Expected layers.77.sa_norm.scale to be bf16, got torch.float32
Expected layers.72.mlp.w2.weight to be bf16, got torch.float32

Expected layers.78.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.72.mlp_norm.scale to be bf16, got torch.float32
Expected layers.67.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.63.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.63.attn.k_proj.weight to be bf16, got torch.float32Expected layers.73.mlp.w3.weight to be bf16, got torch.float32Expected layers.78.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.77.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.72.mlp.w3.weight to be bf16, got torch.float32


Expected layers.72.mlp.w1.weight to be bf16, got torch.float32

Expected layers.67.attn.output_proj.weight to be bf16, got torch.float32Expected layers.63.mlp_norm.scale to be bf16, got torch.float32
Expected layers.63.attn.v_proj.weight to be bf16, got torch.float32Expected layers.74.sa_norm.scale to be bf16, got torch.float32

Expected layers.78.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.77.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.73.sa_norm.scale to be bf16, got torch.float32Expected layers.72.mlp.w2.weight to be bf16, got torch.float32

Expected layers.67.mlp_norm.scale to be bf16, got torch.float32Expected layers.63.mlp.w1.weight to be bf16, got torch.float32

Expected layers.63.attn.output_proj.weight to be bf16, got torch.float32Expected layers.74.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.78.attn.output_proj.weight to be bf16, got torch.float32Expected layers.77.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.73.attn.q_proj.weight to be bf16, got torch.float32Expected layers.72.mlp.w3.weight to be bf16, got torch.float32

Expected layers.67.mlp.w1.weight to be bf16, got torch.float32Expected layers.63.mlp.w2.weight to be bf16, got torch.float32

Expected layers.63.mlp_norm.scale to be bf16, got torch.float32Expected layers.74.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.78.mlp_norm.scale to be bf16, got torch.float32Expected layers.77.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.73.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.73.sa_norm.scale to be bf16, got torch.float32
Expected layers.67.mlp.w2.weight to be bf16, got torch.float32Expected layers.63.mlp.w3.weight to be bf16, got torch.float32

Expected layers.63.mlp.w1.weight to be bf16, got torch.float32Expected layers.74.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.78.mlp.w1.weight to be bf16, got torch.float32
Expected layers.77.mlp_norm.scale to be bf16, got torch.float32
Expected layers.73.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.73.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.67.mlp.w3.weight to be bf16, got torch.float32Expected layers.64.sa_norm.scale to be bf16, got torch.float32
Expected layers.63.mlp.w2.weight to be bf16, got torch.float32
Expected layers.74.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.78.mlp.w2.weight to be bf16, got torch.float32
Expected layers.77.mlp.w1.weight to be bf16, got torch.float32
Expected layers.73.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.73.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.68.sa_norm.scale to be bf16, got torch.float32Expected layers.64.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.63.mlp.w3.weight to be bf16, got torch.float32
Expected layers.74.mlp_norm.scale to be bf16, got torch.float32
Expected layers.78.mlp.w3.weight to be bf16, got torch.float32

Expected layers.77.mlp.w2.weight to be bf16, got torch.float32

Expected layers.73.mlp_norm.scale to be bf16, got torch.float32Expected layers.73.attn.v_proj.weight to be bf16, got torch.float32Expected layers.68.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.64.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.64.sa_norm.scale to be bf16, got torch.float32

Expected layers.74.mlp.w1.weight to be bf16, got torch.float32Expected layers.79.sa_norm.scale to be bf16, got torch.float32

Expected layers.77.mlp.w3.weight to be bf16, got torch.float32
Expected layers.73.mlp.w1.weight to be bf16, got torch.float32
Expected layers.73.attn.output_proj.weight to be bf16, got torch.float32Expected layers.68.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.64.attn.v_proj.weight to be bf16, got torch.float32Expected layers.64.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.74.mlp.w2.weight to be bf16, got torch.float32Expected layers.79.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.78.sa_norm.scale to be bf16, got torch.float32Expected layers.73.mlp.w2.weight to be bf16, got torch.float32

Expected layers.73.mlp_norm.scale to be bf16, got torch.float32Expected layers.68.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.64.attn.output_proj.weight to be bf16, got torch.float32Expected layers.64.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.74.mlp.w3.weight to be bf16, got torch.float32Expected layers.79.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.78.attn.q_proj.weight to be bf16, got torch.float32Expected layers.73.mlp.w3.weight to be bf16, got torch.float32

Expected layers.73.mlp.w1.weight to be bf16, got torch.float32Expected layers.68.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.64.mlp_norm.scale to be bf16, got torch.float32Expected layers.64.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.75.sa_norm.scale to be bf16, got torch.float32Expected layers.74.sa_norm.scale to be bf16, got torch.float32Expected layers.79.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.78.attn.k_proj.weight to be bf16, got torch.float32


Expected layers.73.mlp.w2.weight to be bf16, got torch.float32
Expected layers.68.mlp_norm.scale to be bf16, got torch.float32Expected layers.64.mlp.w1.weight to be bf16, got torch.float32Expected layers.64.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.75.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.74.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.79.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.78.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.73.mlp.w3.weight to be bf16, got torch.float32

Expected layers.64.mlp.w2.weight to be bf16, got torch.float32Expected layers.68.mlp.w1.weight to be bf16, got torch.float32
Expected layers.64.mlp_norm.scale to be bf16, got torch.float32Expected layers.75.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.74.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.79.mlp_norm.scale to be bf16, got torch.float32

Expected layers.78.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.74.sa_norm.scale to be bf16, got torch.float32
Expected layers.64.mlp.w3.weight to be bf16, got torch.float32

Expected layers.64.mlp.w1.weight to be bf16, got torch.float32Expected layers.68.mlp.w2.weight to be bf16, got torch.float32Expected layers.75.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.74.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.79.mlp.w1.weight to be bf16, got torch.float32

Expected layers.78.mlp_norm.scale to be bf16, got torch.float32
Expected layers.74.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.65.sa_norm.scale to be bf16, got torch.float32
Expected layers.64.mlp.w2.weight to be bf16, got torch.float32
Expected layers.75.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.74.attn.output_proj.weight to be bf16, got torch.float32Expected layers.68.mlp.w3.weight to be bf16, got torch.float32
Expected layers.79.mlp.w2.weight to be bf16, got torch.float32
Expected layers.78.mlp.w1.weight to be bf16, got torch.float32

Expected layers.74.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.65.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.64.mlp.w3.weight to be bf16, got torch.float32
Expected layers.75.mlp_norm.scale to be bf16, got torch.float32
Expected layers.74.mlp_norm.scale to be bf16, got torch.float32
Expected layers.79.mlp.w3.weight to be bf16, got torch.float32
Expected layers.69.sa_norm.scale to be bf16, got torch.float32Expected layers.78.mlp.w2.weight to be bf16, got torch.float32
Expected layers.74.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.65.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.65.sa_norm.scale to be bf16, got torch.float32
Expected layers.75.mlp.w1.weight to be bf16, got torch.float32

Expected layers.74.mlp.w1.weight to be bf16, got torch.float32
Expected norm.scale to be bf16, got torch.float32
Expected layers.69.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.78.mlp.w3.weight to be bf16, got torch.float32Expected layers.65.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.74.attn.output_proj.weight to be bf16, got torch.float32Expected layers.65.attn.v_proj.weight to be bf16, got torch.float32


Expected layers.75.mlp.w2.weight to be bf16, got torch.float32
Expected layers.74.mlp.w2.weight to be bf16, got torch.float32
Expected output.weight to be bf16, got torch.float32

Expected layers.69.attn.k_proj.weight to be bf16, got torch.float32Expected layers.79.sa_norm.scale to be bf16, got torch.float32
Expected layers.65.attn.k_proj.weight to be bf16, got torch.float32Expected layers.74.mlp_norm.scale to be bf16, got torch.float32
Expected layers.65.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.75.mlp.w3.weight to be bf16, got torch.float32
Expected layers.74.mlp.w3.weight to be bf16, got torch.float32


Expected layers.69.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.79.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.65.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.74.mlp.w1.weight to be bf16, got torch.float32Expected layers.65.mlp_norm.scale to be bf16, got torch.float32
Expected layers.76.sa_norm.scale to be bf16, got torch.float32Expected layers.75.sa_norm.scale to be bf16, got torch.float32

Expected layers.69.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.79.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.65.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.74.mlp.w2.weight to be bf16, got torch.float32
Expected layers.65.mlp.w1.weight to be bf16, got torch.float32Expected layers.76.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.75.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.69.mlp_norm.scale to be bf16, got torch.float32
Expected layers.79.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.65.mlp_norm.scale to be bf16, got torch.float32

Expected layers.74.mlp.w3.weight to be bf16, got torch.float32
Expected layers.65.mlp.w2.weight to be bf16, got torch.float32Expected layers.76.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.75.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.65.mlp.w1.weight to be bf16, got torch.float32Expected layers.69.mlp.w1.weight to be bf16, got torch.float32
Expected layers.79.attn.output_proj.weight to be bf16, got torch.float32


Expected layers.75.sa_norm.scale to be bf16, got torch.float32
Expected layers.65.mlp.w3.weight to be bf16, got torch.float32Expected layers.75.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.76.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.65.mlp.w2.weight to be bf16, got torch.float32
Expected layers.69.mlp.w2.weight to be bf16, got torch.float32
Expected layers.79.mlp_norm.scale to be bf16, got torch.float32
Expected layers.75.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.66.sa_norm.scale to be bf16, got torch.float32
Expected layers.75.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.76.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.65.mlp.w3.weight to be bf16, got torch.float32
Expected layers.69.mlp.w3.weight to be bf16, got torch.float32Expected layers.79.mlp.w1.weight to be bf16, got torch.float32
Expected layers.75.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.66.attn.q_proj.weight to be bf16, got torch.float32Expected layers.75.mlp_norm.scale to be bf16, got torch.float32
Expected layers.76.mlp_norm.scale to be bf16, got torch.float32
Expected layers.66.sa_norm.scale to be bf16, got torch.float32
Expected layers.70.sa_norm.scale to be bf16, got torch.float32
Expected layers.79.mlp.w2.weight to be bf16, got torch.float32
Expected layers.75.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.66.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.75.mlp.w1.weight to be bf16, got torch.float32
Expected layers.76.mlp.w1.weight to be bf16, got torch.float32

Expected layers.66.attn.q_proj.weight to be bf16, got torch.float32Expected layers.70.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.79.mlp.w3.weight to be bf16, got torch.float32
Expected layers.75.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.66.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.75.mlp.w2.weight to be bf16, got torch.float32
Expected layers.76.mlp.w2.weight to be bf16, got torch.float32

Expected layers.66.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.70.attn.k_proj.weight to be bf16, got torch.float32Expected norm.scale to be bf16, got torch.float32
Expected layers.75.mlp_norm.scale to be bf16, got torch.float32
Expected layers.66.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.75.mlp.w3.weight to be bf16, got torch.float32
Expected layers.76.mlp.w3.weight to be bf16, got torch.float32

Expected layers.66.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.70.attn.v_proj.weight to be bf16, got torch.float32Expected output.weight to be bf16, got torch.float32
Expected layers.75.mlp.w1.weight to be bf16, got torch.float32
Expected layers.66.mlp_norm.scale to be bf16, got torch.float32
Expected layers.76.sa_norm.scale to be bf16, got torch.float32
Expected layers.77.sa_norm.scale to be bf16, got torch.float32

Expected layers.66.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.70.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.75.mlp.w2.weight to be bf16, got torch.float32
Expected layers.66.mlp.w1.weight to be bf16, got torch.float32
Expected layers.76.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.77.attn.q_proj.weight to be bf16, got torch.float32Expected layers.66.mlp_norm.scale to be bf16, got torch.float32
Expected layers.70.mlp_norm.scale to be bf16, got torch.float32

Expected layers.75.mlp.w3.weight to be bf16, got torch.float32
Expected layers.66.mlp.w2.weight to be bf16, got torch.float32
Expected layers.76.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.77.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.66.mlp.w1.weight to be bf16, got torch.float32
Expected layers.70.mlp.w1.weight to be bf16, got torch.float32Expected layers.76.sa_norm.scale to be bf16, got torch.float32

Expected layers.66.mlp.w3.weight to be bf16, got torch.float32
Expected layers.76.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.77.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.66.mlp.w2.weight to be bf16, got torch.float32
Expected layers.70.mlp.w2.weight to be bf16, got torch.float32Expected layers.76.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.67.sa_norm.scale to be bf16, got torch.float32

Expected layers.76.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.77.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.66.mlp.w3.weight to be bf16, got torch.float32
Expected layers.70.mlp.w3.weight to be bf16, got torch.float32Expected layers.76.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.67.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.76.mlp_norm.scale to be bf16, got torch.float32
Expected layers.77.mlp_norm.scale to be bf16, got torch.float32
Expected layers.67.sa_norm.scale to be bf16, got torch.float32
Expected layers.71.sa_norm.scale to be bf16, got torch.float32Expected layers.76.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.67.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.76.mlp.w1.weight to be bf16, got torch.float32
Expected layers.77.mlp.w1.weight to be bf16, got torch.float32
Expected layers.67.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.76.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.67.attn.v_proj.weight to be bf16, got torch.float32Expected layers.71.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.76.mlp.w2.weight to be bf16, got torch.float32

Expected layers.77.mlp.w2.weight to be bf16, got torch.float32
Expected layers.67.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.76.mlp_norm.scale to be bf16, got torch.float32
Expected layers.67.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.76.mlp.w3.weight to be bf16, got torch.float32
Expected layers.71.attn.k_proj.weight to be bf16, got torch.float32Expected layers.77.mlp.w3.weight to be bf16, got torch.float32
Expected layers.67.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.76.mlp.w1.weight to be bf16, got torch.float32Expected layers.67.mlp_norm.scale to be bf16, got torch.float32

Expected layers.77.sa_norm.scale to be bf16, got torch.float32
Expected layers.71.attn.v_proj.weight to be bf16, got torch.float32Expected layers.78.sa_norm.scale to be bf16, got torch.float32

Expected layers.67.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.76.mlp.w2.weight to be bf16, got torch.float32Expected layers.67.mlp.w1.weight to be bf16, got torch.float32


Expected layers.77.attn.q_proj.weight to be bf16, got torch.float32Expected layers.71.attn.output_proj.weight to be bf16, got torch.float32Expected layers.78.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.67.mlp_norm.scale to be bf16, got torch.float32
Expected layers.76.mlp.w3.weight to be bf16, got torch.float32Expected layers.67.mlp.w2.weight to be bf16, got torch.float32


Expected layers.77.attn.k_proj.weight to be bf16, got torch.float32Expected layers.71.mlp_norm.scale to be bf16, got torch.float32Expected layers.78.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.67.mlp.w1.weight to be bf16, got torch.float32

Expected layers.77.sa_norm.scale to be bf16, got torch.float32Expected layers.67.mlp.w3.weight to be bf16, got torch.float32Expected layers.77.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.71.mlp.w1.weight to be bf16, got torch.float32
Expected layers.67.mlp.w2.weight to be bf16, got torch.float32Expected layers.78.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.77.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.68.sa_norm.scale to be bf16, got torch.float32
Expected layers.77.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.71.mlp.w2.weight to be bf16, got torch.float32
Expected layers.67.mlp.w3.weight to be bf16, got torch.float32
Expected layers.78.attn.output_proj.weight to be bf16, got torch.float32Expected layers.77.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.68.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.77.mlp_norm.scale to be bf16, got torch.float32
Expected layers.68.sa_norm.scale to be bf16, got torch.float32Expected layers.71.mlp.w3.weight to be bf16, got torch.float32
Expected layers.77.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.78.mlp_norm.scale to be bf16, got torch.float32

Expected layers.68.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.77.mlp.w1.weight to be bf16, got torch.float32
Expected layers.68.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.77.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.72.sa_norm.scale to be bf16, got torch.float32Expected layers.78.mlp.w1.weight to be bf16, got torch.float32
Expected layers.68.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.77.mlp.w2.weight to be bf16, got torch.float32

Expected layers.68.attn.k_proj.weight to be bf16, got torch.float32Expected layers.77.mlp_norm.scale to be bf16, got torch.float32
Expected layers.78.mlp.w2.weight to be bf16, got torch.float32
Expected layers.68.attn.output_proj.weight to be bf16, got torch.float32Expected layers.72.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.77.mlp.w3.weight to be bf16, got torch.float32Expected layers.77.mlp.w1.weight to be bf16, got torch.float32


Expected layers.68.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.78.mlp.w3.weight to be bf16, got torch.float32
Expected layers.68.mlp_norm.scale to be bf16, got torch.float32RV: missing [] unexpected []
Expected layers.78.sa_norm.scale to be bf16, got torch.float32
Expected layers.77.mlp.w2.weight to be bf16, got torch.float32Expected layers.72.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.68.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.79.sa_norm.scale to be bf16, got torch.float32Expected layers.68.mlp.w1.weight to be bf16, got torch.float32

Expected layers.78.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.77.mlp.w3.weight to be bf16, got torch.float32

Expected layers.79.attn.q_proj.weight to be bf16, got torch.float32Expected layers.68.mlp_norm.scale to be bf16, got torch.float32Expected layers.72.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.68.mlp.w2.weight to be bf16, got torch.float32

Expected layers.78.attn.k_proj.weight to be bf16, got torch.float32Expected layers.78.sa_norm.scale to be bf16, got torch.float32

Expected layers.79.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.68.mlp.w1.weight to be bf16, got torch.float32Expected layers.68.mlp.w3.weight to be bf16, got torch.float32
Expected layers.72.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.78.attn.v_proj.weight to be bf16, got torch.float32Expected layers.78.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.79.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.69.sa_norm.scale to be bf16, got torch.float32
Expected layers.68.mlp.w2.weight to be bf16, got torch.float32

Expected layers.78.attn.output_proj.weight to be bf16, got torch.float32Expected layers.78.attn.k_proj.weight to be bf16, got torch.float32Expected layers.72.mlp_norm.scale to be bf16, got torch.float32
Expected layers.79.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.69.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.78.mlp_norm.scale to be bf16, got torch.float32Expected layers.68.mlp.w3.weight to be bf16, got torch.float32

Expected layers.78.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.72.mlp.w1.weight to be bf16, got torch.float32Expected layers.79.mlp_norm.scale to be bf16, got torch.float32
Expected layers.78.mlp.w1.weight to be bf16, got torch.float32
Expected layers.69.attn.k_proj.weight to be bf16, got torch.float32


Expected layers.69.sa_norm.scale to be bf16, got torch.float32Expected layers.78.attn.output_proj.weight to be bf16, got torch.float32Expected layers.72.mlp.w2.weight to be bf16, got torch.float32

Expected layers.79.mlp.w1.weight to be bf16, got torch.float32
Expected layers.78.mlp.w2.weight to be bf16, got torch.float32Expected layers.69.attn.v_proj.weight to be bf16, got torch.float32


Expected layers.78.mlp_norm.scale to be bf16, got torch.float32Expected layers.69.attn.q_proj.weight to be bf16, got torch.float32Expected layers.79.mlp.w2.weight to be bf16, got torch.float32Expected layers.72.mlp.w3.weight to be bf16, got torch.float32


Expected layers.78.mlp.w3.weight to be bf16, got torch.float32Expected layers.69.attn.output_proj.weight to be bf16, got torch.float32


Expected layers.78.mlp.w1.weight to be bf16, got torch.float32Expected layers.79.mlp.w3.weight to be bf16, got torch.float32
Expected layers.69.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.73.sa_norm.scale to be bf16, got torch.float32Expected layers.79.sa_norm.scale to be bf16, got torch.float32
Expected layers.69.mlp_norm.scale to be bf16, got torch.float32

Expected layers.78.mlp.w2.weight to be bf16, got torch.float32
Expected norm.scale to be bf16, got torch.float32

Expected layers.73.attn.q_proj.weight to be bf16, got torch.float32Expected layers.69.attn.v_proj.weight to be bf16, got torch.float32Expected layers.79.attn.q_proj.weight to be bf16, got torch.float32Expected layers.69.mlp.w1.weight to be bf16, got torch.float32

Expected layers.78.mlp.w3.weight to be bf16, got torch.float32Expected output.weight to be bf16, got torch.float32



Expected layers.73.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.69.attn.output_proj.weight to be bf16, got torch.float32Expected layers.69.mlp.w2.weight to be bf16, got torch.float32Expected layers.79.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.79.sa_norm.scale to be bf16, got torch.float32

Expected layers.73.attn.v_proj.weight to be bf16, got torch.float32
RV: missing [] unexpected []
Expected layers.69.mlp.w3.weight to be bf16, got torch.float32
Expected layers.69.mlp_norm.scale to be bf16, got torch.float32Expected layers.79.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.79.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.73.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.70.sa_norm.scale to be bf16, got torch.float32
Expected layers.79.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.69.mlp.w1.weight to be bf16, got torch.float32Expected layers.79.attn.k_proj.weight to be bf16, got torch.float32Expected layers.73.mlp_norm.scale to be bf16, got torch.float32

Expected layers.70.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.79.mlp_norm.scale to be bf16, got torch.float32

Expected layers.79.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.69.mlp.w2.weight to be bf16, got torch.float32Expected layers.73.mlp.w1.weight to be bf16, got torch.float32
Expected layers.70.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.79.mlp.w1.weight to be bf16, got torch.float32
Expected layers.79.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.73.mlp.w2.weight to be bf16, got torch.float32Expected layers.69.mlp.w3.weight to be bf16, got torch.float32
Expected layers.70.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.79.mlp.w2.weight to be bf16, got torch.float32
Expected layers.79.mlp_norm.scale to be bf16, got torch.float32
Expected layers.73.mlp.w3.weight to be bf16, got torch.float32

Expected layers.70.sa_norm.scale to be bf16, got torch.float32Expected layers.70.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.79.mlp.w3.weight to be bf16, got torch.float32
Expected layers.79.mlp.w1.weight to be bf16, got torch.float32
Expected layers.74.sa_norm.scale to be bf16, got torch.float32

Expected layers.70.mlp_norm.scale to be bf16, got torch.float32Expected layers.70.attn.q_proj.weight to be bf16, got torch.float32
Expected norm.scale to be bf16, got torch.float32
Expected layers.79.mlp.w2.weight to be bf16, got torch.float32
Expected layers.74.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.70.mlp.w1.weight to be bf16, got torch.float32

Expected output.weight to be bf16, got torch.float32Expected layers.70.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.79.mlp.w3.weight to be bf16, got torch.float32
Expected layers.70.mlp.w2.weight to be bf16, got torch.float32
Expected layers.74.attn.k_proj.weight to be bf16, got torch.float32

Expected norm.scale to be bf16, got torch.float32Expected layers.70.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.70.mlp.w3.weight to be bf16, got torch.float32Expected layers.74.attn.v_proj.weight to be bf16, got torch.float32

Expected output.weight to be bf16, got torch.float32
Expected layers.70.attn.output_proj.weight to be bf16, got torch.float32Expected layers.71.sa_norm.scale to be bf16, got torch.float32
Expected layers.74.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.71.attn.q_proj.weight to be bf16, got torch.float32Expected layers.70.mlp_norm.scale to be bf16, got torch.float32
Expected layers.74.mlp_norm.scale to be bf16, got torch.float32

Expected layers.71.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.74.mlp.w1.weight to be bf16, got torch.float32
Expected layers.70.mlp.w1.weight to be bf16, got torch.float32
Expected layers.71.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.74.mlp.w2.weight to be bf16, got torch.float32
Expected layers.71.attn.output_proj.weight to be bf16, got torch.float32Expected layers.70.mlp.w2.weight to be bf16, got torch.float32

Expected layers.74.mlp.w3.weight to be bf16, got torch.float32
Expected layers.71.mlp_norm.scale to be bf16, got torch.float32
Expected layers.75.sa_norm.scale to be bf16, got torch.float32Expected layers.70.mlp.w3.weight to be bf16, got torch.float32

Expected layers.71.mlp.w1.weight to be bf16, got torch.float32
Expected layers.75.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.71.mlp.w2.weight to be bf16, got torch.float32Expected layers.71.sa_norm.scale to be bf16, got torch.float32

Expected layers.75.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.71.mlp.w3.weight to be bf16, got torch.float32
Expected layers.75.attn.v_proj.weight to be bf16, got torch.float32Expected layers.71.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.72.sa_norm.scale to be bf16, got torch.float32
Expected layers.75.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.72.attn.q_proj.weight to be bf16, got torch.float32Expected layers.71.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.75.mlp_norm.scale to be bf16, got torch.float32
Expected layers.72.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.75.mlp.w1.weight to be bf16, got torch.float32Expected layers.71.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.72.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.75.mlp.w2.weight to be bf16, got torch.float32
Expected layers.72.attn.output_proj.weight to be bf16, got torch.float32Expected layers.71.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.75.mlp.w3.weight to be bf16, got torch.float32
Expected layers.72.mlp_norm.scale to be bf16, got torch.float32
Expected layers.76.sa_norm.scale to be bf16, got torch.float32Expected layers.71.mlp_norm.scale to be bf16, got torch.float32

Expected layers.72.mlp.w1.weight to be bf16, got torch.float32
Expected layers.76.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.72.mlp.w2.weight to be bf16, got torch.float32Expected layers.71.mlp.w1.weight to be bf16, got torch.float32

Expected layers.76.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.72.mlp.w3.weight to be bf16, got torch.float32
Expected layers.71.mlp.w2.weight to be bf16, got torch.float32
Expected layers.76.attn.v_proj.weight to be bf16, got torch.float32Expected layers.73.sa_norm.scale to be bf16, got torch.float32

Expected layers.71.mlp.w3.weight to be bf16, got torch.float32Expected layers.76.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.73.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.76.mlp_norm.scale to be bf16, got torch.float32Expected layers.72.sa_norm.scale to be bf16, got torch.float32Expected layers.73.attn.k_proj.weight to be bf16, got torch.float32


Expected layers.76.mlp.w1.weight to be bf16, got torch.float32
Expected layers.73.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.72.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.76.mlp.w2.weight to be bf16, got torch.float32
Expected layers.73.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.76.mlp.w3.weight to be bf16, got torch.float32Expected layers.72.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.73.mlp_norm.scale to be bf16, got torch.float32

Expected layers.77.sa_norm.scale to be bf16, got torch.float32
Expected layers.73.mlp.w1.weight to be bf16, got torch.float32
Expected layers.72.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.77.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.73.mlp.w2.weight to be bf16, got torch.float32
Expected layers.77.attn.k_proj.weight to be bf16, got torch.float32Expected layers.72.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.73.mlp.w3.weight to be bf16, got torch.float32
Expected layers.77.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.74.sa_norm.scale to be bf16, got torch.float32
Expected layers.72.mlp_norm.scale to be bf16, got torch.float32
Expected layers.77.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.74.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.77.mlp_norm.scale to be bf16, got torch.float32Expected layers.72.mlp.w1.weight to be bf16, got torch.float32

Expected layers.74.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.77.mlp.w1.weight to be bf16, got torch.float32
Expected layers.74.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.72.mlp.w2.weight to be bf16, got torch.float32
Expected layers.77.mlp.w2.weight to be bf16, got torch.float32
Expected layers.74.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.77.mlp.w3.weight to be bf16, got torch.float32
Expected layers.72.mlp.w3.weight to be bf16, got torch.float32Expected layers.74.mlp_norm.scale to be bf16, got torch.float32

Expected layers.78.sa_norm.scale to be bf16, got torch.float32
Expected layers.74.mlp.w1.weight to be bf16, got torch.float32
Expected layers.78.attn.q_proj.weight to be bf16, got torch.float32Expected layers.73.sa_norm.scale to be bf16, got torch.float32

Expected layers.74.mlp.w2.weight to be bf16, got torch.float32
Expected layers.78.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.74.mlp.w3.weight to be bf16, got torch.float32Expected layers.73.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.78.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.75.sa_norm.scale to be bf16, got torch.float32
Expected layers.78.attn.output_proj.weight to be bf16, got torch.float32Expected layers.73.attn.k_proj.weight to be bf16, got torch.float32

Expected layers.75.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.78.mlp_norm.scale to be bf16, got torch.float32
Expected layers.75.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.73.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.78.mlp.w1.weight to be bf16, got torch.float32
Expected layers.75.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.78.mlp.w2.weight to be bf16, got torch.float32
Expected layers.73.attn.output_proj.weight to be bf16, got torch.float32Expected layers.75.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.78.mlp.w3.weight to be bf16, got torch.float32
Expected layers.75.mlp_norm.scale to be bf16, got torch.float32
Expected layers.79.sa_norm.scale to be bf16, got torch.float32Expected layers.73.mlp_norm.scale to be bf16, got torch.float32

Expected layers.75.mlp.w1.weight to be bf16, got torch.float32
Expected layers.79.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.75.mlp.w2.weight to be bf16, got torch.float32
Expected layers.73.mlp.w1.weight to be bf16, got torch.float32
Expected layers.79.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.75.mlp.w3.weight to be bf16, got torch.float32
Expected layers.79.attn.v_proj.weight to be bf16, got torch.float32Expected layers.73.mlp.w2.weight to be bf16, got torch.float32

Expected layers.76.sa_norm.scale to be bf16, got torch.float32
Expected layers.79.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.73.mlp.w3.weight to be bf16, got torch.float32
Expected layers.76.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.79.mlp_norm.scale to be bf16, got torch.float32
Expected layers.74.sa_norm.scale to be bf16, got torch.float32
Expected layers.76.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.79.mlp.w1.weight to be bf16, got torch.float32
Expected layers.74.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.76.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.79.mlp.w2.weight to be bf16, got torch.float32
Expected layers.74.attn.k_proj.weight to be bf16, got torch.float32
RV: missing [] unexpected []Expected layers.76.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.79.mlp.w3.weight to be bf16, got torch.float32
Expected layers.74.attn.v_proj.weight to be bf16, got torch.float32

Expected layers.76.mlp_norm.scale to be bf16, got torch.float32
Expected norm.scale to be bf16, got torch.float32Expected layers.74.attn.output_proj.weight to be bf16, got torch.float32

Expected layers.76.mlp.w1.weight to be bf16, got torch.float32
Expected output.weight to be bf16, got torch.float32Expected layers.74.mlp_norm.scale to be bf16, got torch.float32

Expected layers.76.mlp.w2.weight to be bf16, got torch.float32
Expected layers.74.mlp.w1.weight to be bf16, got torch.float32
Expected layers.76.mlp.w3.weight to be bf16, got torch.float32
Expected layers.74.mlp.w2.weight to be bf16, got torch.float32
Expected layers.77.sa_norm.scale to be bf16, got torch.float32
Expected layers.74.mlp.w3.weight to be bf16, got torch.float32
Expected layers.77.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.75.sa_norm.scale to be bf16, got torch.float32
Expected layers.77.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.75.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.77.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.75.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.77.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.75.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.77.mlp_norm.scale to be bf16, got torch.float32
Expected layers.75.attn.output_proj.weight to be bf16, got torch.float32
RV: missing [] unexpected []Expected layers.77.mlp.w1.weight to be bf16, got torch.float32

Expected layers.75.mlp_norm.scale to be bf16, got torch.float32
Expected layers.77.mlp.w2.weight to be bf16, got torch.float32
Expected layers.75.mlp.w1.weight to be bf16, got torch.float32
Expected layers.77.mlp.w3.weight to be bf16, got torch.float32
Expected layers.75.mlp.w2.weight to be bf16, got torch.float32
Expected layers.78.sa_norm.scale to be bf16, got torch.float32
Expected layers.75.mlp.w3.weight to be bf16, got torch.float32
Expected layers.78.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.76.sa_norm.scale to be bf16, got torch.float32
Expected layers.78.attn.k_proj.weight to be bf16, got torch.float32
RV: missing [] unexpected []Expected layers.76.attn.q_proj.weight to be bf16, got torch.float32

Expected layers.78.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.76.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.78.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.76.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.78.mlp_norm.scale to be bf16, got torch.float32
Expected layers.76.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.78.mlp.w1.weight to be bf16, got torch.float32
Expected layers.76.mlp_norm.scale to be bf16, got torch.float32
Expected layers.78.mlp.w2.weight to be bf16, got torch.float32
Expected layers.76.mlp.w1.weight to be bf16, got torch.float32
Expected layers.78.mlp.w3.weight to be bf16, got torch.float32
Expected layers.76.mlp.w2.weight to be bf16, got torch.float32
Expected layers.79.sa_norm.scale to be bf16, got torch.float32
Expected layers.76.mlp.w3.weight to be bf16, got torch.float32
Expected layers.79.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.77.sa_norm.scale to be bf16, got torch.float32
Expected layers.79.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.77.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.79.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.77.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.79.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.77.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.79.mlp_norm.scale to be bf16, got torch.float32
Expected layers.77.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.79.mlp.w1.weight to be bf16, got torch.float32
Expected layers.77.mlp_norm.scale to be bf16, got torch.float32
Expected layers.79.mlp.w2.weight to be bf16, got torch.float32
Expected layers.77.mlp.w1.weight to be bf16, got torch.float32
Expected layers.79.mlp.w3.weight to be bf16, got torch.float32
Expected layers.77.mlp.w2.weight to be bf16, got torch.float32
Expected norm.scale to be bf16, got torch.float32
Expected layers.77.mlp.w3.weight to be bf16, got torch.float32
Expected output.weight to be bf16, got torch.float32
Expected layers.78.sa_norm.scale to be bf16, got torch.float32
Expected layers.78.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.78.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.78.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.78.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.78.mlp_norm.scale to be bf16, got torch.float32
Expected layers.78.mlp.w1.weight to be bf16, got torch.float32
Expected layers.78.mlp.w2.weight to be bf16, got torch.float32
Expected layers.78.mlp.w3.weight to be bf16, got torch.float32
Expected layers.79.sa_norm.scale to be bf16, got torch.float32
Expected layers.79.attn.q_proj.weight to be bf16, got torch.float32
Expected layers.79.attn.k_proj.weight to be bf16, got torch.float32
Expected layers.79.attn.v_proj.weight to be bf16, got torch.float32
Expected layers.79.attn.output_proj.weight to be bf16, got torch.float32
Expected layers.79.mlp_norm.scale to be bf16, got torch.float32
Expected layers.79.mlp.w1.weight to be bf16, got torch.float32
Expected layers.79.mlp.w2.weight to be bf16, got torch.float32
Expected layers.79.mlp.w3.weight to be bf16, got torch.float32
Expected norm.scale to be bf16, got torch.float32
Expected output.weight to be bf16, got torch.float32
RV: missing [] unexpected []
RV: missing [] unexpected []
RV: missing [] unexpected []
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank2]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank0]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
RV: freqs device meta
RV: freqs device cuda:2
RV: freqs device meta
RV: freqs device meta
RV: freqs device cuda:5
RV: freqs device cuda:4
RV: freqs device meta
RV: freqs device cuda:1
RV: freqs device meta
RV: freqs device meta
RV: freqs device cuda:3
RV: freqs device cuda:6
RV: freqs device meta
RV: freqs device meta
RV: freqs device cuda:7
RV: freqs device cuda:0
Expected tok_embeddings.weight to be bf16, got torch.float32
Expected layers.0.attention.wq.weight to be bf16, got torch.float32
Expected layers.0.attention.wk.weight to be bf16, got torch.float32
Expected layers.0.attention.wv.weight to be bf16, got torch.float32
Expected layers.0.attention.wo.weight to be bf16, got torch.float32
Expected layers.0.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.0.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.0.feed_forward.w3.weight to be bf16, got torch.float32
Expected tok_embeddings.weight to be bf16, got torch.float32Expected layers.0.attention_norm.weight to be bf16, got torch.float32

Expected layers.0.ffn_norm.weight to be bf16, got torch.float32Expected layers.0.attention.wq.weight to be bf16, got torch.float32

Expected layers.1.attention.wq.weight to be bf16, got torch.float32Expected layers.0.attention.wk.weight to be bf16, got torch.float32

Expected layers.1.attention.wk.weight to be bf16, got torch.float32Expected layers.0.attention.wv.weight to be bf16, got torch.float32

Expected layers.1.attention.wv.weight to be bf16, got torch.float32
Expected layers.0.attention.wo.weight to be bf16, got torch.float32
Expected layers.1.attention.wo.weight to be bf16, got torch.float32
Expected layers.0.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.1.feed_forward.w1.weight to be bf16, got torch.float32Expected tok_embeddings.weight to be bf16, got torch.float32
Expected layers.0.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.1.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.0.attention.wq.weight to be bf16, got torch.float32Expected layers.0.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.1.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.0.attention.wk.weight to be bf16, got torch.float32Expected layers.0.attention_norm.weight to be bf16, got torch.float32

Expected layers.1.attention_norm.weight to be bf16, got torch.float32
Expected layers.0.attention.wv.weight to be bf16, got torch.float32Expected layers.0.ffn_norm.weight to be bf16, got torch.float32
Expected layers.1.ffn_norm.weight to be bf16, got torch.float32

Expected layers.0.attention.wo.weight to be bf16, got torch.float32
Expected layers.1.attention.wq.weight to be bf16, got torch.float32Expected layers.2.attention.wq.weight to be bf16, got torch.float32

Expected layers.0.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.1.attention.wk.weight to be bf16, got torch.float32
Expected layers.2.attention.wk.weight to be bf16, got torch.float32Expected layers.0.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.1.attention.wv.weight to be bf16, got torch.float32
Expected layers.0.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.2.attention.wv.weight to be bf16, got torch.float32
Expected layers.1.attention.wo.weight to be bf16, got torch.float32

Expected layers.0.attention_norm.weight to be bf16, got torch.float32
Expected layers.2.attention.wo.weight to be bf16, got torch.float32
Expected layers.1.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.0.ffn_norm.weight to be bf16, got torch.float32
Expected layers.2.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.1.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.1.attention.wq.weight to be bf16, got torch.float32
Expected layers.2.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.1.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.1.attention.wk.weight to be bf16, got torch.float32
Expected layers.2.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.1.attention_norm.weight to be bf16, got torch.float32
Expected layers.1.attention.wv.weight to be bf16, got torch.float32
Expected layers.2.attention_norm.weight to be bf16, got torch.float32
Expected layers.1.ffn_norm.weight to be bf16, got torch.float32
Expected layers.1.attention.wo.weight to be bf16, got torch.float32Expected layers.2.ffn_norm.weight to be bf16, got torch.float32
Expected layers.2.attention.wq.weight to be bf16, got torch.float32

Expected layers.1.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.3.attention.wq.weight to be bf16, got torch.float32

Expected layers.2.attention.wk.weight to be bf16, got torch.float32
Expected layers.1.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.3.attention.wk.weight to be bf16, got torch.float32

Expected layers.2.attention.wv.weight to be bf16, got torch.float32
Expected layers.1.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.3.attention.wv.weight to be bf16, got torch.float32

Expected layers.2.attention.wo.weight to be bf16, got torch.float32
Expected layers.1.attention_norm.weight to be bf16, got torch.float32Expected layers.3.attention.wo.weight to be bf16, got torch.float32

Expected layers.2.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.1.ffn_norm.weight to be bf16, got torch.float32Expected layers.3.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.2.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.2.attention.wq.weight to be bf16, got torch.float32
Expected layers.3.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.2.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.2.attention.wk.weight to be bf16, got torch.float32
Expected layers.3.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.2.attention_norm.weight to be bf16, got torch.float32

Expected layers.2.attention.wv.weight to be bf16, got torch.float32
Expected layers.3.attention_norm.weight to be bf16, got torch.float32Expected layers.2.ffn_norm.weight to be bf16, got torch.float32

Expected layers.2.attention.wo.weight to be bf16, got torch.float32
Expected layers.3.ffn_norm.weight to be bf16, got torch.float32Expected layers.3.attention.wq.weight to be bf16, got torch.float32

Expected layers.2.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.4.attention.wq.weight to be bf16, got torch.float32Expected layers.3.attention.wk.weight to be bf16, got torch.float32
Expected layers.2.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.4.attention.wk.weight to be bf16, got torch.float32
Expected layers.3.attention.wv.weight to be bf16, got torch.float32Expected layers.2.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.4.attention.wv.weight to be bf16, got torch.float32
Expected layers.3.attention.wo.weight to be bf16, got torch.float32Expected layers.2.attention_norm.weight to be bf16, got torch.float32

Expected layers.4.attention.wo.weight to be bf16, got torch.float32
Expected layers.3.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.2.ffn_norm.weight to be bf16, got torch.float32

Expected layers.4.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.3.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.3.attention.wq.weight to be bf16, got torch.float32

Expected layers.4.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.3.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.3.attention.wk.weight to be bf16, got torch.float32

Expected layers.4.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.3.attention_norm.weight to be bf16, got torch.float32Expected layers.3.attention.wv.weight to be bf16, got torch.float32

Expected layers.4.attention_norm.weight to be bf16, got torch.float32
Expected layers.3.ffn_norm.weight to be bf16, got torch.float32Expected layers.3.attention.wo.weight to be bf16, got torch.float32
Expected layers.4.ffn_norm.weight to be bf16, got torch.float32

Expected layers.4.attention.wq.weight to be bf16, got torch.float32
Expected layers.3.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.5.attention.wq.weight to be bf16, got torch.float32

Expected layers.4.attention.wk.weight to be bf16, got torch.float32
Expected layers.3.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.5.attention.wk.weight to be bf16, got torch.float32

Expected layers.4.attention.wv.weight to be bf16, got torch.float32
Expected layers.3.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.5.attention.wv.weight to be bf16, got torch.float32

Expected layers.4.attention.wo.weight to be bf16, got torch.float32
Expected tok_embeddings.weight to be bf16, got torch.float32Expected layers.3.attention_norm.weight to be bf16, got torch.float32
Expected layers.5.attention.wo.weight to be bf16, got torch.float32

Expected layers.4.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.0.attention.wq.weight to be bf16, got torch.float32Expected layers.3.ffn_norm.weight to be bf16, got torch.float32
Expected layers.5.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.4.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.0.attention.wk.weight to be bf16, got torch.float32
Expected layers.4.attention.wq.weight to be bf16, got torch.float32
Expected layers.5.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.4.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.0.attention.wv.weight to be bf16, got torch.float32
Expected layers.4.attention.wk.weight to be bf16, got torch.float32
Expected layers.5.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.4.attention_norm.weight to be bf16, got torch.float32
Expected tok_embeddings.weight to be bf16, got torch.float32
Expected layers.0.attention.wo.weight to be bf16, got torch.float32Expected layers.4.attention.wv.weight to be bf16, got torch.float32
Expected layers.5.attention_norm.weight to be bf16, got torch.float32


Expected layers.4.ffn_norm.weight to be bf16, got torch.float32
Expected layers.0.attention.wq.weight to be bf16, got torch.float32Expected layers.0.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.4.attention.wo.weight to be bf16, got torch.float32Expected layers.5.ffn_norm.weight to be bf16, got torch.float32
Expected layers.5.attention.wq.weight to be bf16, got torch.float32


Expected layers.0.attention.wk.weight to be bf16, got torch.float32
Expected layers.0.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.4.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.6.attention.wq.weight to be bf16, got torch.float32Expected layers.5.attention.wk.weight to be bf16, got torch.float32

Expected layers.0.attention.wv.weight to be bf16, got torch.float32
Expected layers.0.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.4.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.6.attention.wk.weight to be bf16, got torch.float32Expected layers.5.attention.wv.weight to be bf16, got torch.float32

Expected layers.0.attention.wo.weight to be bf16, got torch.float32
Expected layers.0.attention_norm.weight to be bf16, got torch.float32Expected layers.4.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.6.attention.wv.weight to be bf16, got torch.float32Expected layers.5.attention.wo.weight to be bf16, got torch.float32

Expected layers.0.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.0.ffn_norm.weight to be bf16, got torch.float32Expected layers.4.attention_norm.weight to be bf16, got torch.float32

Expected layers.6.attention.wo.weight to be bf16, got torch.float32Expected layers.5.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.0.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.1.attention.wq.weight to be bf16, got torch.float32Expected layers.4.ffn_norm.weight to be bf16, got torch.float32

Expected layers.6.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.5.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.0.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.5.attention.wq.weight to be bf16, got torch.float32Expected layers.1.attention.wk.weight to be bf16, got torch.float32

Expected layers.6.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.5.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.0.attention_norm.weight to be bf16, got torch.float32
Expected layers.5.attention.wk.weight to be bf16, got torch.float32Expected layers.1.attention.wv.weight to be bf16, got torch.float32

Expected layers.6.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.5.attention_norm.weight to be bf16, got torch.float32

Expected layers.0.ffn_norm.weight to be bf16, got torch.float32Expected layers.5.attention.wv.weight to be bf16, got torch.float32
Expected layers.1.attention.wo.weight to be bf16, got torch.float32
Expected layers.6.attention_norm.weight to be bf16, got torch.float32
Expected layers.5.ffn_norm.weight to be bf16, got torch.float32

Expected layers.1.attention.wq.weight to be bf16, got torch.float32
Expected layers.5.attention.wo.weight to be bf16, got torch.float32Expected layers.1.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.6.ffn_norm.weight to be bf16, got torch.float32
Expected layers.6.attention.wq.weight to be bf16, got torch.float32

Expected layers.1.attention.wk.weight to be bf16, got torch.float32Expected layers.5.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.1.feed_forward.w2.weight to be bf16, got torch.float32Expected tok_embeddings.weight to be bf16, got torch.float32Expected layers.7.attention.wq.weight to be bf16, got torch.float32Expected layers.6.attention.wk.weight to be bf16, got torch.float32

Expected layers.1.attention.wv.weight to be bf16, got torch.float32
Expected layers.5.feed_forward.w2.weight to be bf16, got torch.float32


Expected layers.1.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.0.attention.wq.weight to be bf16, got torch.float32
Expected layers.7.attention.wk.weight to be bf16, got torch.float32
Expected layers.6.attention.wv.weight to be bf16, got torch.float32
Expected layers.1.attention.wo.weight to be bf16, got torch.float32Expected layers.5.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.1.attention_norm.weight to be bf16, got torch.float32
Expected layers.0.attention.wk.weight to be bf16, got torch.float32Expected layers.7.attention.wv.weight to be bf16, got torch.float32


Expected layers.6.attention.wo.weight to be bf16, got torch.float32Expected layers.1.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.5.attention_norm.weight to be bf16, got torch.float32


Expected layers.1.ffn_norm.weight to be bf16, got torch.float32Expected layers.0.attention.wv.weight to be bf16, got torch.float32Expected tok_embeddings.weight to be bf16, got torch.float32Expected layers.7.attention.wo.weight to be bf16, got torch.float32

Expected layers.6.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.1.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.5.ffn_norm.weight to be bf16, got torch.float32



Expected layers.2.attention.wq.weight to be bf16, got torch.float32Expected layers.0.attention.wo.weight to be bf16, got torch.float32
Expected layers.0.attention.wq.weight to be bf16, got torch.float32
Expected layers.7.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.6.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.1.feed_forward.w3.weight to be bf16, got torch.float32Expected tok_embeddings.weight to be bf16, got torch.float32Expected layers.6.attention.wq.weight to be bf16, got torch.float32

Expected layers.2.attention.wk.weight to be bf16, got torch.float32
Expected layers.0.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.0.attention.wk.weight to be bf16, got torch.float32

Expected layers.7.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.6.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.1.attention_norm.weight to be bf16, got torch.float32Expected layers.0.attention.wq.weight to be bf16, got torch.float32
Expected layers.6.attention.wk.weight to be bf16, got torch.float32

Expected layers.2.attention.wv.weight to be bf16, got torch.float32
Expected layers.0.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.0.attention.wv.weight to be bf16, got torch.float32
Expected layers.7.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.6.attention_norm.weight to be bf16, got torch.float32Expected layers.1.ffn_norm.weight to be bf16, got torch.float32

Expected layers.0.attention.wk.weight to be bf16, got torch.float32Expected layers.6.attention.wv.weight to be bf16, got torch.float32

Expected layers.2.attention.wo.weight to be bf16, got torch.float32Expected layers.0.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.0.attention.wo.weight to be bf16, got torch.float32
Expected layers.7.attention_norm.weight to be bf16, got torch.float32
Expected layers.6.ffn_norm.weight to be bf16, got torch.float32Expected layers.2.attention.wq.weight to be bf16, got torch.float32

Expected layers.0.attention.wv.weight to be bf16, got torch.float32
Expected layers.6.attention.wo.weight to be bf16, got torch.float32
Expected layers.2.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.0.attention_norm.weight to be bf16, got torch.float32

Expected layers.7.ffn_norm.weight to be bf16, got torch.float32Expected layers.0.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.7.attention.wq.weight to be bf16, got torch.float32

Expected layers.2.attention.wk.weight to be bf16, got torch.float32

Expected layers.6.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.0.attention.wo.weight to be bf16, got torch.float32
Expected layers.2.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.0.ffn_norm.weight to be bf16, got torch.float32
Expected layers.8.attention.wq.weight to be bf16, got torch.float32
Expected layers.0.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.7.attention.wk.weight to be bf16, got torch.float32
Expected layers.2.attention.wv.weight to be bf16, got torch.float32
Expected layers.6.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.0.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.2.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.1.attention.wq.weight to be bf16, got torch.float32
Expected layers.8.attention.wk.weight to be bf16, got torch.float32
Expected layers.0.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.7.attention.wv.weight to be bf16, got torch.float32
Expected layers.2.attention.wo.weight to be bf16, got torch.float32
Expected layers.6.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.0.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.2.attention_norm.weight to be bf16, got torch.float32
Expected layers.1.attention.wk.weight to be bf16, got torch.float32
Expected layers.8.attention.wv.weight to be bf16, got torch.float32
Expected layers.0.attention_norm.weight to be bf16, got torch.float32
Expected layers.7.attention.wo.weight to be bf16, got torch.float32
Expected layers.2.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.6.attention_norm.weight to be bf16, got torch.float32
Expected layers.0.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.2.ffn_norm.weight to be bf16, got torch.float32
Expected layers.1.attention.wv.weight to be bf16, got torch.float32
Expected layers.8.attention.wo.weight to be bf16, got torch.float32
Expected layers.0.ffn_norm.weight to be bf16, got torch.float32
Expected layers.7.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.2.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.6.ffn_norm.weight to be bf16, got torch.float32
Expected layers.0.attention_norm.weight to be bf16, got torch.float32
Expected layers.3.attention.wq.weight to be bf16, got torch.float32
Expected layers.1.attention.wo.weight to be bf16, got torch.float32
Expected layers.8.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.1.attention.wq.weight to be bf16, got torch.float32
Expected layers.7.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.2.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.7.attention.wq.weight to be bf16, got torch.float32
Expected layers.0.ffn_norm.weight to be bf16, got torch.float32
Expected layers.3.attention.wk.weight to be bf16, got torch.float32
Expected layers.1.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.8.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.1.attention.wk.weight to be bf16, got torch.float32
Expected layers.7.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.2.attention_norm.weight to be bf16, got torch.float32
Expected layers.3.attention.wv.weight to be bf16, got torch.float32Expected layers.7.attention.wk.weight to be bf16, got torch.float32
Expected layers.1.attention.wq.weight to be bf16, got torch.float32

Expected layers.1.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.8.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.1.attention.wv.weight to be bf16, got torch.float32
Expected layers.7.attention_norm.weight to be bf16, got torch.float32
Expected layers.2.ffn_norm.weight to be bf16, got torch.float32Expected layers.3.attention.wo.weight to be bf16, got torch.float32
Expected layers.7.attention.wv.weight to be bf16, got torch.float32

Expected layers.1.attention.wk.weight to be bf16, got torch.float32
Expected layers.1.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.8.attention_norm.weight to be bf16, got torch.float32
Expected layers.1.attention.wo.weight to be bf16, got torch.float32
Expected layers.7.ffn_norm.weight to be bf16, got torch.float32
Expected layers.3.attention.wq.weight to be bf16, got torch.float32
Expected layers.3.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.7.attention.wo.weight to be bf16, got torch.float32
Expected layers.1.attention.wv.weight to be bf16, got torch.float32
Expected layers.1.attention_norm.weight to be bf16, got torch.float32
Expected layers.8.ffn_norm.weight to be bf16, got torch.float32
Expected layers.1.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.8.attention.wq.weight to be bf16, got torch.float32
Expected layers.3.attention.wk.weight to be bf16, got torch.float32
Expected layers.3.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.7.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.1.attention.wo.weight to be bf16, got torch.float32
Expected layers.1.ffn_norm.weight to be bf16, got torch.float32
Expected layers.9.attention.wq.weight to be bf16, got torch.float32
Expected layers.1.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.8.attention.wk.weight to be bf16, got torch.float32
Expected layers.3.attention.wv.weight to be bf16, got torch.float32
Expected layers.3.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.7.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.1.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.2.attention.wq.weight to be bf16, got torch.float32
Expected layers.9.attention.wk.weight to be bf16, got torch.float32
Expected layers.1.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.8.attention.wv.weight to be bf16, got torch.float32

Expected layers.3.attention.wo.weight to be bf16, got torch.float32
Expected layers.3.attention_norm.weight to be bf16, got torch.float32
Expected layers.7.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.1.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.2.attention.wk.weight to be bf16, got torch.float32
Expected layers.9.attention.wv.weight to be bf16, got torch.float32
Expected layers.1.attention_norm.weight to be bf16, got torch.float32Expected layers.8.attention.wo.weight to be bf16, got torch.float32

Expected layers.3.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.3.ffn_norm.weight to be bf16, got torch.float32
Expected layers.7.attention_norm.weight to be bf16, got torch.float32Expected layers.1.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.2.attention.wv.weight to be bf16, got torch.float32
Expected layers.9.attention.wo.weight to be bf16, got torch.float32
Expected layers.1.ffn_norm.weight to be bf16, got torch.float32
Expected layers.8.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.3.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.4.attention.wq.weight to be bf16, got torch.float32
Expected layers.7.ffn_norm.weight to be bf16, got torch.float32
Expected layers.1.attention_norm.weight to be bf16, got torch.float32
Expected layers.2.attention.wo.weight to be bf16, got torch.float32
Expected layers.9.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.2.attention.wq.weight to be bf16, got torch.float32
Expected layers.8.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.3.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.4.attention.wk.weight to be bf16, got torch.float32
Expected layers.8.attention.wq.weight to be bf16, got torch.float32
Expected layers.1.ffn_norm.weight to be bf16, got torch.float32Expected layers.2.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.9.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.2.attention.wk.weight to be bf16, got torch.float32

Expected layers.8.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.3.attention_norm.weight to be bf16, got torch.float32

Expected layers.4.attention.wv.weight to be bf16, got torch.float32Expected layers.8.attention.wk.weight to be bf16, got torch.float32
Expected layers.2.attention.wq.weight to be bf16, got torch.float32Expected layers.2.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.9.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.2.attention.wv.weight to be bf16, got torch.float32
Expected layers.8.attention_norm.weight to be bf16, got torch.float32
Expected layers.3.ffn_norm.weight to be bf16, got torch.float32

Expected layers.4.attention.wo.weight to be bf16, got torch.float32Expected layers.8.attention.wv.weight to be bf16, got torch.float32
Expected layers.2.attention.wk.weight to be bf16, got torch.float32Expected layers.2.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.9.attention_norm.weight to be bf16, got torch.float32
Expected layers.2.attention.wo.weight to be bf16, got torch.float32
Expected layers.8.ffn_norm.weight to be bf16, got torch.float32
Expected layers.4.attention.wq.weight to be bf16, got torch.float32

Expected layers.4.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.8.attention.wo.weight to be bf16, got torch.float32
Expected layers.2.attention.wv.weight to be bf16, got torch.float32Expected layers.2.attention_norm.weight to be bf16, got torch.float32

Expected layers.9.ffn_norm.weight to be bf16, got torch.float32
Expected layers.2.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.9.attention.wq.weight to be bf16, got torch.float32
Expected layers.4.attention.wk.weight to be bf16, got torch.float32
Expected layers.4.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.8.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.2.attention.wo.weight to be bf16, got torch.float32
Expected layers.2.ffn_norm.weight to be bf16, got torch.float32
Expected layers.10.attention.wq.weight to be bf16, got torch.float32
Expected layers.2.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.9.attention.wk.weight to be bf16, got torch.float32
Expected layers.4.attention.wv.weight to be bf16, got torch.float32
Expected layers.4.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.8.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.2.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.3.attention.wq.weight to be bf16, got torch.float32
Expected layers.10.attention.wk.weight to be bf16, got torch.float32
Expected layers.2.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.9.attention.wv.weight to be bf16, got torch.float32

Expected layers.4.attention.wo.weight to be bf16, got torch.float32
Expected layers.4.attention_norm.weight to be bf16, got torch.float32
Expected layers.8.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.2.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.3.attention.wk.weight to be bf16, got torch.float32
Expected layers.10.attention.wv.weight to be bf16, got torch.float32
Expected layers.2.attention_norm.weight to be bf16, got torch.float32
Expected layers.9.attention.wo.weight to be bf16, got torch.float32
Expected layers.4.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.4.ffn_norm.weight to be bf16, got torch.float32

Expected layers.8.attention_norm.weight to be bf16, got torch.float32
Expected layers.2.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.3.attention.wv.weight to be bf16, got torch.float32Expected layers.10.attention.wo.weight to be bf16, got torch.float32

Expected layers.2.ffn_norm.weight to be bf16, got torch.float32Expected layers.9.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.4.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.5.attention.wq.weight to be bf16, got torch.float32

Expected layers.8.ffn_norm.weight to be bf16, got torch.float32
Expected layers.2.attention_norm.weight to be bf16, got torch.float32
Expected layers.3.attention.wo.weight to be bf16, got torch.float32
Expected layers.10.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.3.attention.wq.weight to be bf16, got torch.float32Expected layers.9.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.4.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.5.attention.wk.weight to be bf16, got torch.float32

Expected layers.9.attention.wq.weight to be bf16, got torch.float32Expected layers.2.ffn_norm.weight to be bf16, got torch.float32

Expected layers.3.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.10.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.4.attention_norm.weight to be bf16, got torch.float32Expected layers.3.attention.wk.weight to be bf16, got torch.float32Expected layers.9.feed_forward.w3.weight to be bf16, got torch.float32


Expected layers.5.attention.wv.weight to be bf16, got torch.float32

Expected layers.9.attention.wk.weight to be bf16, got torch.float32Expected layers.3.attention.wq.weight to be bf16, got torch.float32
Expected layers.3.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.10.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.4.ffn_norm.weight to be bf16, got torch.float32

Expected layers.9.attention_norm.weight to be bf16, got torch.float32Expected layers.3.attention.wv.weight to be bf16, got torch.float32

Expected layers.5.attention.wo.weight to be bf16, got torch.float32
Expected layers.9.attention.wv.weight to be bf16, got torch.float32
Expected layers.3.attention.wk.weight to be bf16, got torch.float32Expected layers.3.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.10.attention_norm.weight to be bf16, got torch.float32
Expected layers.5.attention.wq.weight to be bf16, got torch.float32
Expected layers.9.ffn_norm.weight to be bf16, got torch.float32
Expected layers.3.attention.wo.weight to be bf16, got torch.float32

Expected layers.5.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.9.attention.wo.weight to be bf16, got torch.float32
Expected layers.3.attention.wv.weight to be bf16, got torch.float32Expected layers.3.attention_norm.weight to be bf16, got torch.float32

Expected layers.10.ffn_norm.weight to be bf16, got torch.float32
Expected layers.5.attention.wk.weight to be bf16, got torch.float32
Expected layers.10.attention.wq.weight to be bf16, got torch.float32
Expected layers.3.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.5.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.9.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.3.attention.wo.weight to be bf16, got torch.float32
Expected layers.3.ffn_norm.weight to be bf16, got torch.float32
Expected layers.11.attention.wq.weight to be bf16, got torch.float32
Expected layers.5.attention.wv.weight to be bf16, got torch.float32
Expected layers.10.attention.wk.weight to be bf16, got torch.float32
Expected layers.3.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.5.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.9.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.3.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.4.attention.wq.weight to be bf16, got torch.float32
Expected layers.11.attention.wk.weight to be bf16, got torch.float32
Expected layers.5.attention.wo.weight to be bf16, got torch.float32
Expected layers.10.attention.wv.weight to be bf16, got torch.float32
Expected layers.3.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.5.attention_norm.weight to be bf16, got torch.float32
Expected layers.9.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.3.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.4.attention.wk.weight to be bf16, got torch.float32
Expected layers.11.attention.wv.weight to be bf16, got torch.float32

Expected layers.5.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.10.attention.wo.weight to be bf16, got torch.float32
Expected layers.3.attention_norm.weight to be bf16, got torch.float32
Expected layers.5.ffn_norm.weight to be bf16, got torch.float32
Expected layers.9.attention_norm.weight to be bf16, got torch.float32
Expected layers.3.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.4.attention.wv.weight to be bf16, got torch.float32
Expected layers.11.attention.wo.weight to be bf16, got torch.float32
Expected layers.5.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.10.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.3.ffn_norm.weight to be bf16, got torch.float32
Expected layers.6.attention.wq.weight to be bf16, got torch.float32
Expected layers.9.ffn_norm.weight to be bf16, got torch.float32
Expected layers.3.attention_norm.weight to be bf16, got torch.float32
Expected layers.5.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.4.attention.wo.weight to be bf16, got torch.float32
Expected layers.11.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.10.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.4.attention.wq.weight to be bf16, got torch.float32
Expected layers.6.attention.wk.weight to be bf16, got torch.float32
Expected layers.10.attention.wq.weight to be bf16, got torch.float32
Expected layers.3.ffn_norm.weight to be bf16, got torch.float32
Expected layers.5.attention_norm.weight to be bf16, got torch.float32
Expected layers.4.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.11.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.10.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.4.attention.wk.weight to be bf16, got torch.float32
Expected layers.6.attention.wv.weight to be bf16, got torch.float32
Expected layers.10.attention.wk.weight to be bf16, got torch.float32
Expected layers.4.attention.wq.weight to be bf16, got torch.float32

Expected layers.5.ffn_norm.weight to be bf16, got torch.float32Expected layers.4.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.11.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.10.attention_norm.weight to be bf16, got torch.float32

Expected layers.4.attention.wv.weight to be bf16, got torch.float32
Expected layers.6.attention.wo.weight to be bf16, got torch.float32
Expected layers.10.attention.wv.weight to be bf16, got torch.float32
Expected layers.4.attention.wk.weight to be bf16, got torch.float32

Expected layers.6.attention.wq.weight to be bf16, got torch.float32Expected layers.4.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.11.attention_norm.weight to be bf16, got torch.float32Expected layers.10.ffn_norm.weight to be bf16, got torch.float32

Expected layers.4.attention.wo.weight to be bf16, got torch.float32

Expected layers.6.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.10.attention.wo.weight to be bf16, got torch.float32
Expected layers.4.attention.wv.weight to be bf16, got torch.float32

Expected layers.6.attention.wk.weight to be bf16, got torch.float32Expected layers.4.attention_norm.weight to be bf16, got torch.float32
Expected layers.11.ffn_norm.weight to be bf16, got torch.float32Expected layers.11.attention.wq.weight to be bf16, got torch.float32

Expected layers.4.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.6.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.10.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.4.attention.wo.weight to be bf16, got torch.float32
Expected layers.11.attention.wk.weight to be bf16, got torch.float32
Expected layers.6.attention.wv.weight to be bf16, got torch.float32Expected layers.4.ffn_norm.weight to be bf16, got torch.float32Expected layers.12.attention.wq.weight to be bf16, got torch.float32

Expected layers.4.feed_forward.w2.weight to be bf16, got torch.float32


Expected layers.6.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.10.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.4.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.11.attention.wv.weight to be bf16, got torch.float32
Expected layers.6.attention.wo.weight to be bf16, got torch.float32
Expected layers.5.attention.wq.weight to be bf16, got torch.float32Expected layers.12.attention.wk.weight to be bf16, got torch.float32

Expected layers.4.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.6.attention_norm.weight to be bf16, got torch.float32
Expected layers.10.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.4.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.11.attention.wo.weight to be bf16, got torch.float32
Expected layers.6.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.5.attention.wk.weight to be bf16, got torch.float32
Expected layers.12.attention.wv.weight to be bf16, got torch.float32
Expected layers.4.attention_norm.weight to be bf16, got torch.float32
Expected layers.6.ffn_norm.weight to be bf16, got torch.float32
Expected layers.10.attention_norm.weight to be bf16, got torch.float32
Expected layers.4.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.11.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.6.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.5.attention.wv.weight to be bf16, got torch.float32
Expected layers.12.attention.wo.weight to be bf16, got torch.float32
Expected layers.4.ffn_norm.weight to be bf16, got torch.float32
Expected layers.7.attention.wq.weight to be bf16, got torch.float32
Expected layers.10.ffn_norm.weight to be bf16, got torch.float32
Expected layers.4.attention_norm.weight to be bf16, got torch.float32
Expected layers.11.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.6.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.5.attention.wo.weight to be bf16, got torch.float32
Expected layers.12.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.5.attention.wq.weight to be bf16, got torch.float32
Expected layers.7.attention.wk.weight to be bf16, got torch.float32
Expected layers.11.attention.wq.weight to be bf16, got torch.float32
Expected layers.6.attention_norm.weight to be bf16, got torch.float32Expected layers.4.ffn_norm.weight to be bf16, got torch.float32
Expected layers.11.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.5.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.12.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.5.attention.wk.weight to be bf16, got torch.float32
Expected layers.7.attention.wv.weight to be bf16, got torch.float32
Expected layers.11.attention.wk.weight to be bf16, got torch.float32Expected layers.6.ffn_norm.weight to be bf16, got torch.float32

Expected layers.5.attention.wq.weight to be bf16, got torch.float32
Expected layers.11.attention_norm.weight to be bf16, got torch.float32
Expected layers.5.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.12.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.5.attention.wv.weight to be bf16, got torch.float32
Expected layers.7.attention.wo.weight to be bf16, got torch.float32
Expected layers.11.attention.wv.weight to be bf16, got torch.float32
Expected layers.7.attention.wq.weight to be bf16, got torch.float32
Expected layers.5.attention.wk.weight to be bf16, got torch.float32
Expected layers.11.ffn_norm.weight to be bf16, got torch.float32
Expected layers.5.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.12.attention_norm.weight to be bf16, got torch.float32

Expected layers.5.attention.wo.weight to be bf16, got torch.float32Expected layers.7.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.11.attention.wo.weight to be bf16, got torch.float32Expected layers.7.attention.wk.weight to be bf16, got torch.float32

Expected layers.5.attention.wv.weight to be bf16, got torch.float32Expected layers.12.attention.wq.weight to be bf16, got torch.float32

Expected layers.5.attention_norm.weight to be bf16, got torch.float32Expected layers.12.ffn_norm.weight to be bf16, got torch.float32

Expected layers.5.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.7.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.11.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.7.attention.wv.weight to be bf16, got torch.float32

Expected layers.5.attention.wo.weight to be bf16, got torch.float32
Expected layers.12.attention.wk.weight to be bf16, got torch.float32
Expected layers.5.ffn_norm.weight to be bf16, got torch.float32
Expected layers.13.attention.wq.weight to be bf16, got torch.float32
Expected layers.5.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.7.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.11.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.7.attention.wo.weight to be bf16, got torch.float32
Expected layers.5.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.12.attention.wv.weight to be bf16, got torch.float32
Expected layers.6.attention.wq.weight to be bf16, got torch.float32
Expected layers.13.attention.wk.weight to be bf16, got torch.float32
Expected layers.5.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.7.attention_norm.weight to be bf16, got torch.float32
Expected layers.11.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.7.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.5.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.12.attention.wo.weight to be bf16, got torch.float32
Expected layers.6.attention.wk.weight to be bf16, got torch.float32Expected layers.13.attention.wv.weight to be bf16, got torch.float32

Expected layers.5.attention_norm.weight to be bf16, got torch.float32
Expected layers.7.ffn_norm.weight to be bf16, got torch.float32
Expected layers.11.attention_norm.weight to be bf16, got torch.float32Expected layers.7.feed_forward.w2.weight to be bf16, got torch.float32


Expected layers.12.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.5.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.6.attention.wv.weight to be bf16, got torch.float32Expected layers.13.attention.wo.weight to be bf16, got torch.float32


Expected layers.5.ffn_norm.weight to be bf16, got torch.float32
Expected layers.8.attention.wq.weight to be bf16, got torch.float32Expected layers.11.ffn_norm.weight to be bf16, got torch.float32Expected layers.7.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.12.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.5.attention_norm.weight to be bf16, got torch.float32
Expected layers.6.attention.wo.weight to be bf16, got torch.float32Expected layers.13.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.6.attention.wq.weight to be bf16, got torch.float32
Expected layers.8.attention.wk.weight to be bf16, got torch.float32
Expected layers.12.attention.wq.weight to be bf16, got torch.float32
Expected layers.7.attention_norm.weight to be bf16, got torch.float32
Expected layers.12.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.5.ffn_norm.weight to be bf16, got torch.float32
Expected layers.6.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.13.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.6.attention.wk.weight to be bf16, got torch.float32
Expected layers.8.attention.wv.weight to be bf16, got torch.float32
Expected layers.12.attention.wk.weight to be bf16, got torch.float32
Expected layers.7.ffn_norm.weight to be bf16, got torch.float32Expected layers.12.attention_norm.weight to be bf16, got torch.float32
Expected layers.6.attention.wq.weight to be bf16, got torch.float32
Expected layers.6.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.13.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.6.attention.wv.weight to be bf16, got torch.float32
Expected layers.8.attention.wo.weight to be bf16, got torch.float32
Expected layers.12.attention.wv.weight to be bf16, got torch.float32
Expected layers.8.attention.wq.weight to be bf16, got torch.float32
Expected layers.12.ffn_norm.weight to be bf16, got torch.float32
Expected layers.6.attention.wk.weight to be bf16, got torch.float32
Expected layers.6.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.13.attention_norm.weight to be bf16, got torch.float32
Expected layers.6.attention.wo.weight to be bf16, got torch.float32Expected layers.8.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.12.attention.wo.weight to be bf16, got torch.float32

Expected layers.8.attention.wk.weight to be bf16, got torch.float32
Expected layers.13.attention.wq.weight to be bf16, got torch.float32
Expected layers.6.attention.wv.weight to be bf16, got torch.float32
Expected layers.6.attention_norm.weight to be bf16, got torch.float32
Expected layers.13.ffn_norm.weight to be bf16, got torch.float32
Expected layers.6.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.8.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.12.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.8.attention.wv.weight to be bf16, got torch.float32
Expected layers.13.attention.wk.weight to be bf16, got torch.float32
Expected layers.6.attention.wo.weight to be bf16, got torch.float32
Expected layers.6.ffn_norm.weight to be bf16, got torch.float32
Expected layers.14.attention.wq.weight to be bf16, got torch.float32
Expected layers.6.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.8.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.12.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.6.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.8.attention.wo.weight to be bf16, got torch.float32Expected layers.13.attention.wv.weight to be bf16, got torch.float32


Expected layers.7.attention.wq.weight to be bf16, got torch.float32

Expected layers.14.attention.wk.weight to be bf16, got torch.float32Expected layers.6.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.8.attention_norm.weight to be bf16, got torch.float32
Expected layers.12.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.6.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.8.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.14.attention.wv.weight to be bf16, got torch.float32Expected layers.13.attention.wo.weight to be bf16, got torch.float32

Expected layers.7.attention.wk.weight to be bf16, got torch.float32


Expected layers.6.attention_norm.weight to be bf16, got torch.float32Expected layers.8.ffn_norm.weight to be bf16, got torch.float32
Expected layers.12.attention_norm.weight to be bf16, got torch.float32

Expected layers.6.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.14.attention.wo.weight to be bf16, got torch.float32Expected layers.8.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.13.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.7.attention.wv.weight to be bf16, got torch.float32

Expected layers.6.ffn_norm.weight to be bf16, got torch.float32
Expected layers.9.attention.wq.weight to be bf16, got torch.float32
Expected layers.12.ffn_norm.weight to be bf16, got torch.float32
Expected layers.6.attention_norm.weight to be bf16, got torch.float32
Expected layers.14.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.8.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.13.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.7.attention.wo.weight to be bf16, got torch.float32Expected layers.7.attention.wq.weight to be bf16, got torch.float32
Expected layers.9.attention.wk.weight to be bf16, got torch.float32
Expected layers.13.attention.wq.weight to be bf16, got torch.float32

Expected layers.6.ffn_norm.weight to be bf16, got torch.float32
Expected layers.14.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.8.attention_norm.weight to be bf16, got torch.float32Expected layers.13.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.7.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.7.attention.wk.weight to be bf16, got torch.float32

Expected layers.9.attention.wv.weight to be bf16, got torch.float32
Expected layers.13.attention.wk.weight to be bf16, got torch.float32
Expected layers.7.attention.wq.weight to be bf16, got torch.float32
Expected layers.14.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.8.ffn_norm.weight to be bf16, got torch.float32
Expected layers.13.attention_norm.weight to be bf16, got torch.float32
Expected layers.7.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.7.attention.wv.weight to be bf16, got torch.float32
Expected layers.9.attention.wo.weight to be bf16, got torch.float32
Expected layers.13.attention.wv.weight to be bf16, got torch.float32
Expected layers.7.attention.wk.weight to be bf16, got torch.float32
Expected layers.14.attention_norm.weight to be bf16, got torch.float32
Expected layers.9.attention.wq.weight to be bf16, got torch.float32
Expected layers.13.ffn_norm.weight to be bf16, got torch.float32
Expected layers.7.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.7.attention.wo.weight to be bf16, got torch.float32
Expected layers.9.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.13.attention.wo.weight to be bf16, got torch.float32

Expected layers.7.attention.wv.weight to be bf16, got torch.float32
Expected layers.14.ffn_norm.weight to be bf16, got torch.float32
Expected layers.9.attention.wk.weight to be bf16, got torch.float32
Expected layers.14.attention.wq.weight to be bf16, got torch.float32
Expected layers.7.attention_norm.weight to be bf16, got torch.float32
Expected layers.7.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.9.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.13.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.7.attention.wo.weight to be bf16, got torch.float32
Expected layers.15.attention.wq.weight to be bf16, got torch.float32
Expected layers.9.attention.wv.weight to be bf16, got torch.float32
Expected layers.14.attention.wk.weight to be bf16, got torch.float32
Expected layers.7.ffn_norm.weight to be bf16, got torch.float32
Expected layers.7.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.9.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.13.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.7.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.15.attention.wk.weight to be bf16, got torch.float32
Expected layers.9.attention.wo.weight to be bf16, got torch.float32
Expected layers.14.attention.wv.weight to be bf16, got torch.float32
Expected layers.8.attention.wq.weight to be bf16, got torch.float32Expected layers.13.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.7.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.9.attention_norm.weight to be bf16, got torch.float32

Expected layers.7.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.15.attention.wv.weight to be bf16, got torch.float32
Expected layers.9.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.14.attention.wo.weight to be bf16, got torch.float32
Expected layers.8.attention.wk.weight to be bf16, got torch.float32
Expected layers.13.attention_norm.weight to be bf16, got torch.float32
Expected layers.7.attention_norm.weight to be bf16, got torch.float32
Expected layers.9.ffn_norm.weight to be bf16, got torch.float32Expected layers.7.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.15.attention.wo.weight to be bf16, got torch.float32
Expected layers.9.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.14.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.8.attention.wv.weight to be bf16, got torch.float32
Expected layers.13.ffn_norm.weight to be bf16, got torch.float32
Expected layers.7.ffn_norm.weight to be bf16, got torch.float32
Expected layers.10.attention.wq.weight to be bf16, got torch.float32
Expected layers.7.attention_norm.weight to be bf16, got torch.float32
Expected layers.15.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.9.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.14.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.8.attention.wo.weight to be bf16, got torch.float32
Expected layers.14.attention.wq.weight to be bf16, got torch.float32
Expected layers.8.attention.wq.weight to be bf16, got torch.float32
Expected layers.10.attention.wk.weight to be bf16, got torch.float32
Expected layers.7.ffn_norm.weight to be bf16, got torch.float32
Expected layers.15.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.9.attention_norm.weight to be bf16, got torch.float32
Expected layers.14.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.8.attention.wk.weight to be bf16, got torch.float32Expected layers.8.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.14.attention.wk.weight to be bf16, got torch.float32

Expected layers.10.attention.wv.weight to be bf16, got torch.float32
Expected layers.8.attention.wq.weight to be bf16, got torch.float32
Expected layers.14.attention_norm.weight to be bf16, got torch.float32Expected layers.15.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.9.ffn_norm.weight to be bf16, got torch.float32

Expected layers.8.attention.wv.weight to be bf16, got torch.float32
Expected layers.8.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.14.attention.wv.weight to be bf16, got torch.float32
Expected layers.10.attention.wo.weight to be bf16, got torch.float32
Expected layers.8.attention.wk.weight to be bf16, got torch.float32Expected layers.14.ffn_norm.weight to be bf16, got torch.float32
Expected layers.15.attention_norm.weight to be bf16, got torch.float32

Expected layers.10.attention.wq.weight to be bf16, got torch.float32
Expected layers.8.attention.wo.weight to be bf16, got torch.float32
Expected layers.8.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.14.attention.wo.weight to be bf16, got torch.float32
Expected layers.10.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.8.attention.wv.weight to be bf16, got torch.float32
Expected layers.15.attention.wq.weight to be bf16, got torch.float32
Expected layers.15.ffn_norm.weight to be bf16, got torch.float32

Expected layers.10.attention.wk.weight to be bf16, got torch.float32
Expected layers.8.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.8.attention_norm.weight to be bf16, got torch.float32
Expected layers.14.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.10.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.8.attention.wo.weight to be bf16, got torch.float32
Expected layers.15.attention.wk.weight to be bf16, got torch.float32
Expected layers.16.attention.wq.weight to be bf16, got torch.float32

Expected layers.10.attention.wv.weight to be bf16, got torch.float32
Expected layers.8.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.8.ffn_norm.weight to be bf16, got torch.float32Expected layers.14.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.10.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.8.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.15.attention.wv.weight to be bf16, got torch.float32
Expected layers.16.attention.wk.weight to be bf16, got torch.float32
Expected layers.10.attention.wo.weight to be bf16, got torch.float32

Expected layers.8.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.9.attention.wq.weight to be bf16, got torch.float32Expected layers.14.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.10.attention_norm.weight to be bf16, got torch.float32
Expected layers.8.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.15.attention.wo.weight to be bf16, got torch.float32
Expected layers.16.attention.wv.weight to be bf16, got torch.float32
Expected layers.10.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.8.attention_norm.weight to be bf16, got torch.float32
Expected layers.9.attention.wk.weight to be bf16, got torch.float32
Expected layers.14.attention_norm.weight to be bf16, got torch.float32
Expected layers.10.ffn_norm.weight to be bf16, got torch.float32
Expected layers.8.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.15.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.16.attention.wo.weight to be bf16, got torch.float32
Expected layers.10.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.8.ffn_norm.weight to be bf16, got torch.float32
Expected layers.9.attention.wv.weight to be bf16, got torch.float32
Expected layers.14.ffn_norm.weight to be bf16, got torch.float32
Expected layers.11.attention.wq.weight to be bf16, got torch.float32
Expected layers.8.attention_norm.weight to be bf16, got torch.float32Expected layers.15.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.16.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.10.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.9.attention.wq.weight to be bf16, got torch.float32
Expected layers.9.attention.wo.weight to be bf16, got torch.float32
Expected layers.15.attention.wq.weight to be bf16, got torch.float32
Expected layers.11.attention.wk.weight to be bf16, got torch.float32
Expected layers.8.ffn_norm.weight to be bf16, got torch.float32
Expected layers.15.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.16.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.10.attention_norm.weight to be bf16, got torch.float32
Expected layers.9.attention.wk.weight to be bf16, got torch.float32
Expected layers.9.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.15.attention.wk.weight to be bf16, got torch.float32Expected layers.11.attention.wv.weight to be bf16, got torch.float32


Expected layers.15.attention_norm.weight to be bf16, got torch.float32Expected layers.9.attention.wq.weight to be bf16, got torch.float32
Expected layers.16.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.10.ffn_norm.weight to be bf16, got torch.float32
Expected layers.11.attention.wo.weight to be bf16, got torch.float32
Expected layers.9.attention.wv.weight to be bf16, got torch.float32Expected layers.9.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.15.attention.wv.weight to be bf16, got torch.float32
Expected layers.15.ffn_norm.weight to be bf16, got torch.float32
Expected layers.9.attention.wk.weight to be bf16, got torch.float32
Expected layers.16.attention_norm.weight to be bf16, got torch.float32
Expected layers.11.attention.wq.weight to be bf16, got torch.float32
Expected layers.11.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.9.attention.wo.weight to be bf16, got torch.float32
Expected layers.9.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.15.attention.wo.weight to be bf16, got torch.float32
Expected layers.16.attention.wq.weight to be bf16, got torch.float32
Expected layers.9.attention.wv.weight to be bf16, got torch.float32Expected layers.11.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.16.ffn_norm.weight to be bf16, got torch.float32
Expected layers.11.attention.wk.weight to be bf16, got torch.float32

Expected layers.9.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.9.attention_norm.weight to be bf16, got torch.float32
Expected layers.15.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.16.attention.wk.weight to be bf16, got torch.float32
Expected layers.9.attention.wo.weight to be bf16, got torch.float32Expected layers.11.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.17.attention.wq.weight to be bf16, got torch.float32
Expected layers.11.attention.wv.weight to be bf16, got torch.float32

Expected layers.9.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.9.ffn_norm.weight to be bf16, got torch.float32Expected layers.15.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.16.attention.wv.weight to be bf16, got torch.float32
Expected layers.9.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.11.attention_norm.weight to be bf16, got torch.float32
Expected layers.17.attention.wk.weight to be bf16, got torch.float32Expected layers.11.attention.wo.weight to be bf16, got torch.float32

Expected layers.9.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.10.attention.wq.weight to be bf16, got torch.float32
Expected layers.15.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.16.attention.wo.weight to be bf16, got torch.float32

Expected layers.9.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.11.ffn_norm.weight to be bf16, got torch.float32

Expected layers.17.attention.wv.weight to be bf16, got torch.float32
Expected layers.11.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.9.attention_norm.weight to be bf16, got torch.float32Expected layers.10.attention.wk.weight to be bf16, got torch.float32
Expected layers.15.attention_norm.weight to be bf16, got torch.float32Expected layers.16.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.9.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.12.attention.wq.weight to be bf16, got torch.float32
Expected layers.17.attention.wo.weight to be bf16, got torch.float32
Expected layers.11.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.9.ffn_norm.weight to be bf16, got torch.float32Expected layers.10.attention.wv.weight to be bf16, got torch.float32
Expected layers.15.ffn_norm.weight to be bf16, got torch.float32Expected layers.16.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.9.attention_norm.weight to be bf16, got torch.float32
Expected layers.12.attention.wk.weight to be bf16, got torch.float32
Expected layers.17.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.11.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.10.attention.wq.weight to be bf16, got torch.float32Expected layers.10.attention.wo.weight to be bf16, got torch.float32
Expected layers.16.attention.wq.weight to be bf16, got torch.float32Expected layers.16.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.9.ffn_norm.weight to be bf16, got torch.float32
Expected layers.12.attention.wv.weight to be bf16, got torch.float32
Expected layers.17.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.11.attention_norm.weight to be bf16, got torch.float32

Expected layers.10.attention.wk.weight to be bf16, got torch.float32Expected layers.10.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.16.attention.wk.weight to be bf16, got torch.float32
Expected layers.16.attention_norm.weight to be bf16, got torch.float32
Expected layers.10.attention.wq.weight to be bf16, got torch.float32
Expected layers.12.attention.wo.weight to be bf16, got torch.float32Expected layers.17.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.11.ffn_norm.weight to be bf16, got torch.float32
Expected layers.10.attention.wv.weight to be bf16, got torch.float32
Expected layers.10.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.16.attention.wv.weight to be bf16, got torch.float32
Expected layers.16.ffn_norm.weight to be bf16, got torch.float32Expected layers.10.attention.wk.weight to be bf16, got torch.float32
Expected layers.12.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.17.attention_norm.weight to be bf16, got torch.float32

Expected layers.12.attention.wq.weight to be bf16, got torch.float32

Expected layers.10.attention.wo.weight to be bf16, got torch.float32
Expected layers.10.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.16.attention.wo.weight to be bf16, got torch.float32
Expected layers.17.attention.wq.weight to be bf16, got torch.float32Expected layers.10.attention.wv.weight to be bf16, got torch.float32

Expected layers.12.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.17.ffn_norm.weight to be bf16, got torch.float32

Expected layers.12.attention.wk.weight to be bf16, got torch.float32

Expected layers.10.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.10.attention_norm.weight to be bf16, got torch.float32Expected layers.16.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.17.attention.wk.weight to be bf16, got torch.float32Expected layers.10.attention.wo.weight to be bf16, got torch.float32

Expected layers.12.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.18.attention.wq.weight to be bf16, got torch.float32

Expected layers.12.attention.wv.weight to be bf16, got torch.float32
Expected layers.10.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.10.ffn_norm.weight to be bf16, got torch.float32
Expected layers.16.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.17.attention.wv.weight to be bf16, got torch.float32
Expected layers.10.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.12.attention_norm.weight to be bf16, got torch.float32Expected layers.18.attention.wk.weight to be bf16, got torch.float32

Expected layers.12.attention.wo.weight to be bf16, got torch.float32
Expected layers.10.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.11.attention.wq.weight to be bf16, got torch.float32
Expected layers.16.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.17.attention.wo.weight to be bf16, got torch.float32
Expected layers.10.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.12.ffn_norm.weight to be bf16, got torch.float32
Expected layers.18.attention.wv.weight to be bf16, got torch.float32
Expected layers.12.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.10.attention_norm.weight to be bf16, got torch.float32
Expected layers.11.attention.wk.weight to be bf16, got torch.float32
Expected layers.16.attention_norm.weight to be bf16, got torch.float32Expected layers.17.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.10.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.13.attention.wq.weight to be bf16, got torch.float32

Expected layers.18.attention.wo.weight to be bf16, got torch.float32
Expected layers.12.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.10.ffn_norm.weight to be bf16, got torch.float32
Expected layers.11.attention.wv.weight to be bf16, got torch.float32Expected layers.16.ffn_norm.weight to be bf16, got torch.float32Expected layers.17.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.10.attention_norm.weight to be bf16, got torch.float32

Expected layers.13.attention.wk.weight to be bf16, got torch.float32
Expected layers.18.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.12.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.11.attention.wq.weight to be bf16, got torch.float32

Expected layers.17.attention.wq.weight to be bf16, got torch.float32Expected layers.11.attention.wo.weight to be bf16, got torch.float32Expected layers.17.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.10.ffn_norm.weight to be bf16, got torch.float32

Expected layers.13.attention.wv.weight to be bf16, got torch.float32
Expected layers.18.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.12.attention_norm.weight to be bf16, got torch.float32

Expected layers.11.attention.wk.weight to be bf16, got torch.float32

Expected layers.17.attention.wk.weight to be bf16, got torch.float32Expected layers.11.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.17.attention_norm.weight to be bf16, got torch.float32Expected layers.11.attention.wq.weight to be bf16, got torch.float32
Expected layers.13.attention.wo.weight to be bf16, got torch.float32
Expected layers.18.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.12.ffn_norm.weight to be bf16, got torch.float32

Expected layers.11.attention.wv.weight to be bf16, got torch.float32

Expected layers.17.attention.wv.weight to be bf16, got torch.float32Expected layers.11.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.17.ffn_norm.weight to be bf16, got torch.float32Expected layers.11.attention.wk.weight to be bf16, got torch.float32
Expected layers.13.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.18.attention_norm.weight to be bf16, got torch.float32
Expected layers.13.attention.wq.weight to be bf16, got torch.float32

Expected layers.11.attention.wo.weight to be bf16, got torch.float32

Expected layers.17.attention.wo.weight to be bf16, got torch.float32Expected layers.11.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.18.attention.wq.weight to be bf16, got torch.float32
Expected layers.13.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.11.attention.wv.weight to be bf16, got torch.float32


Expected layers.18.ffn_norm.weight to be bf16, got torch.float32Expected layers.13.attention.wk.weight to be bf16, got torch.float32


Expected layers.11.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.17.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.11.attention_norm.weight to be bf16, got torch.float32
Expected layers.18.attention.wk.weight to be bf16, got torch.float32
Expected layers.13.attention.wv.weight to be bf16, got torch.float32
Expected layers.13.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.11.attention.wo.weight to be bf16, got torch.float32

Expected layers.19.attention.wq.weight to be bf16, got torch.float32
Expected layers.11.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.17.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.11.ffn_norm.weight to be bf16, got torch.float32

Expected layers.18.attention.wv.weight to be bf16, got torch.float32
Expected layers.13.attention.wo.weight to be bf16, got torch.float32
Expected layers.13.attention_norm.weight to be bf16, got torch.float32Expected layers.11.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.19.attention.wk.weight to be bf16, got torch.float32
Expected layers.11.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.17.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.12.attention.wq.weight to be bf16, got torch.float32
Expected layers.18.attention.wo.weight to be bf16, got torch.float32
Expected layers.13.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.13.ffn_norm.weight to be bf16, got torch.float32
Expected layers.11.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.19.attention.wv.weight to be bf16, got torch.float32

Expected layers.11.attention_norm.weight to be bf16, got torch.float32
Expected layers.17.attention_norm.weight to be bf16, got torch.float32
Expected layers.12.attention.wk.weight to be bf16, got torch.float32Expected layers.18.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.13.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.14.attention.wq.weight to be bf16, got torch.float32

Expected layers.11.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.19.attention.wo.weight to be bf16, got torch.float32

Expected layers.11.ffn_norm.weight to be bf16, got torch.float32Expected layers.17.ffn_norm.weight to be bf16, got torch.float32

Expected layers.12.attention.wv.weight to be bf16, got torch.float32Expected layers.18.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.13.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.14.attention.wk.weight to be bf16, got torch.float32
Expected layers.18.attention.wq.weight to be bf16, got torch.float32
Expected layers.19.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.11.attention_norm.weight to be bf16, got torch.float32
Expected layers.12.attention.wq.weight to be bf16, got torch.float32

Expected layers.13.attention_norm.weight to be bf16, got torch.float32
Expected layers.12.attention.wo.weight to be bf16, got torch.float32
Expected layers.18.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.14.attention.wv.weight to be bf16, got torch.float32
Expected layers.18.attention.wk.weight to be bf16, got torch.float32Expected layers.19.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.11.ffn_norm.weight to be bf16, got torch.float32
Expected layers.12.attention.wk.weight to be bf16, got torch.float32Expected layers.18.attention_norm.weight to be bf16, got torch.float32
Expected layers.13.ffn_norm.weight to be bf16, got torch.float32

Expected layers.12.feed_forward.w1.weight to be bf16, got torch.float32


Expected layers.14.attention.wo.weight to be bf16, got torch.float32
Expected layers.18.attention.wv.weight to be bf16, got torch.float32Expected layers.19.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.12.attention.wq.weight to be bf16, got torch.float32Expected layers.12.attention.wv.weight to be bf16, got torch.float32

Expected layers.18.ffn_norm.weight to be bf16, got torch.float32Expected layers.14.attention.wq.weight to be bf16, got torch.float32

Expected layers.12.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.18.attention.wo.weight to be bf16, got torch.float32
Expected layers.14.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.19.attention_norm.weight to be bf16, got torch.float32

Expected layers.12.attention.wk.weight to be bf16, got torch.float32Expected layers.12.attention.wo.weight to be bf16, got torch.float32
Expected layers.19.attention.wq.weight to be bf16, got torch.float32
Expected layers.14.attention.wk.weight to be bf16, got torch.float32
Expected layers.12.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.18.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.14.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.19.ffn_norm.weight to be bf16, got torch.float32
Expected layers.12.attention.wv.weight to be bf16, got torch.float32
Expected layers.12.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.19.attention.wk.weight to be bf16, got torch.float32
Expected layers.14.attention.wv.weight to be bf16, got torch.float32
Expected layers.12.attention_norm.weight to be bf16, got torch.float32
Expected layers.18.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.14.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.20.attention.wq.weight to be bf16, got torch.float32
Expected layers.12.attention.wo.weight to be bf16, got torch.float32
Expected layers.12.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.19.attention.wv.weight to be bf16, got torch.float32
Expected layers.14.attention.wo.weight to be bf16, got torch.float32
Expected layers.12.ffn_norm.weight to be bf16, got torch.float32Expected layers.18.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.14.attention_norm.weight to be bf16, got torch.float32
Expected layers.20.attention.wk.weight to be bf16, got torch.float32
Expected layers.12.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.12.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.19.attention.wo.weight to be bf16, got torch.float32
Expected layers.14.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.13.attention.wq.weight to be bf16, got torch.float32
Expected layers.18.attention_norm.weight to be bf16, got torch.float32
Expected layers.14.ffn_norm.weight to be bf16, got torch.float32

Expected layers.20.attention.wv.weight to be bf16, got torch.float32
Expected layers.12.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.12.attention_norm.weight to be bf16, got torch.float32
Expected layers.19.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.14.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.13.attention.wk.weight to be bf16, got torch.float32Expected layers.20.attention.wo.weight to be bf16, got torch.float32Expected layers.18.ffn_norm.weight to be bf16, got torch.float32

Expected layers.15.attention.wq.weight to be bf16, got torch.float32


Expected layers.12.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.12.ffn_norm.weight to be bf16, got torch.float32Expected layers.19.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.14.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.13.attention.wv.weight to be bf16, got torch.float32

Expected layers.20.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.19.attention.wq.weight to be bf16, got torch.float32

Expected layers.15.attention.wk.weight to be bf16, got torch.float32

Expected layers.12.attention_norm.weight to be bf16, got torch.float32Expected layers.13.attention.wq.weight to be bf16, got torch.float32
Expected layers.13.attention.wo.weight to be bf16, got torch.float32Expected layers.19.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.14.attention_norm.weight to be bf16, got torch.float32
Expected layers.20.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.19.attention.wk.weight to be bf16, got torch.float32

Expected layers.15.attention.wv.weight to be bf16, got torch.float32

Expected layers.12.ffn_norm.weight to be bf16, got torch.float32Expected layers.13.attention.wk.weight to be bf16, got torch.float32
Expected layers.13.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.19.attention_norm.weight to be bf16, got torch.float32
Expected layers.14.ffn_norm.weight to be bf16, got torch.float32Expected layers.20.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.19.attention.wv.weight to be bf16, got torch.float32

Expected layers.15.attention.wo.weight to be bf16, got torch.float32
Expected layers.13.attention.wq.weight to be bf16, got torch.float32
Expected layers.13.attention.wv.weight to be bf16, got torch.float32
Expected layers.13.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.19.ffn_norm.weight to be bf16, got torch.float32
Expected layers.15.attention.wq.weight to be bf16, got torch.float32Expected layers.20.attention_norm.weight to be bf16, got torch.float32
Expected layers.19.attention.wo.weight to be bf16, got torch.float32

Expected layers.15.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.13.attention.wk.weight to be bf16, got torch.float32
Expected layers.13.attention.wo.weight to be bf16, got torch.float32
Expected layers.13.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.20.attention.wq.weight to be bf16, got torch.float32
Expected layers.15.attention.wk.weight to be bf16, got torch.float32
Expected layers.20.ffn_norm.weight to be bf16, got torch.float32Expected layers.19.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.15.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.13.attention.wv.weight to be bf16, got torch.float32
Expected layers.13.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.13.attention_norm.weight to be bf16, got torch.float32
Expected layers.20.attention.wk.weight to be bf16, got torch.float32
Expected layers.15.attention.wv.weight to be bf16, got torch.float32
Expected layers.21.attention.wq.weight to be bf16, got torch.float32
Expected layers.19.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.15.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.13.attention.wo.weight to be bf16, got torch.float32

Expected layers.13.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.13.ffn_norm.weight to be bf16, got torch.float32
Expected layers.20.attention.wv.weight to be bf16, got torch.float32
Expected layers.15.attention.wo.weight to be bf16, got torch.float32
Expected layers.21.attention.wk.weight to be bf16, got torch.float32
Expected layers.19.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.15.attention_norm.weight to be bf16, got torch.float32
Expected layers.14.attention.wq.weight to be bf16, got torch.float32Expected layers.13.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.13.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.20.attention.wo.weight to be bf16, got torch.float32
Expected layers.15.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.21.attention.wv.weight to be bf16, got torch.float32
Expected layers.19.attention_norm.weight to be bf16, got torch.float32
Expected layers.15.ffn_norm.weight to be bf16, got torch.float32
Expected layers.14.attention.wk.weight to be bf16, got torch.float32
Expected layers.13.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.13.attention_norm.weight to be bf16, got torch.float32
Expected layers.20.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.15.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.21.attention.wo.weight to be bf16, got torch.float32
Expected layers.19.ffn_norm.weight to be bf16, got torch.float32

Expected layers.16.attention.wq.weight to be bf16, got torch.float32
Expected layers.14.attention.wv.weight to be bf16, got torch.float32
Expected layers.13.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.13.ffn_norm.weight to be bf16, got torch.float32
Expected layers.20.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.15.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.21.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.20.attention.wq.weight to be bf16, got torch.float32
Expected layers.16.attention.wk.weight to be bf16, got torch.float32
Expected layers.14.attention.wo.weight to be bf16, got torch.float32
Expected layers.13.attention_norm.weight to be bf16, got torch.float32
Expected layers.14.attention.wq.weight to be bf16, got torch.float32
Expected layers.20.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.15.attention_norm.weight to be bf16, got torch.float32Expected layers.21.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.20.attention.wk.weight to be bf16, got torch.float32

Expected layers.16.attention.wv.weight to be bf16, got torch.float32
Expected layers.14.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.13.ffn_norm.weight to be bf16, got torch.float32
Expected layers.14.attention.wk.weight to be bf16, got torch.float32
Expected layers.20.attention_norm.weight to be bf16, got torch.float32
Expected layers.15.ffn_norm.weight to be bf16, got torch.float32
Expected layers.21.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.20.attention.wv.weight to be bf16, got torch.float32
Expected layers.16.attention.wo.weight to be bf16, got torch.float32Expected layers.14.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.14.attention.wq.weight to be bf16, got torch.float32
Expected layers.14.attention.wv.weight to be bf16, got torch.float32
Expected layers.20.ffn_norm.weight to be bf16, got torch.float32Expected layers.16.attention.wq.weight to be bf16, got torch.float32


Expected layers.20.attention.wo.weight to be bf16, got torch.float32Expected layers.21.attention_norm.weight to be bf16, got torch.float32
Expected layers.16.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.14.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.14.attention.wk.weight to be bf16, got torch.float32Expected layers.14.attention.wo.weight to be bf16, got torch.float32Expected layers.21.attention.wq.weight to be bf16, got torch.float32



Expected layers.16.attention.wk.weight to be bf16, got torch.float32

Expected layers.20.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.16.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.21.ffn_norm.weight to be bf16, got torch.float32Expected layers.14.attention_norm.weight to be bf16, got torch.float32Expected layers.14.attention.wv.weight to be bf16, got torch.float32

Expected layers.14.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.21.attention.wk.weight to be bf16, got torch.float32
Expected layers.16.attention.wv.weight to be bf16, got torch.float32Expected layers.20.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.16.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.22.attention.wq.weight to be bf16, got torch.float32
Expected layers.14.ffn_norm.weight to be bf16, got torch.float32Expected layers.14.attention.wo.weight to be bf16, got torch.float32


Expected layers.21.attention.wv.weight to be bf16, got torch.float32
Expected layers.14.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.16.attention.wo.weight to be bf16, got torch.float32Expected layers.20.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.16.attention_norm.weight to be bf16, got torch.float32
Expected layers.22.attention.wk.weight to be bf16, got torch.float32
Expected layers.15.attention.wq.weight to be bf16, got torch.float32Expected layers.14.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.21.attention.wo.weight to be bf16, got torch.float32
Expected layers.14.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.16.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.20.attention_norm.weight to be bf16, got torch.float32

Expected layers.16.ffn_norm.weight to be bf16, got torch.float32Expected layers.22.attention.wv.weight to be bf16, got torch.float32
Expected layers.15.attention.wk.weight to be bf16, got torch.float32Expected layers.14.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.21.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.14.attention_norm.weight to be bf16, got torch.float32Expected layers.16.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.20.ffn_norm.weight to be bf16, got torch.float32
Expected layers.17.attention.wq.weight to be bf16, got torch.float32
Expected layers.22.attention.wo.weight to be bf16, got torch.float32
Expected layers.15.attention.wv.weight to be bf16, got torch.float32Expected layers.14.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.21.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.14.ffn_norm.weight to be bf16, got torch.float32
Expected layers.16.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.21.attention.wq.weight to be bf16, got torch.float32

Expected layers.17.attention.wk.weight to be bf16, got torch.float32Expected layers.22.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.15.attention.wo.weight to be bf16, got torch.float32Expected layers.14.attention_norm.weight to be bf16, got torch.float32

Expected layers.21.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.15.attention.wq.weight to be bf16, got torch.float32Expected layers.16.attention_norm.weight to be bf16, got torch.float32
Expected layers.21.attention.wk.weight to be bf16, got torch.float32

Expected layers.17.attention.wv.weight to be bf16, got torch.float32Expected layers.22.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.15.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.14.ffn_norm.weight to be bf16, got torch.float32

Expected layers.21.attention_norm.weight to be bf16, got torch.float32
Expected layers.15.attention.wk.weight to be bf16, got torch.float32
Expected layers.16.ffn_norm.weight to be bf16, got torch.float32
Expected layers.21.attention.wv.weight to be bf16, got torch.float32
Expected layers.22.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.17.attention.wo.weight to be bf16, got torch.float32

Expected layers.15.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.15.attention.wq.weight to be bf16, got torch.float32Expected layers.21.ffn_norm.weight to be bf16, got torch.float32
Expected layers.15.attention.wv.weight to be bf16, got torch.float32

Expected layers.17.attention.wq.weight to be bf16, got torch.float32Expected layers.21.attention.wo.weight to be bf16, got torch.float32
Expected layers.22.attention_norm.weight to be bf16, got torch.float32Expected layers.17.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.15.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.15.attention.wk.weight to be bf16, got torch.float32Expected layers.22.attention.wq.weight to be bf16, got torch.float32
Expected layers.15.attention.wo.weight to be bf16, got torch.float32
Expected layers.17.attention.wk.weight to be bf16, got torch.float32
Expected layers.21.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.22.ffn_norm.weight to be bf16, got torch.float32
Expected layers.17.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.15.attention_norm.weight to be bf16, got torch.float32
Expected layers.15.attention.wv.weight to be bf16, got torch.float32
Expected layers.22.attention.wk.weight to be bf16, got torch.float32
Expected layers.15.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.17.attention.wv.weight to be bf16, got torch.float32
Expected layers.21.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.23.attention.wq.weight to be bf16, got torch.float32
Expected layers.17.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.15.ffn_norm.weight to be bf16, got torch.float32
Expected layers.15.attention.wo.weight to be bf16, got torch.float32
Expected layers.22.attention.wv.weight to be bf16, got torch.float32
Expected layers.15.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.17.attention.wo.weight to be bf16, got torch.float32
Expected layers.21.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.23.attention.wk.weight to be bf16, got torch.float32
Expected layers.17.attention_norm.weight to be bf16, got torch.float32
Expected layers.16.attention.wq.weight to be bf16, got torch.float32
Expected layers.15.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.22.attention.wo.weight to be bf16, got torch.float32
Expected layers.15.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.17.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.21.attention_norm.weight to be bf16, got torch.float32
Expected layers.23.attention.wv.weight to be bf16, got torch.float32
Expected layers.17.ffn_norm.weight to be bf16, got torch.float32
Expected layers.16.attention.wk.weight to be bf16, got torch.float32
Expected layers.15.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.22.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.15.attention_norm.weight to be bf16, got torch.float32
Expected layers.17.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.21.ffn_norm.weight to be bf16, got torch.float32
Expected layers.23.attention.wo.weight to be bf16, got torch.float32
Expected layers.18.attention.wq.weight to be bf16, got torch.float32
Expected layers.16.attention.wv.weight to be bf16, got torch.float32
Expected layers.15.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.22.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.15.ffn_norm.weight to be bf16, got torch.float32
Expected layers.17.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.22.attention.wq.weight to be bf16, got torch.float32
Expected layers.23.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.18.attention.wk.weight to be bf16, got torch.float32
Expected layers.16.attention.wo.weight to be bf16, got torch.float32
Expected layers.15.attention_norm.weight to be bf16, got torch.float32

Expected layers.22.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.16.attention.wq.weight to be bf16, got torch.float32
Expected layers.17.attention_norm.weight to be bf16, got torch.float32
Expected layers.22.attention.wk.weight to be bf16, got torch.float32

Expected layers.23.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.18.attention.wv.weight to be bf16, got torch.float32
Expected layers.16.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.15.ffn_norm.weight to be bf16, got torch.float32

Expected layers.22.attention_norm.weight to be bf16, got torch.float32

Expected layers.16.attention.wk.weight to be bf16, got torch.float32Expected layers.17.ffn_norm.weight to be bf16, got torch.float32
Expected layers.22.attention.wv.weight to be bf16, got torch.float32
Expected layers.23.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.18.attention.wo.weight to be bf16, got torch.float32
Expected layers.16.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.16.attention.wq.weight to be bf16, got torch.float32

Expected layers.22.ffn_norm.weight to be bf16, got torch.float32
Expected layers.16.attention.wv.weight to be bf16, got torch.float32
Expected layers.18.attention.wq.weight to be bf16, got torch.float32
Expected layers.22.attention.wo.weight to be bf16, got torch.float32
Expected layers.23.attention_norm.weight to be bf16, got torch.float32
Expected layers.18.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.16.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.16.attention.wk.weight to be bf16, got torch.float32
Expected layers.23.attention.wq.weight to be bf16, got torch.float32
Expected layers.16.attention.wo.weight to be bf16, got torch.float32Expected layers.18.attention.wk.weight to be bf16, got torch.float32

Expected layers.22.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.23.ffn_norm.weight to be bf16, got torch.float32
Expected layers.18.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.16.attention_norm.weight to be bf16, got torch.float32
Expected layers.16.attention.wv.weight to be bf16, got torch.float32
Expected layers.23.attention.wk.weight to be bf16, got torch.float32
Expected layers.16.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.18.attention.wv.weight to be bf16, got torch.float32Expected layers.22.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.24.attention.wq.weight to be bf16, got torch.float32
Expected layers.18.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.16.ffn_norm.weight to be bf16, got torch.float32
Expected layers.16.attention.wo.weight to be bf16, got torch.float32
Expected layers.23.attention.wv.weight to be bf16, got torch.float32
Expected layers.16.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.18.attention.wo.weight to be bf16, got torch.float32Expected layers.22.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.24.attention.wk.weight to be bf16, got torch.float32
Expected layers.18.attention_norm.weight to be bf16, got torch.float32
Expected layers.17.attention.wq.weight to be bf16, got torch.float32
Expected layers.16.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.23.attention.wo.weight to be bf16, got torch.float32
Expected layers.16.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.18.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.22.attention_norm.weight to be bf16, got torch.float32
Expected layers.24.attention.wv.weight to be bf16, got torch.float32
Expected layers.18.ffn_norm.weight to be bf16, got torch.float32
Expected layers.17.attention.wk.weight to be bf16, got torch.float32Expected layers.16.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.23.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.16.attention_norm.weight to be bf16, got torch.float32

Expected layers.18.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.22.ffn_norm.weight to be bf16, got torch.float32

Expected layers.24.attention.wo.weight to be bf16, got torch.float32Expected layers.19.attention.wq.weight to be bf16, got torch.float32
Expected layers.17.attention.wv.weight to be bf16, got torch.float32Expected layers.16.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.23.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.16.ffn_norm.weight to be bf16, got torch.float32
Expected layers.18.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.23.attention.wq.weight to be bf16, got torch.float32

Expected layers.24.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.19.attention.wk.weight to be bf16, got torch.float32
Expected layers.17.attention.wo.weight to be bf16, got torch.float32Expected layers.16.attention_norm.weight to be bf16, got torch.float32

Expected layers.23.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.17.attention.wq.weight to be bf16, got torch.float32
Expected layers.18.attention_norm.weight to be bf16, got torch.float32
Expected layers.23.attention.wk.weight to be bf16, got torch.float32

Expected layers.24.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.19.attention.wv.weight to be bf16, got torch.float32
Expected layers.17.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.16.ffn_norm.weight to be bf16, got torch.float32

Expected layers.23.attention_norm.weight to be bf16, got torch.float32

Expected layers.17.attention.wk.weight to be bf16, got torch.float32Expected layers.18.ffn_norm.weight to be bf16, got torch.float32
Expected layers.23.attention.wv.weight to be bf16, got torch.float32
Expected layers.24.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.19.attention.wo.weight to be bf16, got torch.float32
Expected layers.17.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.17.attention.wq.weight to be bf16, got torch.float32

Expected layers.23.ffn_norm.weight to be bf16, got torch.float32
Expected layers.17.attention.wv.weight to be bf16, got torch.float32
Expected layers.19.attention.wq.weight to be bf16, got torch.float32
Expected layers.23.attention.wo.weight to be bf16, got torch.float32

Expected layers.24.attention_norm.weight to be bf16, got torch.float32Expected layers.19.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.17.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.17.attention.wk.weight to be bf16, got torch.float32

Expected layers.24.attention.wq.weight to be bf16, got torch.float32
Expected layers.23.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.17.attention.wo.weight to be bf16, got torch.float32Expected layers.19.attention.wk.weight to be bf16, got torch.float32

Expected layers.24.ffn_norm.weight to be bf16, got torch.float32
Expected layers.19.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.17.attention_norm.weight to be bf16, got torch.float32
Expected layers.17.attention.wv.weight to be bf16, got torch.float32
Expected layers.24.attention.wk.weight to be bf16, got torch.float32
Expected layers.23.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.17.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.19.attention.wv.weight to be bf16, got torch.float32Expected layers.17.ffn_norm.weight to be bf16, got torch.float32
Expected layers.25.attention.wq.weight to be bf16, got torch.float32
Expected layers.19.feed_forward.w3.weight to be bf16, got torch.float32


Expected layers.17.attention.wo.weight to be bf16, got torch.float32Expected layers.24.attention.wv.weight to be bf16, got torch.float32
Expected layers.23.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.17.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.19.attention.wo.weight to be bf16, got torch.float32Expected layers.18.attention.wq.weight to be bf16, got torch.float32Expected layers.25.attention.wk.weight to be bf16, got torch.float32

Expected layers.19.attention_norm.weight to be bf16, got torch.float32


Expected layers.17.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.24.attention.wo.weight to be bf16, got torch.float32
Expected layers.23.attention_norm.weight to be bf16, got torch.float32Expected layers.17.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.19.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.19.ffn_norm.weight to be bf16, got torch.float32
Expected layers.18.attention.wk.weight to be bf16, got torch.float32
Expected layers.25.attention.wv.weight to be bf16, got torch.float32

Expected layers.17.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.24.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.23.ffn_norm.weight to be bf16, got torch.float32
Expected layers.17.attention_norm.weight to be bf16, got torch.float32
Expected layers.19.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.20.attention.wq.weight to be bf16, got torch.float32
Expected layers.18.attention.wv.weight to be bf16, got torch.float32
Expected layers.25.attention.wo.weight to be bf16, got torch.float32
Expected layers.17.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.24.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.24.attention.wq.weight to be bf16, got torch.float32
Expected layers.17.ffn_norm.weight to be bf16, got torch.float32
Expected layers.19.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.20.attention.wk.weight to be bf16, got torch.float32
Expected layers.18.attention.wo.weight to be bf16, got torch.float32
Expected layers.25.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.17.attention_norm.weight to be bf16, got torch.float32
Expected layers.24.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.24.attention.wk.weight to be bf16, got torch.float32
Expected layers.18.attention.wq.weight to be bf16, got torch.float32
Expected layers.19.attention_norm.weight to be bf16, got torch.float32
Expected layers.20.attention.wv.weight to be bf16, got torch.float32
Expected layers.18.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.25.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.17.ffn_norm.weight to be bf16, got torch.float32
Expected layers.24.attention_norm.weight to be bf16, got torch.float32
Expected layers.24.attention.wv.weight to be bf16, got torch.float32
Expected layers.18.attention.wk.weight to be bf16, got torch.float32Expected layers.19.ffn_norm.weight to be bf16, got torch.float32

Expected layers.20.attention.wo.weight to be bf16, got torch.float32
Expected layers.18.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.25.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.18.attention.wq.weight to be bf16, got torch.float32
Expected layers.24.ffn_norm.weight to be bf16, got torch.float32
Expected layers.24.attention.wo.weight to be bf16, got torch.float32
Expected layers.18.attention.wv.weight to be bf16, got torch.float32
Expected layers.20.attention.wq.weight to be bf16, got torch.float32
Expected layers.20.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.18.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.25.attention_norm.weight to be bf16, got torch.float32
Expected layers.18.attention.wk.weight to be bf16, got torch.float32
Expected layers.25.attention.wq.weight to be bf16, got torch.float32
Expected layers.24.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.18.attention.wo.weight to be bf16, got torch.float32
Expected layers.20.attention.wk.weight to be bf16, got torch.float32
Expected layers.20.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.18.attention_norm.weight to be bf16, got torch.float32
Expected layers.25.ffn_norm.weight to be bf16, got torch.float32
Expected layers.18.attention.wv.weight to be bf16, got torch.float32

Expected layers.25.attention.wk.weight to be bf16, got torch.float32
Expected layers.24.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.20.attention.wv.weight to be bf16, got torch.float32
Expected layers.18.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.18.ffn_norm.weight to be bf16, got torch.float32


Expected layers.20.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.26.attention.wq.weight to be bf16, got torch.float32Expected layers.18.attention.wo.weight to be bf16, got torch.float32
Expected layers.25.attention.wv.weight to be bf16, got torch.float32Expected layers.24.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.20.attention.wo.weight to be bf16, got torch.float32

Expected layers.18.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.19.attention.wq.weight to be bf16, got torch.float32
Expected layers.20.attention_norm.weight to be bf16, got torch.float32
Expected layers.26.attention.wk.weight to be bf16, got torch.float32
Expected layers.18.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.25.attention.wo.weight to be bf16, got torch.float32Expected layers.24.attention_norm.weight to be bf16, got torch.float32

Expected layers.20.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.18.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.19.attention.wk.weight to be bf16, got torch.float32
Expected layers.20.ffn_norm.weight to be bf16, got torch.float32
Expected layers.26.attention.wv.weight to be bf16, got torch.float32
Expected layers.18.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.25.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.24.ffn_norm.weight to be bf16, got torch.float32

Expected layers.20.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.18.attention_norm.weight to be bf16, got torch.float32Expected layers.19.attention.wv.weight to be bf16, got torch.float32
Expected layers.21.attention.wq.weight to be bf16, got torch.float32
Expected layers.26.attention.wo.weight to be bf16, got torch.float32
Expected layers.18.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.25.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.25.attention.wq.weight to be bf16, got torch.float32

Expected layers.20.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.18.ffn_norm.weight to be bf16, got torch.float32Expected layers.19.attention.wo.weight to be bf16, got torch.float32
Expected layers.21.attention.wk.weight to be bf16, got torch.float32Expected layers.26.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.18.attention_norm.weight to be bf16, got torch.float32
Expected layers.25.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.25.attention.wk.weight to be bf16, got torch.float32
Expected layers.20.attention_norm.weight to be bf16, got torch.float32

Expected layers.19.attention.wq.weight to be bf16, got torch.float32Expected layers.19.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.21.attention.wv.weight to be bf16, got torch.float32Expected layers.26.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.18.ffn_norm.weight to be bf16, got torch.float32
Expected layers.25.attention_norm.weight to be bf16, got torch.float32
Expected layers.25.attention.wv.weight to be bf16, got torch.float32
Expected layers.20.ffn_norm.weight to be bf16, got torch.float32

Expected layers.19.attention.wk.weight to be bf16, got torch.float32Expected layers.19.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.21.attention.wo.weight to be bf16, got torch.float32Expected layers.26.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.19.attention.wq.weight to be bf16, got torch.float32Expected layers.25.ffn_norm.weight to be bf16, got torch.float32Expected layers.25.attention.wo.weight to be bf16, got torch.float32

Expected layers.21.attention.wq.weight to be bf16, got torch.float32


Expected layers.19.attention.wv.weight to be bf16, got torch.float32Expected layers.19.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.21.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.26.attention_norm.weight to be bf16, got torch.float32
Expected layers.19.attention.wk.weight to be bf16, got torch.float32Expected layers.26.attention.wq.weight to be bf16, got torch.float32
Expected layers.25.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.21.attention.wk.weight to be bf16, got torch.float32
Expected layers.19.attention.wo.weight to be bf16, got torch.float32
Expected layers.19.attention_norm.weight to be bf16, got torch.float32
Expected layers.21.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.26.ffn_norm.weight to be bf16, got torch.float32Expected layers.19.attention.wv.weight to be bf16, got torch.float32
Expected layers.26.attention.wk.weight to be bf16, got torch.float32
Expected layers.25.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.21.attention.wv.weight to be bf16, got torch.float32

Expected layers.19.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.19.ffn_norm.weight to be bf16, got torch.float32Expected layers.21.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.27.attention.wq.weight to be bf16, got torch.float32
Expected layers.19.attention.wo.weight to be bf16, got torch.float32
Expected layers.26.attention.wv.weight to be bf16, got torch.float32Expected layers.25.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.21.attention.wo.weight to be bf16, got torch.float32

Expected layers.19.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.20.attention.wq.weight to be bf16, got torch.float32
Expected layers.21.attention_norm.weight to be bf16, got torch.float32
Expected layers.27.attention.wk.weight to be bf16, got torch.float32
Expected layers.19.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.26.attention.wo.weight to be bf16, got torch.float32
Expected layers.25.attention_norm.weight to be bf16, got torch.float32
Expected layers.21.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.19.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.20.attention.wk.weight to be bf16, got torch.float32
Expected layers.21.ffn_norm.weight to be bf16, got torch.float32
Expected layers.27.attention.wv.weight to be bf16, got torch.float32
Expected layers.19.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.26.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.25.ffn_norm.weight to be bf16, got torch.float32
Expected layers.21.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.19.attention_norm.weight to be bf16, got torch.float32
Expected layers.20.attention.wv.weight to be bf16, got torch.float32
Expected layers.27.attention.wo.weight to be bf16, got torch.float32Expected layers.22.attention.wq.weight to be bf16, got torch.float32

Expected layers.19.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.26.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.26.attention.wq.weight to be bf16, got torch.float32
Expected layers.21.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.19.ffn_norm.weight to be bf16, got torch.float32Expected layers.20.attention.wo.weight to be bf16, got torch.float32
Expected layers.27.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.22.attention.wk.weight to be bf16, got torch.float32
Expected layers.19.attention_norm.weight to be bf16, got torch.float32
Expected layers.26.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.26.attention.wk.weight to be bf16, got torch.float32
Expected layers.21.attention_norm.weight to be bf16, got torch.float32
Expected layers.20.attention.wq.weight to be bf16, got torch.float32

Expected layers.20.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.27.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.26.attention_norm.weight to be bf16, got torch.float32Expected layers.22.attention.wv.weight to be bf16, got torch.float32
Expected layers.19.ffn_norm.weight to be bf16, got torch.float32
Expected layers.26.attention.wv.weight to be bf16, got torch.float32

Expected layers.21.ffn_norm.weight to be bf16, got torch.float32
Expected layers.20.attention.wk.weight to be bf16, got torch.float32

Expected layers.20.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.27.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.26.ffn_norm.weight to be bf16, got torch.float32Expected layers.22.attention.wo.weight to be bf16, got torch.float32

Expected layers.20.attention.wq.weight to be bf16, got torch.float32Expected layers.26.attention.wo.weight to be bf16, got torch.float32

Expected layers.22.attention.wq.weight to be bf16, got torch.float32
Expected layers.20.attention.wv.weight to be bf16, got torch.float32
Expected layers.20.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.27.attention_norm.weight to be bf16, got torch.float32
Expected layers.27.attention.wq.weight to be bf16, got torch.float32
Expected layers.22.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.20.attention.wk.weight to be bf16, got torch.float32Expected layers.26.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.22.attention.wk.weight to be bf16, got torch.float32
Expected layers.20.attention.wo.weight to be bf16, got torch.float32
Expected layers.20.attention_norm.weight to be bf16, got torch.float32
Expected layers.27.ffn_norm.weight to be bf16, got torch.float32
Expected layers.27.attention.wk.weight to be bf16, got torch.float32Expected layers.22.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.20.attention.wv.weight to be bf16, got torch.float32
Expected layers.26.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.22.attention.wv.weight to be bf16, got torch.float32

Expected layers.20.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.20.ffn_norm.weight to be bf16, got torch.float32
Expected layers.28.attention.wq.weight to be bf16, got torch.float32
Expected layers.27.attention.wv.weight to be bf16, got torch.float32
Expected layers.22.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.20.attention.wo.weight to be bf16, got torch.float32Expected layers.26.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.22.attention.wo.weight to be bf16, got torch.float32
Expected layers.20.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.21.attention.wq.weight to be bf16, got torch.float32
Expected layers.28.attention.wk.weight to be bf16, got torch.float32
Expected layers.27.attention.wo.weight to be bf16, got torch.float32
Expected layers.22.attention_norm.weight to be bf16, got torch.float32
Expected layers.20.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.26.attention_norm.weight to be bf16, got torch.float32

Expected layers.22.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.20.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.21.attention.wk.weight to be bf16, got torch.float32Expected layers.28.attention.wv.weight to be bf16, got torch.float32

Expected layers.27.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.22.ffn_norm.weight to be bf16, got torch.float32
Expected layers.20.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.26.ffn_norm.weight to be bf16, got torch.float32

Expected layers.22.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.20.attention_norm.weight to be bf16, got torch.float32

Expected layers.21.attention.wv.weight to be bf16, got torch.float32Expected layers.28.attention.wo.weight to be bf16, got torch.float32

Expected layers.27.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.23.attention.wq.weight to be bf16, got torch.float32

Expected layers.20.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.27.attention.wq.weight to be bf16, got torch.float32

Expected layers.22.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.20.ffn_norm.weight to be bf16, got torch.float32
Expected layers.21.attention.wo.weight to be bf16, got torch.float32
Expected layers.28.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.27.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.23.attention.wk.weight to be bf16, got torch.float32Expected layers.22.attention_norm.weight to be bf16, got torch.float32
Expected layers.20.attention_norm.weight to be bf16, got torch.float32Expected layers.27.attention.wk.weight to be bf16, got torch.float32


Expected layers.21.attention.wq.weight to be bf16, got torch.float32
Expected layers.21.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.28.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.27.attention_norm.weight to be bf16, got torch.float32Expected layers.23.attention.wv.weight to be bf16, got torch.float32
Expected layers.22.ffn_norm.weight to be bf16, got torch.float32
Expected layers.20.ffn_norm.weight to be bf16, got torch.float32
Expected layers.27.attention.wv.weight to be bf16, got torch.float32
Expected layers.21.attention.wk.weight to be bf16, got torch.float32Expected layers.21.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.28.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.27.ffn_norm.weight to be bf16, got torch.float32Expected layers.23.attention.wo.weight to be bf16, got torch.float32
Expected layers.23.attention.wq.weight to be bf16, got torch.float32
Expected layers.21.attention.wq.weight to be bf16, got torch.float32
Expected layers.27.attention.wo.weight to be bf16, got torch.float32
Expected layers.21.attention.wv.weight to be bf16, got torch.float32Expected layers.21.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.28.attention_norm.weight to be bf16, got torch.float32

Expected layers.28.attention.wq.weight to be bf16, got torch.float32
Expected layers.23.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.23.attention.wk.weight to be bf16, got torch.float32
Expected layers.21.attention.wk.weight to be bf16, got torch.float32
Expected layers.27.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.21.attention.wo.weight to be bf16, got torch.float32Expected layers.28.ffn_norm.weight to be bf16, got torch.float32
Expected layers.21.attention_norm.weight to be bf16, got torch.float32
Expected layers.28.attention.wk.weight to be bf16, got torch.float32

Expected layers.23.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.23.attention.wv.weight to be bf16, got torch.float32
Expected layers.21.attention.wv.weight to be bf16, got torch.float32
Expected layers.27.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.21.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.29.attention.wq.weight to be bf16, got torch.float32
Expected layers.21.ffn_norm.weight to be bf16, got torch.float32Expected layers.28.attention.wv.weight to be bf16, got torch.float32

Expected layers.23.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.23.attention.wo.weight to be bf16, got torch.float32

Expected layers.21.attention.wo.weight to be bf16, got torch.float32Expected layers.27.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.29.attention.wk.weight to be bf16, got torch.float32Expected layers.21.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.23.attention_norm.weight to be bf16, got torch.float32Expected layers.28.attention.wo.weight to be bf16, got torch.float32Expected layers.22.attention.wq.weight to be bf16, got torch.float32

Expected layers.23.feed_forward.w1.weight to be bf16, got torch.float32


Expected layers.21.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.27.attention_norm.weight to be bf16, got torch.float32
Expected layers.29.attention.wv.weight to be bf16, got torch.float32
Expected layers.21.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.23.ffn_norm.weight to be bf16, got torch.float32Expected layers.28.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.22.attention.wk.weight to be bf16, got torch.float32


Expected layers.23.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.21.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.27.ffn_norm.weight to be bf16, got torch.float32
Expected layers.29.attention.wo.weight to be bf16, got torch.float32
Expected layers.21.attention_norm.weight to be bf16, got torch.float32
Expected layers.24.attention.wq.weight to be bf16, got torch.float32Expected layers.28.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.22.attention.wv.weight to be bf16, got torch.float32


Expected layers.23.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.21.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.28.attention.wq.weight to be bf16, got torch.float32
Expected layers.29.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.21.ffn_norm.weight to be bf16, got torch.float32
Expected layers.24.attention.wk.weight to be bf16, got torch.float32Expected layers.28.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.22.attention.wo.weight to be bf16, got torch.float32


Expected layers.23.attention_norm.weight to be bf16, got torch.float32
Expected layers.21.attention_norm.weight to be bf16, got torch.float32Expected layers.28.attention.wk.weight to be bf16, got torch.float32
Expected layers.29.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.22.attention.wq.weight to be bf16, got torch.float32
Expected layers.24.attention.wv.weight to be bf16, got torch.float32Expected layers.28.attention_norm.weight to be bf16, got torch.float32
Expected layers.22.feed_forward.w1.weight to be bf16, got torch.float32


Expected layers.23.ffn_norm.weight to be bf16, got torch.float32
Expected layers.21.ffn_norm.weight to be bf16, got torch.float32Expected layers.28.attention.wv.weight to be bf16, got torch.float32
Expected layers.29.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.22.attention.wk.weight to be bf16, got torch.float32
Expected layers.24.attention.wo.weight to be bf16, got torch.float32Expected layers.28.ffn_norm.weight to be bf16, got torch.float32
Expected layers.22.feed_forward.w2.weight to be bf16, got torch.float32


Expected layers.24.attention.wq.weight to be bf16, got torch.float32
Expected layers.22.attention.wq.weight to be bf16, got torch.float32Expected layers.28.attention.wo.weight to be bf16, got torch.float32
Expected layers.29.attention_norm.weight to be bf16, got torch.float32
Expected layers.22.attention.wv.weight to be bf16, got torch.float32
Expected layers.24.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.29.attention.wq.weight to be bf16, got torch.float32
Expected layers.22.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.24.attention.wk.weight to be bf16, got torch.float32
Expected layers.22.attention.wk.weight to be bf16, got torch.float32
Expected layers.28.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.29.ffn_norm.weight to be bf16, got torch.float32
Expected layers.22.attention.wo.weight to be bf16, got torch.float32
Expected layers.24.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.29.attention.wk.weight to be bf16, got torch.float32
Expected layers.22.attention_norm.weight to be bf16, got torch.float32
Expected layers.24.attention.wv.weight to be bf16, got torch.float32
Expected layers.22.attention.wv.weight to be bf16, got torch.float32Expected layers.28.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.30.attention.wq.weight to be bf16, got torch.float32
Expected layers.22.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.24.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.29.attention.wv.weight to be bf16, got torch.float32
Expected layers.22.ffn_norm.weight to be bf16, got torch.float32
Expected layers.24.attention.wo.weight to be bf16, got torch.float32
Expected layers.22.attention.wo.weight to be bf16, got torch.float32
Expected layers.28.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.30.attention.wk.weight to be bf16, got torch.float32
Expected layers.22.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.24.attention_norm.weight to be bf16, got torch.float32
Expected layers.29.attention.wo.weight to be bf16, got torch.float32
Expected layers.23.attention.wq.weight to be bf16, got torch.float32
Expected layers.24.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.22.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.28.attention_norm.weight to be bf16, got torch.float32Expected layers.30.attention.wv.weight to be bf16, got torch.float32
Expected layers.22.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.24.ffn_norm.weight to be bf16, got torch.float32

Expected layers.29.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.23.attention.wk.weight to be bf16, got torch.float32

Expected layers.24.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.22.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.28.ffn_norm.weight to be bf16, got torch.float32Expected layers.30.attention.wo.weight to be bf16, got torch.float32

Expected layers.22.attention_norm.weight to be bf16, got torch.float32Expected layers.25.attention.wq.weight to be bf16, got torch.float32
Expected layers.29.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.23.attention.wv.weight to be bf16, got torch.float32


Expected layers.24.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.22.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.29.attention.wq.weight to be bf16, got torch.float32Expected layers.30.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.22.ffn_norm.weight to be bf16, got torch.float32
Expected layers.25.attention.wk.weight to be bf16, got torch.float32
Expected layers.29.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.23.attention.wo.weight to be bf16, got torch.float32

Expected layers.24.attention_norm.weight to be bf16, got torch.float32Expected layers.22.attention_norm.weight to be bf16, got torch.float32
Expected layers.29.attention.wk.weight to be bf16, got torch.float32Expected layers.30.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.23.attention.wq.weight to be bf16, got torch.float32
Expected layers.25.attention.wv.weight to be bf16, got torch.float32
Expected layers.29.attention_norm.weight to be bf16, got torch.float32
Expected layers.23.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.24.ffn_norm.weight to be bf16, got torch.float32Expected layers.22.ffn_norm.weight to be bf16, got torch.float32
Expected layers.29.attention.wv.weight to be bf16, got torch.float32Expected layers.30.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.23.attention.wk.weight to be bf16, got torch.float32
Expected layers.25.attention.wo.weight to be bf16, got torch.float32
Expected layers.29.ffn_norm.weight to be bf16, got torch.float32
Expected layers.23.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.25.attention.wq.weight to be bf16, got torch.float32Expected layers.23.attention.wq.weight to be bf16, got torch.float32
Expected layers.29.attention.wo.weight to be bf16, got torch.float32Expected layers.30.attention_norm.weight to be bf16, got torch.float32

Expected layers.23.attention.wv.weight to be bf16, got torch.float32Expected layers.25.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.30.attention.wq.weight to be bf16, got torch.float32

Expected layers.23.feed_forward.w3.weight to be bf16, got torch.float32


Expected layers.25.attention.wk.weight to be bf16, got torch.float32Expected layers.23.attention.wk.weight to be bf16, got torch.float32
Expected layers.29.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.30.ffn_norm.weight to be bf16, got torch.float32
Expected layers.23.attention.wo.weight to be bf16, got torch.float32
Expected layers.25.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.30.attention.wk.weight to be bf16, got torch.float32
Expected layers.23.attention_norm.weight to be bf16, got torch.float32


Expected layers.25.attention.wv.weight to be bf16, got torch.float32
Expected layers.23.attention.wv.weight to be bf16, got torch.float32Expected layers.29.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.23.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.31.attention.wq.weight to be bf16, got torch.float32

Expected layers.25.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.30.attention.wv.weight to be bf16, got torch.float32
Expected layers.23.ffn_norm.weight to be bf16, got torch.float32


Expected layers.25.attention.wo.weight to be bf16, got torch.float32
Expected layers.23.attention.wo.weight to be bf16, got torch.float32Expected layers.29.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.23.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.31.attention.wk.weight to be bf16, got torch.float32
Expected layers.25.attention_norm.weight to be bf16, got torch.float32Expected layers.30.attention.wo.weight to be bf16, got torch.float32
Expected layers.24.attention.wq.weight to be bf16, got torch.float32


Expected layers.25.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.23.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.29.attention_norm.weight to be bf16, got torch.float32
Expected layers.23.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.31.attention.wv.weight to be bf16, got torch.float32
Expected layers.25.ffn_norm.weight to be bf16, got torch.float32Expected layers.30.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.24.attention.wk.weight to be bf16, got torch.float32

Expected layers.25.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.23.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.29.ffn_norm.weight to be bf16, got torch.float32
Expected layers.23.attention_norm.weight to be bf16, got torch.float32
Expected layers.31.attention.wo.weight to be bf16, got torch.float32
Expected layers.26.attention.wq.weight to be bf16, got torch.float32
Expected layers.30.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.24.attention.wv.weight to be bf16, got torch.float32

Expected layers.25.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.23.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.30.attention.wq.weight to be bf16, got torch.float32
Expected layers.23.ffn_norm.weight to be bf16, got torch.float32
Expected layers.31.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.26.attention.wk.weight to be bf16, got torch.float32
Expected layers.30.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.24.attention.wo.weight to be bf16, got torch.float32


Expected layers.25.attention_norm.weight to be bf16, got torch.float32Expected layers.23.attention_norm.weight to be bf16, got torch.float32
Expected layers.30.attention.wk.weight to be bf16, got torch.float32
Expected layers.24.attention.wq.weight to be bf16, got torch.float32
Expected layers.31.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.26.attention.wv.weight to be bf16, got torch.float32Expected layers.30.attention_norm.weight to be bf16, got torch.float32
Expected layers.23.ffn_norm.weight to be bf16, got torch.float32
Expected layers.24.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.25.ffn_norm.weight to be bf16, got torch.float32

Expected layers.30.attention.wv.weight to be bf16, got torch.float32
Expected layers.24.attention.wk.weight to be bf16, got torch.float32
Expected layers.31.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.26.attention.wo.weight to be bf16, got torch.float32
Expected layers.30.ffn_norm.weight to be bf16, got torch.float32
Expected layers.24.attention.wq.weight to be bf16, got torch.float32
Expected layers.24.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.30.attention.wo.weight to be bf16, got torch.float32Expected layers.26.attention.wq.weight to be bf16, got torch.float32
Expected layers.24.attention.wv.weight to be bf16, got torch.float32Expected layers.31.attention_norm.weight to be bf16, got torch.float32

Expected layers.26.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.31.attention.wq.weight to be bf16, got torch.float32
Expected layers.24.attention.wk.weight to be bf16, got torch.float32Expected layers.24.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.30.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.26.attention.wk.weight to be bf16, got torch.float32

Expected layers.24.attention.wo.weight to be bf16, got torch.float32
Expected layers.31.ffn_norm.weight to be bf16, got torch.float32


Expected layers.26.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.31.attention.wk.weight to be bf16, got torch.float32Expected layers.24.attention.wv.weight to be bf16, got torch.float32Expected layers.24.attention_norm.weight to be bf16, got torch.float32Expected layers.30.feed_forward.w2.weight to be bf16, got torch.float32


Expected layers.32.attention.wq.weight to be bf16, got torch.float32
Expected layers.26.attention.wv.weight to be bf16, got torch.float32
Expected layers.24.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.26.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.31.attention.wv.weight to be bf16, got torch.float32Expected layers.24.attention.wo.weight to be bf16, got torch.float32
Expected layers.24.ffn_norm.weight to be bf16, got torch.float32
Expected layers.30.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.32.attention.wk.weight to be bf16, got torch.float32
Expected layers.26.attention.wo.weight to be bf16, got torch.float32
Expected layers.24.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.26.attention_norm.weight to be bf16, got torch.float32
Expected layers.31.attention.wo.weight to be bf16, got torch.float32
Expected layers.24.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.25.attention.wq.weight to be bf16, got torch.float32
Expected layers.30.attention_norm.weight to be bf16, got torch.float32
Expected layers.32.attention.wv.weight to be bf16, got torch.float32
Expected layers.26.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.24.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.26.ffn_norm.weight to be bf16, got torch.float32
Expected layers.31.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.24.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.25.attention.wk.weight to be bf16, got torch.float32
Expected layers.30.ffn_norm.weight to be bf16, got torch.float32
Expected layers.32.attention.wo.weight to be bf16, got torch.float32Expected layers.26.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.24.attention_norm.weight to be bf16, got torch.float32
Expected layers.27.attention.wq.weight to be bf16, got torch.float32
Expected layers.31.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.24.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.25.attention.wv.weight to be bf16, got torch.float32
Expected layers.31.attention.wq.weight to be bf16, got torch.float32
Expected layers.32.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.26.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.24.ffn_norm.weight to be bf16, got torch.float32
Expected layers.27.attention.wk.weight to be bf16, got torch.float32
Expected layers.31.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.24.attention_norm.weight to be bf16, got torch.float32

Expected layers.25.attention.wo.weight to be bf16, got torch.float32
Expected layers.31.attention.wk.weight to be bf16, got torch.float32
Expected layers.32.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.26.attention_norm.weight to be bf16, got torch.float32
Expected layers.25.attention.wq.weight to be bf16, got torch.float32
Expected layers.27.attention.wv.weight to be bf16, got torch.float32
Expected layers.31.attention_norm.weight to be bf16, got torch.float32Expected layers.24.ffn_norm.weight to be bf16, got torch.float32

Expected layers.25.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.31.attention.wv.weight to be bf16, got torch.float32
Expected layers.32.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.26.ffn_norm.weight to be bf16, got torch.float32
Expected layers.25.attention.wk.weight to be bf16, got torch.float32
Expected layers.27.attention.wo.weight to be bf16, got torch.float32
Expected layers.31.ffn_norm.weight to be bf16, got torch.float32
Expected layers.25.attention.wq.weight to be bf16, got torch.float32
Expected layers.25.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.31.attention.wo.weight to be bf16, got torch.float32Expected layers.32.attention_norm.weight to be bf16, got torch.float32
Expected layers.27.attention.wq.weight to be bf16, got torch.float32

Expected layers.25.attention.wv.weight to be bf16, got torch.float32
Expected layers.27.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.32.attention.wq.weight to be bf16, got torch.float32
Expected layers.25.attention.wk.weight to be bf16, got torch.float32
Expected layers.25.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.31.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.32.ffn_norm.weight to be bf16, got torch.float32
Expected layers.27.attention.wk.weight to be bf16, got torch.float32
Expected layers.25.attention.wo.weight to be bf16, got torch.float32
Expected layers.27.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.32.attention.wk.weight to be bf16, got torch.float32
Expected layers.25.attention.wv.weight to be bf16, got torch.float32
Expected layers.25.attention_norm.weight to be bf16, got torch.float32
Expected layers.31.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.33.attention.wq.weight to be bf16, got torch.float32
Expected layers.27.attention.wv.weight to be bf16, got torch.float32
Expected layers.25.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.27.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.25.attention.wo.weight to be bf16, got torch.float32
Expected layers.32.attention.wv.weight to be bf16, got torch.float32


Expected layers.25.ffn_norm.weight to be bf16, got torch.float32
Expected layers.31.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.33.attention.wk.weight to be bf16, got torch.float32Expected layers.27.attention.wo.weight to be bf16, got torch.float32

Expected layers.25.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.27.attention_norm.weight to be bf16, got torch.float32
Expected layers.25.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.32.attention.wo.weight to be bf16, got torch.float32

Expected layers.26.attention.wq.weight to be bf16, got torch.float32
Expected layers.31.attention_norm.weight to be bf16, got torch.float32
Expected layers.33.attention.wv.weight to be bf16, got torch.float32Expected layers.27.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.27.ffn_norm.weight to be bf16, got torch.float32Expected layers.25.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.26.attention.wk.weight to be bf16, got torch.float32Expected layers.32.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.25.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.31.ffn_norm.weight to be bf16, got torch.float32


Expected layers.33.attention.wo.weight to be bf16, got torch.float32Expected layers.27.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.28.attention.wq.weight to be bf16, got torch.float32
Expected layers.25.attention_norm.weight to be bf16, got torch.float32
Expected layers.26.attention.wv.weight to be bf16, got torch.float32
Expected layers.32.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.25.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.32.attention.wq.weight to be bf16, got torch.float32

Expected layers.33.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.27.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.28.attention.wk.weight to be bf16, got torch.float32
Expected layers.25.ffn_norm.weight to be bf16, got torch.float32

Expected layers.32.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.26.attention.wo.weight to be bf16, got torch.float32
Expected layers.25.attention_norm.weight to be bf16, got torch.float32Expected layers.32.attention.wk.weight to be bf16, got torch.float32

Expected layers.33.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.27.attention_norm.weight to be bf16, got torch.float32
Expected layers.28.attention.wv.weight to be bf16, got torch.float32

Expected layers.32.attention_norm.weight to be bf16, got torch.float32Expected layers.26.attention.wq.weight to be bf16, got torch.float32
Expected layers.26.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.25.ffn_norm.weight to be bf16, got torch.float32

Expected layers.32.attention.wv.weight to be bf16, got torch.float32Expected layers.33.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.27.ffn_norm.weight to be bf16, got torch.float32
Expected layers.28.attention.wo.weight to be bf16, got torch.float32


Expected layers.26.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.32.ffn_norm.weight to be bf16, got torch.float32Expected layers.26.attention.wk.weight to be bf16, got torch.float32

Expected layers.26.attention.wq.weight to be bf16, got torch.float32

Expected layers.32.attention.wo.weight to be bf16, got torch.float32Expected layers.33.attention_norm.weight to be bf16, got torch.float32Expected layers.28.attention.wq.weight to be bf16, got torch.float32

Expected layers.28.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.26.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.33.attention.wq.weight to be bf16, got torch.float32Expected layers.26.attention.wv.weight to be bf16, got torch.float32

Expected layers.26.attention.wk.weight to be bf16, got torch.float32Expected layers.32.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.33.ffn_norm.weight to be bf16, got torch.float32Expected layers.28.attention.wk.weight to be bf16, got torch.float32

Expected layers.28.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.26.attention_norm.weight to be bf16, got torch.float32

Expected layers.33.attention.wk.weight to be bf16, got torch.float32
Expected layers.26.attention.wo.weight to be bf16, got torch.float32
Expected layers.26.attention.wv.weight to be bf16, got torch.float32
Expected layers.32.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.34.attention.wq.weight to be bf16, got torch.float32
Expected layers.28.attention.wv.weight to be bf16, got torch.float32
Expected layers.28.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.26.ffn_norm.weight to be bf16, got torch.float32
Expected layers.33.attention.wv.weight to be bf16, got torch.float32
Expected layers.26.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.26.attention.wo.weight to be bf16, got torch.float32
Expected layers.32.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.34.attention.wk.weight to be bf16, got torch.float32

Expected layers.28.attention.wo.weight to be bf16, got torch.float32
Expected layers.28.attention_norm.weight to be bf16, got torch.float32
Expected layers.27.attention.wq.weight to be bf16, got torch.float32
Expected layers.33.attention.wo.weight to be bf16, got torch.float32
Expected layers.26.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.26.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.32.attention_norm.weight to be bf16, got torch.float32
Expected layers.34.attention.wv.weight to be bf16, got torch.float32Expected layers.28.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.28.ffn_norm.weight to be bf16, got torch.float32
Expected layers.27.attention.wk.weight to be bf16, got torch.float32
Expected layers.33.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.26.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.26.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.32.ffn_norm.weight to be bf16, got torch.float32
Expected layers.34.attention.wo.weight to be bf16, got torch.float32
Expected layers.28.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.29.attention.wq.weight to be bf16, got torch.float32
Expected layers.27.attention.wv.weight to be bf16, got torch.float32
Expected layers.33.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.26.attention_norm.weight to be bf16, got torch.float32
Expected layers.26.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.33.attention.wq.weight to be bf16, got torch.float32
Expected layers.34.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.28.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.29.attention.wk.weight to be bf16, got torch.float32
Expected layers.27.attention.wo.weight to be bf16, got torch.float32
Expected layers.33.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.26.ffn_norm.weight to be bf16, got torch.float32
Expected layers.26.attention_norm.weight to be bf16, got torch.float32
Expected layers.33.attention.wk.weight to be bf16, got torch.float32
Expected layers.34.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.28.attention_norm.weight to be bf16, got torch.float32
Expected layers.29.attention.wv.weight to be bf16, got torch.float32
Expected layers.27.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.33.attention_norm.weight to be bf16, got torch.float32
Expected layers.27.attention.wq.weight to be bf16, got torch.float32
Expected layers.26.ffn_norm.weight to be bf16, got torch.float32
Expected layers.33.attention.wv.weight to be bf16, got torch.float32
Expected layers.34.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.28.ffn_norm.weight to be bf16, got torch.float32
Expected layers.29.attention.wo.weight to be bf16, got torch.float32
Expected layers.27.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.33.ffn_norm.weight to be bf16, got torch.float32
Expected layers.27.attention.wk.weight to be bf16, got torch.float32
Expected layers.27.attention.wq.weight to be bf16, got torch.float32
Expected layers.33.attention.wo.weight to be bf16, got torch.float32Expected layers.34.attention_norm.weight to be bf16, got torch.float32

Expected layers.29.attention.wq.weight to be bf16, got torch.float32
Expected layers.29.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.27.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.34.attention.wq.weight to be bf16, got torch.float32

Expected layers.27.attention.wv.weight to be bf16, got torch.float32Expected layers.27.attention.wk.weight to be bf16, got torch.float32
Expected layers.33.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.34.ffn_norm.weight to be bf16, got torch.float32

Expected layers.29.attention.wk.weight to be bf16, got torch.float32Expected layers.29.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.27.attention_norm.weight to be bf16, got torch.float32
Expected layers.34.attention.wk.weight to be bf16, got torch.float32


Expected layers.27.attention.wo.weight to be bf16, got torch.float32Expected layers.27.attention.wv.weight to be bf16, got torch.float32
Expected layers.33.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.35.attention.wq.weight to be bf16, got torch.float32

Expected layers.29.attention.wv.weight to be bf16, got torch.float32
Expected layers.29.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.27.ffn_norm.weight to be bf16, got torch.float32

Expected layers.34.attention.wv.weight to be bf16, got torch.float32

Expected layers.27.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.27.attention.wo.weight to be bf16, got torch.float32
Expected layers.33.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.35.attention.wk.weight to be bf16, got torch.float32

Expected layers.29.attention.wo.weight to be bf16, got torch.float32
Expected layers.29.attention_norm.weight to be bf16, got torch.float32
Expected layers.28.attention.wq.weight to be bf16, got torch.float32
Expected layers.34.attention.wo.weight to be bf16, got torch.float32

Expected layers.27.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.27.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.33.attention_norm.weight to be bf16, got torch.float32Expected layers.35.attention.wv.weight to be bf16, got torch.float32

Expected layers.29.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.29.ffn_norm.weight to be bf16, got torch.float32
Expected layers.28.attention.wk.weight to be bf16, got torch.float32
Expected layers.34.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.27.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.27.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.33.ffn_norm.weight to be bf16, got torch.float32
Expected layers.35.attention.wo.weight to be bf16, got torch.float32
Expected layers.29.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.30.attention.wq.weight to be bf16, got torch.float32
Expected layers.28.attention.wv.weight to be bf16, got torch.float32Expected layers.34.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.27.attention_norm.weight to be bf16, got torch.float32
Expected layers.27.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.34.attention.wq.weight to be bf16, got torch.float32Expected layers.35.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.29.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.30.attention.wk.weight to be bf16, got torch.float32

Expected layers.28.attention.wo.weight to be bf16, got torch.float32
Expected layers.34.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.27.ffn_norm.weight to be bf16, got torch.float32
Expected layers.27.attention_norm.weight to be bf16, got torch.float32
Expected layers.34.attention.wk.weight to be bf16, got torch.float32
Expected layers.35.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.29.attention_norm.weight to be bf16, got torch.float32
Expected layers.30.attention.wv.weight to be bf16, got torch.float32
Expected layers.28.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.34.attention_norm.weight to be bf16, got torch.float32
Expected layers.28.attention.wq.weight to be bf16, got torch.float32
Expected layers.27.ffn_norm.weight to be bf16, got torch.float32
Expected layers.34.attention.wv.weight to be bf16, got torch.float32
Expected layers.35.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.29.ffn_norm.weight to be bf16, got torch.float32
Expected layers.30.attention.wo.weight to be bf16, got torch.float32Expected layers.28.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.34.ffn_norm.weight to be bf16, got torch.float32
Expected layers.28.attention.wk.weight to be bf16, got torch.float32

Expected layers.28.attention.wq.weight to be bf16, got torch.float32
Expected layers.34.attention.wo.weight to be bf16, got torch.float32
Expected layers.35.attention_norm.weight to be bf16, got torch.float32
Expected layers.30.attention.wq.weight to be bf16, got torch.float32
Expected layers.30.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.28.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.35.attention.wq.weight to be bf16, got torch.float32
Expected layers.28.attention.wv.weight to be bf16, got torch.float32
Expected layers.28.attention.wk.weight to be bf16, got torch.float32
Expected layers.34.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.35.ffn_norm.weight to be bf16, got torch.float32
Expected layers.30.attention.wk.weight to be bf16, got torch.float32
Expected layers.30.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.28.attention_norm.weight to be bf16, got torch.float32
Expected layers.35.attention.wk.weight to be bf16, got torch.float32
Expected layers.28.attention.wo.weight to be bf16, got torch.float32
Expected layers.28.attention.wv.weight to be bf16, got torch.float32
Expected layers.34.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.36.attention.wq.weight to be bf16, got torch.float32
Expected layers.30.attention.wv.weight to be bf16, got torch.float32
Expected layers.30.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.28.ffn_norm.weight to be bf16, got torch.float32
Expected layers.35.attention.wv.weight to be bf16, got torch.float32
Expected layers.28.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.28.attention.wo.weight to be bf16, got torch.float32
Expected layers.34.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.36.attention.wk.weight to be bf16, got torch.float32
Expected layers.30.attention.wo.weight to be bf16, got torch.float32
Expected layers.30.attention_norm.weight to be bf16, got torch.float32
Expected layers.29.attention.wq.weight to be bf16, got torch.float32
Expected layers.35.attention.wo.weight to be bf16, got torch.float32
Expected layers.28.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.28.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.34.attention_norm.weight to be bf16, got torch.float32
Expected layers.36.attention.wv.weight to be bf16, got torch.float32
Expected layers.30.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.30.ffn_norm.weight to be bf16, got torch.float32
Expected layers.29.attention.wk.weight to be bf16, got torch.float32
Expected layers.35.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.28.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.28.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.34.ffn_norm.weight to be bf16, got torch.float32
Expected layers.36.attention.wo.weight to be bf16, got torch.float32
Expected layers.30.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.31.attention.wq.weight to be bf16, got torch.float32
Expected layers.29.attention.wv.weight to be bf16, got torch.float32
Expected layers.35.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.28.attention_norm.weight to be bf16, got torch.float32
Expected layers.28.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.35.attention.wq.weight to be bf16, got torch.float32Expected layers.36.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.30.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.31.attention.wk.weight to be bf16, got torch.float32Expected layers.29.attention.wo.weight to be bf16, got torch.float32
Expected layers.35.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.28.ffn_norm.weight to be bf16, got torch.float32Expected layers.28.attention_norm.weight to be bf16, got torch.float32
Expected layers.35.attention.wk.weight to be bf16, got torch.float32
Expected layers.36.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.30.attention_norm.weight to be bf16, got torch.float32
Expected layers.31.attention.wv.weight to be bf16, got torch.float32
Expected layers.29.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.35.attention_norm.weight to be bf16, got torch.float32
Expected layers.29.attention.wq.weight to be bf16, got torch.float32
Expected layers.28.ffn_norm.weight to be bf16, got torch.float32
Expected layers.35.attention.wv.weight to be bf16, got torch.float32Expected layers.36.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.30.ffn_norm.weight to be bf16, got torch.float32

Expected layers.31.attention.wo.weight to be bf16, got torch.float32
Expected layers.29.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.35.ffn_norm.weight to be bf16, got torch.float32
Expected layers.29.attention.wk.weight to be bf16, got torch.float32
Expected layers.29.attention.wq.weight to be bf16, got torch.float32
Expected layers.35.attention.wo.weight to be bf16, got torch.float32Expected layers.36.attention_norm.weight to be bf16, got torch.float32

Expected layers.31.attention.wq.weight to be bf16, got torch.float32
Expected layers.31.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.29.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.36.attention.wq.weight to be bf16, got torch.float32

Expected layers.29.attention.wv.weight to be bf16, got torch.float32
Expected layers.29.attention.wk.weight to be bf16, got torch.float32
Expected layers.35.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.36.ffn_norm.weight to be bf16, got torch.float32

Expected layers.31.attention.wk.weight to be bf16, got torch.float32
Expected layers.31.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.29.attention_norm.weight to be bf16, got torch.float32
Expected layers.36.attention.wk.weight to be bf16, got torch.float32

Expected layers.29.attention.wv.weight to be bf16, got torch.float32
Expected layers.35.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.37.attention.wq.weight to be bf16, got torch.float32
Expected layers.31.attention.wv.weight to be bf16, got torch.float32

Expected layers.31.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.29.attention.wo.weight to be bf16, got torch.float32Expected layers.29.ffn_norm.weight to be bf16, got torch.float32
Expected layers.36.attention.wv.weight to be bf16, got torch.float32

Expected layers.29.attention.wo.weight to be bf16, got torch.float32
Expected layers.35.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.37.attention.wk.weight to be bf16, got torch.float32
Expected layers.31.attention.wo.weight to be bf16, got torch.float32
Expected layers.31.attention_norm.weight to be bf16, got torch.float32
Expected layers.29.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.30.attention.wq.weight to be bf16, got torch.float32
Expected layers.36.attention.wo.weight to be bf16, got torch.float32
Expected layers.29.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.31.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.35.attention_norm.weight to be bf16, got torch.float32
Expected layers.37.attention.wv.weight to be bf16, got torch.float32

Expected layers.31.ffn_norm.weight to be bf16, got torch.float32
Expected layers.29.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.30.attention.wk.weight to be bf16, got torch.float32
Expected layers.36.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.29.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.31.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.35.ffn_norm.weight to be bf16, got torch.float32
Expected layers.37.attention.wo.weight to be bf16, got torch.float32Expected layers.32.attention.wq.weight to be bf16, got torch.float32
Expected layers.29.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.30.attention.wv.weight to be bf16, got torch.float32

Expected layers.36.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.29.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.31.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.37.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.36.attention.wq.weight to be bf16, got torch.float32Expected layers.29.attention_norm.weight to be bf16, got torch.float32

Expected layers.32.attention.wk.weight to be bf16, got torch.float32


Expected layers.30.attention.wo.weight to be bf16, got torch.float32
Expected layers.36.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.29.attention_norm.weight to be bf16, got torch.float32
Expected layers.31.attention_norm.weight to be bf16, got torch.float32
Expected layers.37.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.36.attention.wk.weight to be bf16, got torch.float32Expected layers.29.ffn_norm.weight to be bf16, got torch.float32
Expected layers.32.attention.wv.weight to be bf16, got torch.float32

Expected layers.30.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.36.attention_norm.weight to be bf16, got torch.float32
Expected layers.31.ffn_norm.weight to be bf16, got torch.float32Expected layers.29.ffn_norm.weight to be bf16, got torch.float32

Expected layers.37.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.36.attention.wv.weight to be bf16, got torch.float32
Expected layers.30.attention.wq.weight to be bf16, got torch.float32
Expected layers.32.attention.wo.weight to be bf16, got torch.float32
Expected layers.30.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.36.ffn_norm.weight to be bf16, got torch.float32

Expected layers.32.attention.wq.weight to be bf16, got torch.float32

Expected layers.30.attention.wq.weight to be bf16, got torch.float32Expected layers.37.attention_norm.weight to be bf16, got torch.float32Expected layers.36.attention.wo.weight to be bf16, got torch.float32

Expected layers.30.attention.wk.weight to be bf16, got torch.float32
Expected layers.32.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.30.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.37.attention.wq.weight to be bf16, got torch.float32

Expected layers.32.attention.wk.weight to be bf16, got torch.float32
Expected layers.30.attention.wk.weight to be bf16, got torch.float32
Expected layers.37.ffn_norm.weight to be bf16, got torch.float32Expected layers.36.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.30.attention.wv.weight to be bf16, got torch.float32
Expected layers.32.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.30.attention_norm.weight to be bf16, got torch.float32
Expected layers.37.attention.wk.weight to be bf16, got torch.float32
Expected layers.32.attention.wv.weight to be bf16, got torch.float32
Expected layers.30.attention.wv.weight to be bf16, got torch.float32
Expected layers.38.attention.wq.weight to be bf16, got torch.float32
Expected layers.36.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.30.attention.wo.weight to be bf16, got torch.float32
Expected layers.32.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.30.ffn_norm.weight to be bf16, got torch.float32
Expected layers.37.attention.wv.weight to be bf16, got torch.float32
Expected layers.32.attention.wo.weight to be bf16, got torch.float32
Expected layers.30.attention.wo.weight to be bf16, got torch.float32
Expected layers.38.attention.wk.weight to be bf16, got torch.float32
Expected layers.36.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.30.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.32.attention_norm.weight to be bf16, got torch.float32Expected layers.31.attention.wq.weight to be bf16, got torch.float32
Expected layers.37.attention.wo.weight to be bf16, got torch.float32
Expected layers.32.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.30.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.38.attention.wv.weight to be bf16, got torch.float32
Expected layers.36.attention_norm.weight to be bf16, got torch.float32

Expected layers.30.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.32.ffn_norm.weight to be bf16, got torch.float32
Expected layers.31.attention.wk.weight to be bf16, got torch.float32Expected layers.37.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.32.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.30.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.38.attention.wo.weight to be bf16, got torch.float32
Expected layers.36.ffn_norm.weight to be bf16, got torch.float32
Expected layers.30.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.33.attention.wq.weight to be bf16, got torch.float32
Expected layers.31.attention.wv.weight to be bf16, got torch.float32
Expected layers.37.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.32.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.30.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.38.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.37.attention.wq.weight to be bf16, got torch.float32
Expected layers.30.attention_norm.weight to be bf16, got torch.float32
Expected layers.33.attention.wk.weight to be bf16, got torch.float32
Expected layers.31.attention.wo.weight to be bf16, got torch.float32
Expected layers.37.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.32.attention_norm.weight to be bf16, got torch.float32
Expected layers.30.attention_norm.weight to be bf16, got torch.float32
Expected layers.38.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.37.attention.wk.weight to be bf16, got torch.float32
Expected layers.30.ffn_norm.weight to be bf16, got torch.float32
Expected layers.33.attention.wv.weight to be bf16, got torch.float32
Expected layers.31.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.37.attention_norm.weight to be bf16, got torch.float32
Expected layers.32.ffn_norm.weight to be bf16, got torch.float32
Expected layers.30.ffn_norm.weight to be bf16, got torch.float32
Expected layers.38.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.37.attention.wv.weight to be bf16, got torch.float32
Expected layers.31.attention.wq.weight to be bf16, got torch.float32
Expected layers.33.attention.wo.weight to be bf16, got torch.float32
Expected layers.31.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.37.ffn_norm.weight to be bf16, got torch.float32
Expected layers.33.attention.wq.weight to be bf16, got torch.float32
Expected layers.31.attention.wq.weight to be bf16, got torch.float32
Expected layers.38.attention_norm.weight to be bf16, got torch.float32
Expected layers.37.attention.wo.weight to be bf16, got torch.float32
Expected layers.31.attention.wk.weight to be bf16, got torch.float32
Expected layers.33.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.31.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.38.attention.wq.weight to be bf16, got torch.float32
Expected layers.33.attention.wk.weight to be bf16, got torch.float32
Expected layers.31.attention.wk.weight to be bf16, got torch.float32
Expected layers.38.ffn_norm.weight to be bf16, got torch.float32
Expected layers.37.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.31.attention.wv.weight to be bf16, got torch.float32
Expected layers.33.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.31.attention_norm.weight to be bf16, got torch.float32
Expected layers.38.attention.wk.weight to be bf16, got torch.float32
Expected layers.33.attention.wv.weight to be bf16, got torch.float32
Expected layers.31.attention.wv.weight to be bf16, got torch.float32
Expected layers.39.attention.wq.weight to be bf16, got torch.float32Expected layers.37.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.31.attention.wo.weight to be bf16, got torch.float32
Expected layers.33.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.31.ffn_norm.weight to be bf16, got torch.float32
Expected layers.38.attention.wv.weight to be bf16, got torch.float32
Expected layers.33.attention.wo.weight to be bf16, got torch.float32
Expected layers.31.attention.wo.weight to be bf16, got torch.float32
Expected layers.39.attention.wk.weight to be bf16, got torch.float32
Expected layers.37.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.31.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.33.attention_norm.weight to be bf16, got torch.float32
Expected layers.32.attention.wq.weight to be bf16, got torch.float32
Expected layers.38.attention.wo.weight to be bf16, got torch.float32
Expected layers.33.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.31.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.39.attention.wv.weight to be bf16, got torch.float32
Expected layers.37.attention_norm.weight to be bf16, got torch.float32
Expected layers.31.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.33.ffn_norm.weight to be bf16, got torch.float32
Expected layers.32.attention.wk.weight to be bf16, got torch.float32
Expected layers.38.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.33.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.31.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.39.attention.wo.weight to be bf16, got torch.float32
Expected layers.37.ffn_norm.weight to be bf16, got torch.float32
Expected layers.31.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.34.attention.wq.weight to be bf16, got torch.float32
Expected layers.32.attention.wv.weight to be bf16, got torch.float32
Expected layers.38.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.33.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.31.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.39.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.38.attention.wq.weight to be bf16, got torch.float32
Expected layers.31.attention_norm.weight to be bf16, got torch.float32
Expected layers.34.attention.wk.weight to be bf16, got torch.float32Expected layers.32.attention.wo.weight to be bf16, got torch.float32

Expected layers.38.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.39.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.33.attention_norm.weight to be bf16, got torch.float32
Expected layers.31.attention_norm.weight to be bf16, got torch.float32


Expected layers.38.attention.wk.weight to be bf16, got torch.float32Expected layers.31.ffn_norm.weight to be bf16, got torch.float32
Expected layers.34.attention.wv.weight to be bf16, got torch.float32
Expected layers.32.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.38.attention_norm.weight to be bf16, got torch.float32
Expected layers.39.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.33.ffn_norm.weight to be bf16, got torch.float32

Expected layers.32.attention.wq.weight to be bf16, got torch.float32Expected layers.31.ffn_norm.weight to be bf16, got torch.float32
Expected layers.38.attention.wv.weight to be bf16, got torch.float32

Expected layers.34.attention.wo.weight to be bf16, got torch.float32

Expected layers.32.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.38.ffn_norm.weight to be bf16, got torch.float32
Expected layers.39.attention_norm.weight to be bf16, got torch.float32Expected layers.34.attention.wq.weight to be bf16, got torch.float32
Expected layers.32.attention.wk.weight to be bf16, got torch.float32

Expected layers.32.attention.wq.weight to be bf16, got torch.float32Expected layers.38.attention.wo.weight to be bf16, got torch.float32

Expected layers.34.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.32.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.34.attention.wk.weight to be bf16, got torch.float32Expected layers.39.attention.wq.weight to be bf16, got torch.float32Expected layers.39.ffn_norm.weight to be bf16, got torch.float32

Expected layers.32.attention.wv.weight to be bf16, got torch.float32


Expected layers.32.attention.wk.weight to be bf16, got torch.float32Expected layers.34.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.38.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.32.attention_norm.weight to be bf16, got torch.float32

Expected layers.34.attention.wv.weight to be bf16, got torch.float32Expected layers.39.attention.wk.weight to be bf16, got torch.float32Expected layers.40.attention.wq.weight to be bf16, got torch.float32

Expected layers.32.attention.wo.weight to be bf16, got torch.float32
Expected layers.32.attention.wv.weight to be bf16, got torch.float32
Expected layers.34.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.38.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.32.ffn_norm.weight to be bf16, got torch.float32
Expected layers.34.attention.wo.weight to be bf16, got torch.float32
Expected layers.39.attention.wv.weight to be bf16, got torch.float32
Expected layers.40.attention.wk.weight to be bf16, got torch.float32
Expected layers.32.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.32.attention.wo.weight to be bf16, got torch.float32
Expected layers.34.attention_norm.weight to be bf16, got torch.float32
Expected layers.38.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.33.attention.wq.weight to be bf16, got torch.float32
Expected layers.34.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.39.attention.wo.weight to be bf16, got torch.float32
Expected layers.40.attention.wv.weight to be bf16, got torch.float32
Expected layers.32.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.32.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.34.ffn_norm.weight to be bf16, got torch.float32Expected layers.38.attention_norm.weight to be bf16, got torch.float32Expected layers.33.attention.wk.weight to be bf16, got torch.float32
Expected layers.34.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.39.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.40.attention.wo.weight to be bf16, got torch.float32
Expected layers.32.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.35.attention.wq.weight to be bf16, got torch.float32
Expected layers.32.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.38.ffn_norm.weight to be bf16, got torch.float32
Expected layers.33.attention.wv.weight to be bf16, got torch.float32
Expected layers.34.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.39.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.40.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.32.attention_norm.weight to be bf16, got torch.float32
Expected layers.35.attention.wk.weight to be bf16, got torch.float32
Expected layers.32.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.39.attention.wq.weight to be bf16, got torch.float32
Expected layers.33.attention.wo.weight to be bf16, got torch.float32
Expected layers.34.attention_norm.weight to be bf16, got torch.float32
Expected layers.39.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.40.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.32.ffn_norm.weight to be bf16, got torch.float32
Expected layers.35.attention.wv.weight to be bf16, got torch.float32
Expected layers.32.attention_norm.weight to be bf16, got torch.float32
Expected layers.39.attention.wk.weight to be bf16, got torch.float32Expected layers.33.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.34.ffn_norm.weight to be bf16, got torch.float32
Expected layers.39.attention_norm.weight to be bf16, got torch.float32

Expected layers.40.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.33.attention.wq.weight to be bf16, got torch.float32
Expected layers.35.attention.wo.weight to be bf16, got torch.float32
Expected layers.32.ffn_norm.weight to be bf16, got torch.float32
Expected layers.39.attention.wv.weight to be bf16, got torch.float32Expected layers.33.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.35.attention.wq.weight to be bf16, got torch.float32

Expected layers.39.ffn_norm.weight to be bf16, got torch.float32
Expected layers.40.attention_norm.weight to be bf16, got torch.float32
Expected layers.33.attention.wk.weight to be bf16, got torch.float32
Expected layers.35.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.33.attention.wq.weight to be bf16, got torch.float32
Expected layers.39.attention.wo.weight to be bf16, got torch.float32
Expected layers.33.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.35.attention.wk.weight to be bf16, got torch.float32
Expected layers.40.attention.wq.weight to be bf16, got torch.float32
Expected layers.40.ffn_norm.weight to be bf16, got torch.float32
Expected layers.33.attention.wv.weight to be bf16, got torch.float32
Expected layers.35.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.33.attention.wk.weight to be bf16, got torch.float32
Expected layers.39.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.33.attention_norm.weight to be bf16, got torch.float32
Expected layers.35.attention.wv.weight to be bf16, got torch.float32Expected layers.40.attention.wk.weight to be bf16, got torch.float32

Expected layers.41.attention.wq.weight to be bf16, got torch.float32

Expected layers.33.attention.wo.weight to be bf16, got torch.float32Expected layers.35.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.33.attention.wv.weight to be bf16, got torch.float32
Expected layers.39.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.33.ffn_norm.weight to be bf16, got torch.float32

Expected layers.35.attention.wo.weight to be bf16, got torch.float32Expected layers.40.attention.wv.weight to be bf16, got torch.float32

Expected layers.41.attention.wk.weight to be bf16, got torch.float32Expected layers.33.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.35.attention_norm.weight to be bf16, got torch.float32
Expected layers.33.attention.wo.weight to be bf16, got torch.float32
Expected layers.39.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.34.attention.wq.weight to be bf16, got torch.float32

Expected layers.35.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.40.attention.wo.weight to be bf16, got torch.float32
Expected layers.41.attention.wv.weight to be bf16, got torch.float32
Expected layers.33.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.35.ffn_norm.weight to be bf16, got torch.float32Expected layers.33.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.39.attention_norm.weight to be bf16, got torch.float32Expected layers.34.attention.wk.weight to be bf16, got torch.float32

Expected layers.35.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.40.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.41.attention.wo.weight to be bf16, got torch.float32
Expected layers.33.feed_forward.w3.weight to be bf16, got torch.float32


Expected layers.36.attention.wq.weight to be bf16, got torch.float32Expected layers.33.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.39.ffn_norm.weight to be bf16, got torch.float32

Expected layers.34.attention.wv.weight to be bf16, got torch.float32
Expected layers.35.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.40.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.41.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.33.attention_norm.weight to be bf16, got torch.float32
Expected layers.36.attention.wk.weight to be bf16, got torch.float32
Expected layers.33.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.40.attention.wq.weight to be bf16, got torch.float32

Expected layers.34.attention.wo.weight to be bf16, got torch.float32

Expected layers.35.attention_norm.weight to be bf16, got torch.float32Expected layers.33.ffn_norm.weight to be bf16, got torch.float32Expected layers.40.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.41.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.36.attention.wv.weight to be bf16, got torch.float32
Expected layers.40.attention.wk.weight to be bf16, got torch.float32

Expected layers.34.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.35.ffn_norm.weight to be bf16, got torch.float32
Expected layers.34.attention.wq.weight to be bf16, got torch.float32Expected layers.33.attention_norm.weight to be bf16, got torch.float32Expected layers.40.attention_norm.weight to be bf16, got torch.float32
Expected layers.41.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.36.attention.wo.weight to be bf16, got torch.float32

Expected layers.40.attention.wv.weight to be bf16, got torch.float32Expected layers.34.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.36.attention.wq.weight to be bf16, got torch.float32

Expected layers.34.attention.wk.weight to be bf16, got torch.float32Expected layers.33.ffn_norm.weight to be bf16, got torch.float32
Expected layers.40.ffn_norm.weight to be bf16, got torch.float32
Expected layers.41.attention_norm.weight to be bf16, got torch.float32
Expected layers.36.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.40.attention.wo.weight to be bf16, got torch.float32Expected layers.34.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.36.attention.wk.weight to be bf16, got torch.float32
Expected layers.34.attention.wv.weight to be bf16, got torch.float32
Expected layers.34.attention.wq.weight to be bf16, got torch.float32
Expected layers.41.attention.wq.weight to be bf16, got torch.float32
Expected layers.41.ffn_norm.weight to be bf16, got torch.float32
Expected layers.36.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.40.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.34.attention_norm.weight to be bf16, got torch.float32
Expected layers.36.attention.wv.weight to be bf16, got torch.float32

Expected layers.34.attention.wo.weight to be bf16, got torch.float32
Expected layers.34.attention.wk.weight to be bf16, got torch.float32
Expected layers.41.attention.wk.weight to be bf16, got torch.float32
Expected layers.42.attention.wq.weight to be bf16, got torch.float32
Expected layers.36.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.40.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.34.ffn_norm.weight to be bf16, got torch.float32
Expected layers.36.attention.wo.weight to be bf16, got torch.float32
Expected layers.34.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.34.attention.wv.weight to be bf16, got torch.float32
Expected layers.41.attention.wv.weight to be bf16, got torch.float32Expected layers.42.attention.wk.weight to be bf16, got torch.float32

Expected layers.36.attention_norm.weight to be bf16, got torch.float32
Expected layers.40.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.35.attention.wq.weight to be bf16, got torch.float32
Expected layers.36.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.34.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.34.attention.wo.weight to be bf16, got torch.float32
Expected layers.41.attention.wo.weight to be bf16, got torch.float32Expected layers.42.attention.wv.weight to be bf16, got torch.float32
Expected layers.36.ffn_norm.weight to be bf16, got torch.float32
Expected layers.40.attention_norm.weight to be bf16, got torch.float32
Expected layers.35.attention.wk.weight to be bf16, got torch.float32
Expected layers.36.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.34.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.34.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.42.attention.wo.weight to be bf16, got torch.float32
Expected layers.41.feed_forward.w1.weight to be bf16, got torch.float32


Expected layers.37.attention.wq.weight to be bf16, got torch.float32
Expected layers.40.ffn_norm.weight to be bf16, got torch.float32Expected layers.35.attention.wv.weight to be bf16, got torch.float32
Expected layers.36.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.34.attention_norm.weight to be bf16, got torch.float32Expected layers.34.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.42.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.41.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.37.attention.wk.weight to be bf16, got torch.float32
Expected layers.41.attention.wq.weight to be bf16, got torch.float32
Expected layers.35.attention.wo.weight to be bf16, got torch.float32

Expected layers.36.attention_norm.weight to be bf16, got torch.float32
Expected layers.34.ffn_norm.weight to be bf16, got torch.float32Expected layers.34.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.42.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.41.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.37.attention.wv.weight to be bf16, got torch.float32Expected layers.41.attention.wk.weight to be bf16, got torch.float32
Expected layers.35.feed_forward.w1.weight to be bf16, got torch.float32


Expected layers.36.ffn_norm.weight to be bf16, got torch.float32
Expected layers.35.attention.wq.weight to be bf16, got torch.float32Expected layers.34.attention_norm.weight to be bf16, got torch.float32
Expected layers.42.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.41.attention_norm.weight to be bf16, got torch.float32
Expected layers.37.attention.wo.weight to be bf16, got torch.float32Expected layers.41.attention.wv.weight to be bf16, got torch.float32
Expected layers.35.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.37.attention.wq.weight to be bf16, got torch.float32
Expected layers.35.attention.wk.weight to be bf16, got torch.float32
Expected layers.34.ffn_norm.weight to be bf16, got torch.float32
Expected layers.42.attention_norm.weight to be bf16, got torch.float32
Expected layers.41.ffn_norm.weight to be bf16, got torch.float32
Expected layers.37.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.41.attention.wo.weight to be bf16, got torch.float32
Expected layers.35.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.37.attention.wk.weight to be bf16, got torch.float32
Expected layers.35.attention.wv.weight to be bf16, got torch.float32
Expected layers.35.attention.wq.weight to be bf16, got torch.float32Expected layers.42.ffn_norm.weight to be bf16, got torch.float32
Expected layers.42.attention.wq.weight to be bf16, got torch.float32
Expected layers.37.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.41.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.35.attention_norm.weight to be bf16, got torch.float32

Expected layers.37.attention.wv.weight to be bf16, got torch.float32Expected layers.35.attention.wo.weight to be bf16, got torch.float32
Expected layers.35.attention.wk.weight to be bf16, got torch.float32
Expected layers.43.attention.wq.weight to be bf16, got torch.float32
Expected layers.42.attention.wk.weight to be bf16, got torch.float32
Expected layers.37.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.41.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.35.ffn_norm.weight to be bf16, got torch.float32
Expected layers.37.attention.wo.weight to be bf16, got torch.float32
Expected layers.35.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.35.attention.wv.weight to be bf16, got torch.float32

Expected layers.43.attention.wk.weight to be bf16, got torch.float32Expected layers.42.attention.wv.weight to be bf16, got torch.float32

Expected layers.37.attention_norm.weight to be bf16, got torch.float32
Expected layers.37.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.41.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.36.attention.wq.weight to be bf16, got torch.float32


Expected layers.35.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.35.attention.wo.weight to be bf16, got torch.float32
Expected layers.43.attention.wv.weight to be bf16, got torch.float32Expected layers.42.attention.wo.weight to be bf16, got torch.float32

Expected layers.37.ffn_norm.weight to be bf16, got torch.float32
Expected layers.37.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.41.attention_norm.weight to be bf16, got torch.float32
Expected layers.36.attention.wk.weight to be bf16, got torch.float32
Expected layers.43.attention.wo.weight to be bf16, got torch.float32
Expected layers.35.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.35.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.42.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.38.attention.wq.weight to be bf16, got torch.float32Expected layers.37.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.41.ffn_norm.weight to be bf16, got torch.float32
Expected layers.43.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.36.attention.wv.weight to be bf16, got torch.float32

Expected layers.35.attention_norm.weight to be bf16, got torch.float32
Expected layers.35.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.42.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.38.attention.wk.weight to be bf16, got torch.float32
Expected layers.37.attention_norm.weight to be bf16, got torch.float32Expected layers.42.attention.wq.weight to be bf16, got torch.float32
Expected layers.43.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.36.attention.wo.weight to be bf16, got torch.float32

Expected layers.35.ffn_norm.weight to be bf16, got torch.float32Expected layers.42.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.35.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.38.attention.wv.weight to be bf16, got torch.float32Expected layers.37.ffn_norm.weight to be bf16, got torch.float32
Expected layers.42.attention.wk.weight to be bf16, got torch.float32
Expected layers.43.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.36.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.36.attention.wq.weight to be bf16, got torch.float32
Expected layers.42.attention_norm.weight to be bf16, got torch.float32
Expected layers.35.attention_norm.weight to be bf16, got torch.float32Expected layers.38.attention.wo.weight to be bf16, got torch.float32
Expected layers.38.attention.wq.weight to be bf16, got torch.float32

Expected layers.42.attention.wv.weight to be bf16, got torch.float32
Expected layers.43.attention_norm.weight to be bf16, got torch.float32
Expected layers.36.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.36.attention.wk.weight to be bf16, got torch.float32
Expected layers.42.ffn_norm.weight to be bf16, got torch.float32
Expected layers.35.ffn_norm.weight to be bf16, got torch.float32

Expected layers.38.attention.wk.weight to be bf16, got torch.float32Expected layers.38.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.42.attention.wo.weight to be bf16, got torch.float32
Expected layers.43.ffn_norm.weight to be bf16, got torch.float32
Expected layers.36.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.36.attention.wv.weight to be bf16, got torch.float32Expected layers.43.attention.wq.weight to be bf16, got torch.float32

Expected layers.36.attention.wq.weight to be bf16, got torch.float32
Expected layers.38.attention.wv.weight to be bf16, got torch.float32

Expected layers.42.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.38.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.44.attention.wq.weight to be bf16, got torch.float32Expected layers.36.attention_norm.weight to be bf16, got torch.float32
Expected layers.36.attention.wo.weight to be bf16, got torch.float32
Expected layers.43.attention.wk.weight to be bf16, got torch.float32
Expected layers.36.attention.wk.weight to be bf16, got torch.float32
Expected layers.38.attention.wo.weight to be bf16, got torch.float32


Expected layers.42.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.38.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.44.attention.wk.weight to be bf16, got torch.float32
Expected layers.36.ffn_norm.weight to be bf16, got torch.float32Expected layers.36.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.43.attention.wv.weight to be bf16, got torch.float32Expected layers.36.attention.wv.weight to be bf16, got torch.float32

Expected layers.38.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.42.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.38.attention_norm.weight to be bf16, got torch.float32Expected layers.44.attention.wv.weight to be bf16, got torch.float32
Expected layers.37.attention.wq.weight to be bf16, got torch.float32Expected layers.36.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.43.attention.wo.weight to be bf16, got torch.float32
Expected layers.36.attention.wo.weight to be bf16, got torch.float32
Expected layers.38.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.42.attention_norm.weight to be bf16, got torch.float32

Expected layers.38.ffn_norm.weight to be bf16, got torch.float32Expected layers.44.attention.wo.weight to be bf16, got torch.float32
Expected layers.37.attention.wk.weight to be bf16, got torch.float32Expected layers.36.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.43.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.36.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.38.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.42.ffn_norm.weight to be bf16, got torch.float32

Expected layers.39.attention.wq.weight to be bf16, got torch.float32Expected layers.44.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.37.attention.wv.weight to be bf16, got torch.float32Expected layers.36.attention_norm.weight to be bf16, got torch.float32

Expected layers.43.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.36.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.38.attention_norm.weight to be bf16, got torch.float32
Expected layers.43.attention.wq.weight to be bf16, got torch.float32

Expected layers.39.attention.wk.weight to be bf16, got torch.float32Expected layers.44.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.37.attention.wo.weight to be bf16, got torch.float32
Expected layers.36.ffn_norm.weight to be bf16, got torch.float32
Expected layers.43.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.36.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.38.ffn_norm.weight to be bf16, got torch.float32
Expected layers.43.attention.wk.weight to be bf16, got torch.float32
Expected layers.39.attention.wv.weight to be bf16, got torch.float32

Expected layers.44.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.37.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.37.attention.wq.weight to be bf16, got torch.float32Expected layers.43.attention_norm.weight to be bf16, got torch.float32
Expected layers.36.attention_norm.weight to be bf16, got torch.float32Expected layers.39.attention.wq.weight to be bf16, got torch.float32
Expected layers.43.attention.wv.weight to be bf16, got torch.float32
Expected layers.39.attention.wo.weight to be bf16, got torch.float32Expected layers.44.attention_norm.weight to be bf16, got torch.float32


Expected layers.37.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.37.attention.wk.weight to be bf16, got torch.float32Expected layers.43.ffn_norm.weight to be bf16, got torch.float32
Expected layers.36.ffn_norm.weight to be bf16, got torch.float32Expected layers.43.attention.wo.weight to be bf16, got torch.float32Expected layers.39.attention.wk.weight to be bf16, got torch.float32

Expected layers.39.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.44.ffn_norm.weight to be bf16, got torch.float32
Expected layers.37.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.37.attention.wv.weight to be bf16, got torch.float32Expected layers.44.attention.wq.weight to be bf16, got torch.float32
Expected layers.37.attention.wq.weight to be bf16, got torch.float32Expected layers.43.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.39.attention.wv.weight to be bf16, got torch.float32

Expected layers.39.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.45.attention.wq.weight to be bf16, got torch.float32
Expected layers.37.attention_norm.weight to be bf16, got torch.float32
Expected layers.37.attention.wo.weight to be bf16, got torch.float32
Expected layers.44.attention.wk.weight to be bf16, got torch.float32
Expected layers.37.attention.wk.weight to be bf16, got torch.float32
Expected layers.43.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.39.attention.wo.weight to be bf16, got torch.float32
Expected layers.39.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.45.attention.wk.weight to be bf16, got torch.float32Expected layers.37.ffn_norm.weight to be bf16, got torch.float32


Expected layers.37.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.44.attention.wv.weight to be bf16, got torch.float32
Expected layers.37.attention.wv.weight to be bf16, got torch.float32Expected layers.43.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.39.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.39.attention_norm.weight to be bf16, got torch.float32Expected layers.45.attention.wv.weight to be bf16, got torch.float32
Expected layers.38.attention.wq.weight to be bf16, got torch.float32


Expected layers.37.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.44.attention.wo.weight to be bf16, got torch.float32
Expected layers.37.attention.wo.weight to be bf16, got torch.float32
Expected layers.43.attention_norm.weight to be bf16, got torch.float32
Expected layers.39.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.39.ffn_norm.weight to be bf16, got torch.float32Expected layers.45.attention.wo.weight to be bf16, got torch.float32

Expected layers.38.attention.wk.weight to be bf16, got torch.float32
Expected layers.37.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.44.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.37.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.43.ffn_norm.weight to be bf16, got torch.float32
Expected layers.39.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.40.attention.wq.weight to be bf16, got torch.float32
Expected layers.45.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.38.attention.wv.weight to be bf16, got torch.float32
Expected layers.37.attention_norm.weight to be bf16, got torch.float32
Expected layers.44.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.37.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.44.attention.wq.weight to be bf16, got torch.float32
Expected layers.39.attention_norm.weight to be bf16, got torch.float32
Expected layers.40.attention.wk.weight to be bf16, got torch.float32
Expected layers.45.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.38.attention.wo.weight to be bf16, got torch.float32
Expected layers.37.ffn_norm.weight to be bf16, got torch.float32Expected layers.44.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.37.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.44.attention.wk.weight to be bf16, got torch.float32

Expected layers.39.ffn_norm.weight to be bf16, got torch.float32
Expected layers.40.attention.wv.weight to be bf16, got torch.float32
Expected layers.45.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.38.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.38.attention.wq.weight to be bf16, got torch.float32
Expected layers.44.attention_norm.weight to be bf16, got torch.float32
Expected layers.37.attention_norm.weight to be bf16, got torch.float32
Expected layers.44.attention.wv.weight to be bf16, got torch.float32

Expected layers.40.attention.wq.weight to be bf16, got torch.float32
Expected layers.40.attention.wo.weight to be bf16, got torch.float32
Expected layers.45.attention_norm.weight to be bf16, got torch.float32
Expected layers.38.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.38.attention.wk.weight to be bf16, got torch.float32
Expected layers.44.ffn_norm.weight to be bf16, got torch.float32Expected layers.37.ffn_norm.weight to be bf16, got torch.float32
Expected layers.44.attention.wo.weight to be bf16, got torch.float32
Expected layers.45.ffn_norm.weight to be bf16, got torch.float32
Expected layers.40.attention.wk.weight to be bf16, got torch.float32
Expected layers.40.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.38.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.38.attention.wv.weight to be bf16, got torch.float32
Expected layers.45.attention.wq.weight to be bf16, got torch.float32
Expected layers.38.attention.wq.weight to be bf16, got torch.float32Expected layers.44.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.46.attention.wq.weight to be bf16, got torch.float32
Expected layers.40.attention.wv.weight to be bf16, got torch.float32
Expected layers.40.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.38.attention_norm.weight to be bf16, got torch.float32

Expected layers.38.attention.wo.weight to be bf16, got torch.float32
Expected layers.45.attention.wk.weight to be bf16, got torch.float32
Expected layers.38.attention.wk.weight to be bf16, got torch.float32Expected layers.44.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.46.attention.wk.weight to be bf16, got torch.float32
Expected layers.40.attention.wo.weight to be bf16, got torch.float32
Expected layers.40.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.38.ffn_norm.weight to be bf16, got torch.float32

Expected layers.38.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.45.attention.wv.weight to be bf16, got torch.float32
Expected layers.38.attention.wv.weight to be bf16, got torch.float32Expected layers.44.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.46.attention.wv.weight to be bf16, got torch.float32
Expected layers.40.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.40.attention_norm.weight to be bf16, got torch.float32
Expected layers.39.attention.wq.weight to be bf16, got torch.float32

Expected layers.38.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.45.attention.wo.weight to be bf16, got torch.float32
Expected layers.38.attention.wo.weight to be bf16, got torch.float32Expected layers.44.attention_norm.weight to be bf16, got torch.float32
Expected layers.46.attention.wo.weight to be bf16, got torch.float32
Expected layers.40.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.38.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.40.ffn_norm.weight to be bf16, got torch.float32
Expected layers.39.attention.wk.weight to be bf16, got torch.float32

Expected layers.45.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.38.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.44.ffn_norm.weight to be bf16, got torch.float32
Expected layers.46.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.40.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.38.attention_norm.weight to be bf16, got torch.float32
Expected layers.41.attention.wq.weight to be bf16, got torch.float32
Expected layers.39.attention.wv.weight to be bf16, got torch.float32
Expected layers.45.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.38.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.45.attention.wq.weight to be bf16, got torch.float32
Expected layers.46.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.40.attention_norm.weight to be bf16, got torch.float32
Expected layers.41.attention.wk.weight to be bf16, got torch.float32Expected layers.38.ffn_norm.weight to be bf16, got torch.float32


Expected layers.45.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.39.attention.wo.weight to be bf16, got torch.float32

Expected layers.38.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.45.attention.wk.weight to be bf16, got torch.float32Expected layers.46.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.40.ffn_norm.weight to be bf16, got torch.float32
Expected layers.41.attention.wv.weight to be bf16, got torch.float32

Expected layers.39.attention.wq.weight to be bf16, got torch.float32Expected layers.45.attention_norm.weight to be bf16, got torch.float32

Expected layers.39.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.38.attention_norm.weight to be bf16, got torch.float32

Expected layers.45.attention.wv.weight to be bf16, got torch.float32Expected layers.46.attention_norm.weight to be bf16, got torch.float32

Expected layers.41.attention.wq.weight to be bf16, got torch.float32Expected layers.41.attention.wo.weight to be bf16, got torch.float32

Expected layers.39.attention.wk.weight to be bf16, got torch.float32Expected layers.45.ffn_norm.weight to be bf16, got torch.float32

Expected layers.39.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.38.ffn_norm.weight to be bf16, got torch.float32

Expected layers.45.attention.wo.weight to be bf16, got torch.float32Expected layers.46.ffn_norm.weight to be bf16, got torch.float32

Expected layers.41.attention.wk.weight to be bf16, got torch.float32Expected layers.41.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.39.attention.wv.weight to be bf16, got torch.float32Expected layers.46.attention.wq.weight to be bf16, got torch.float32

Expected layers.39.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.39.attention.wq.weight to be bf16, got torch.float32Expected layers.45.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.47.attention.wq.weight to be bf16, got torch.float32
Expected layers.41.attention.wv.weight to be bf16, got torch.float32Expected layers.41.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.39.attention.wo.weight to be bf16, got torch.float32
Expected layers.46.attention.wk.weight to be bf16, got torch.float32

Expected layers.39.attention_norm.weight to be bf16, got torch.float32

Expected layers.39.attention.wk.weight to be bf16, got torch.float32Expected layers.45.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.47.attention.wk.weight to be bf16, got torch.float32
Expected layers.41.attention.wo.weight to be bf16, got torch.float32
Expected layers.41.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.39.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.46.attention.wv.weight to be bf16, got torch.float32

Expected layers.39.ffn_norm.weight to be bf16, got torch.float32
Expected layers.39.attention.wv.weight to be bf16, got torch.float32
Expected layers.45.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.47.attention.wv.weight to be bf16, got torch.float32

Expected layers.41.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.41.attention_norm.weight to be bf16, got torch.float32

Expected layers.39.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.46.attention.wo.weight to be bf16, got torch.float32

Expected layers.40.attention.wq.weight to be bf16, got torch.float32Expected layers.39.attention.wo.weight to be bf16, got torch.float32
Expected layers.45.attention_norm.weight to be bf16, got torch.float32Expected layers.47.attention.wo.weight to be bf16, got torch.float32

Expected layers.41.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.41.ffn_norm.weight to be bf16, got torch.float32Expected layers.39.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.46.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.40.attention.wk.weight to be bf16, got torch.float32
Expected layers.39.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.45.ffn_norm.weight to be bf16, got torch.float32Expected layers.47.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.41.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.42.attention.wq.weight to be bf16, got torch.float32
Expected layers.39.attention_norm.weight to be bf16, got torch.float32
Expected layers.39.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.46.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.40.attention.wv.weight to be bf16, got torch.float32

Expected layers.46.attention.wq.weight to be bf16, got torch.float32
Expected layers.47.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.41.attention_norm.weight to be bf16, got torch.float32

Expected layers.42.attention.wk.weight to be bf16, got torch.float32
Expected layers.39.ffn_norm.weight to be bf16, got torch.float32Expected layers.39.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.46.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.40.attention.wo.weight to be bf16, got torch.float32

Expected layers.46.attention.wk.weight to be bf16, got torch.float32Expected layers.47.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.41.ffn_norm.weight to be bf16, got torch.float32

Expected layers.42.attention.wv.weight to be bf16, got torch.float32
Expected layers.40.attention.wq.weight to be bf16, got torch.float32Expected layers.39.attention_norm.weight to be bf16, got torch.float32
Expected layers.46.attention_norm.weight to be bf16, got torch.float32
Expected layers.40.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.46.attention.wv.weight to be bf16, got torch.float32Expected layers.47.attention_norm.weight to be bf16, got torch.float32
Expected layers.42.attention.wq.weight to be bf16, got torch.float32
Expected layers.42.attention.wo.weight to be bf16, got torch.float32

Expected layers.40.attention.wk.weight to be bf16, got torch.float32
Expected layers.39.ffn_norm.weight to be bf16, got torch.float32
Expected layers.46.ffn_norm.weight to be bf16, got torch.float32
Expected layers.40.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.46.attention.wo.weight to be bf16, got torch.float32
Expected layers.47.ffn_norm.weight to be bf16, got torch.float32
Expected layers.42.attention.wk.weight to be bf16, got torch.float32
Expected layers.42.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.40.attention.wv.weight to be bf16, got torch.float32
Expected layers.40.attention.wq.weight to be bf16, got torch.float32
Expected layers.47.attention.wq.weight to be bf16, got torch.float32
Expected layers.40.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.42.attention.wv.weight to be bf16, got torch.float32Expected layers.46.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.48.attention.wq.weight to be bf16, got torch.float32

Expected layers.42.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.40.attention.wo.weight to be bf16, got torch.float32
Expected layers.40.attention.wk.weight to be bf16, got torch.float32
Expected layers.47.attention.wk.weight to be bf16, got torch.float32
Expected layers.40.attention_norm.weight to be bf16, got torch.float32Expected layers.42.attention.wo.weight to be bf16, got torch.float32

Expected layers.46.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.48.attention.wk.weight to be bf16, got torch.float32
Expected layers.42.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.40.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.40.attention.wv.weight to be bf16, got torch.float32
Expected layers.47.attention.wv.weight to be bf16, got torch.float32Expected layers.40.ffn_norm.weight to be bf16, got torch.float32

Expected layers.42.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.46.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.48.attention.wv.weight to be bf16, got torch.float32
Expected layers.42.attention_norm.weight to be bf16, got torch.float32
Expected layers.40.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.40.attention.wo.weight to be bf16, got torch.float32
Expected layers.47.attention.wo.weight to be bf16, got torch.float32
Expected layers.41.attention.wq.weight to be bf16, got torch.float32
Expected layers.42.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.46.attention_norm.weight to be bf16, got torch.float32
Expected layers.48.attention.wo.weight to be bf16, got torch.float32
Expected layers.42.ffn_norm.weight to be bf16, got torch.float32
Expected layers.40.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.40.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.47.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.41.attention.wk.weight to be bf16, got torch.float32
Expected layers.42.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.46.ffn_norm.weight to be bf16, got torch.float32
Expected layers.48.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.43.attention.wq.weight to be bf16, got torch.float32
Expected layers.40.attention_norm.weight to be bf16, got torch.float32
Expected layers.40.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.47.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.41.attention.wv.weight to be bf16, got torch.float32
Expected layers.42.attention_norm.weight to be bf16, got torch.float32
Expected layers.47.attention.wq.weight to be bf16, got torch.float32
Expected layers.48.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.43.attention.wk.weight to be bf16, got torch.float32
Expected layers.40.ffn_norm.weight to be bf16, got torch.float32
Expected layers.40.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.47.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.41.attention.wo.weight to be bf16, got torch.float32
Expected layers.42.ffn_norm.weight to be bf16, got torch.float32
Expected layers.47.attention.wk.weight to be bf16, got torch.float32
Expected layers.48.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.43.attention.wv.weight to be bf16, got torch.float32
Expected layers.41.attention.wq.weight to be bf16, got torch.float32
Expected layers.40.attention_norm.weight to be bf16, got torch.float32
Expected layers.47.attention_norm.weight to be bf16, got torch.float32
Expected layers.41.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.43.attention.wq.weight to be bf16, got torch.float32
Expected layers.47.attention.wv.weight to be bf16, got torch.float32
Expected layers.48.attention_norm.weight to be bf16, got torch.float32
Expected layers.43.attention.wo.weight to be bf16, got torch.float32
Expected layers.41.attention.wk.weight to be bf16, got torch.float32
Expected layers.40.ffn_norm.weight to be bf16, got torch.float32Expected layers.47.ffn_norm.weight to be bf16, got torch.float32
Expected layers.41.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.43.attention.wk.weight to be bf16, got torch.float32

Expected layers.47.attention.wo.weight to be bf16, got torch.float32
Expected layers.48.ffn_norm.weight to be bf16, got torch.float32
Expected layers.43.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.41.attention.wv.weight to be bf16, got torch.float32
Expected layers.41.attention.wq.weight to be bf16, got torch.float32
Expected layers.48.attention.wq.weight to be bf16, got torch.float32
Expected layers.41.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.43.attention.wv.weight to be bf16, got torch.float32
Expected layers.47.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.49.attention.wq.weight to be bf16, got torch.float32
Expected layers.43.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.41.attention.wo.weight to be bf16, got torch.float32
Expected layers.41.attention.wk.weight to be bf16, got torch.float32
Expected layers.48.attention.wk.weight to be bf16, got torch.float32
Expected layers.41.attention_norm.weight to be bf16, got torch.float32
Expected layers.43.attention.wo.weight to be bf16, got torch.float32
Expected layers.47.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.49.attention.wk.weight to be bf16, got torch.float32
Expected layers.43.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.41.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.41.attention.wv.weight to be bf16, got torch.float32
Expected layers.48.attention.wv.weight to be bf16, got torch.float32Expected layers.41.ffn_norm.weight to be bf16, got torch.float32
Expected layers.43.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.47.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.43.attention_norm.weight to be bf16, got torch.float32

Expected layers.49.attention.wv.weight to be bf16, got torch.float32


Expected layers.41.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.41.attention.wo.weight to be bf16, got torch.float32Expected layers.43.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.48.attention.wo.weight to be bf16, got torch.float32Expected layers.42.attention.wq.weight to be bf16, got torch.float32


Expected layers.47.attention_norm.weight to be bf16, got torch.float32
Expected layers.43.ffn_norm.weight to be bf16, got torch.float32
Expected layers.49.attention.wo.weight to be bf16, got torch.float32

Expected layers.41.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.41.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.43.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.48.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.42.attention.wk.weight to be bf16, got torch.float32
Expected layers.47.ffn_norm.weight to be bf16, got torch.float32
Expected layers.44.attention.wq.weight to be bf16, got torch.float32
Expected layers.49.feed_forward.w1.weight to be bf16, got torch.float32


Expected layers.41.attention_norm.weight to be bf16, got torch.float32
Expected layers.41.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.43.attention_norm.weight to be bf16, got torch.float32
Expected layers.48.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.42.attention.wv.weight to be bf16, got torch.float32
Expected layers.48.attention.wq.weight to be bf16, got torch.float32
Expected layers.44.attention.wk.weight to be bf16, got torch.float32
Expected layers.49.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.41.ffn_norm.weight to be bf16, got torch.float32

Expected layers.41.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.43.ffn_norm.weight to be bf16, got torch.float32Expected layers.48.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.42.attention.wo.weight to be bf16, got torch.float32Expected layers.48.attention.wk.weight to be bf16, got torch.float32
Expected layers.44.attention.wv.weight to be bf16, got torch.float32Expected layers.49.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.42.attention.wq.weight to be bf16, got torch.float32



Expected layers.41.attention_norm.weight to be bf16, got torch.float32
Expected layers.44.attention.wq.weight to be bf16, got torch.float32Expected layers.48.attention_norm.weight to be bf16, got torch.float32
Expected layers.42.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.48.attention.wv.weight to be bf16, got torch.float32
Expected layers.44.attention.wo.weight to be bf16, got torch.float32
Expected layers.49.attention_norm.weight to be bf16, got torch.float32Expected layers.42.attention.wk.weight to be bf16, got torch.float32

Expected layers.41.ffn_norm.weight to be bf16, got torch.float32
Expected layers.44.attention.wk.weight to be bf16, got torch.float32
Expected layers.48.ffn_norm.weight to be bf16, got torch.float32Expected layers.42.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.48.attention.wo.weight to be bf16, got torch.float32Expected layers.44.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.42.attention.wv.weight to be bf16, got torch.float32

Expected layers.49.ffn_norm.weight to be bf16, got torch.float32


Expected layers.42.attention.wq.weight to be bf16, got torch.float32Expected layers.44.attention.wv.weight to be bf16, got torch.float32
Expected layers.49.attention.wq.weight to be bf16, got torch.float32
Expected layers.42.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.48.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.44.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.42.attention.wo.weight to be bf16, got torch.float32

Expected layers.50.attention.wq.weight to be bf16, got torch.float32

Expected layers.42.attention.wk.weight to be bf16, got torch.float32Expected layers.44.attention.wo.weight to be bf16, got torch.float32
Expected layers.49.attention.wk.weight to be bf16, got torch.float32
Expected layers.42.attention_norm.weight to be bf16, got torch.float32
Expected layers.48.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.44.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.50.attention.wk.weight to be bf16, got torch.float32Expected layers.42.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.42.attention.wv.weight to be bf16, got torch.float32
Expected layers.44.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.49.attention.wv.weight to be bf16, got torch.float32
Expected layers.42.ffn_norm.weight to be bf16, got torch.float32Expected layers.48.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.44.attention_norm.weight to be bf16, got torch.float32
Expected layers.50.attention.wv.weight to be bf16, got torch.float32
Expected layers.42.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.42.attention.wo.weight to be bf16, got torch.float32
Expected layers.44.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.49.attention.wo.weight to be bf16, got torch.float32

Expected layers.43.attention.wq.weight to be bf16, got torch.float32Expected layers.48.attention_norm.weight to be bf16, got torch.float32

Expected layers.44.ffn_norm.weight to be bf16, got torch.float32Expected layers.50.attention.wo.weight to be bf16, got torch.float32

Expected layers.42.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.42.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.44.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.49.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.43.attention.wk.weight to be bf16, got torch.float32
Expected layers.48.ffn_norm.weight to be bf16, got torch.float32
Expected layers.45.attention.wq.weight to be bf16, got torch.float32
Expected layers.50.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.42.attention_norm.weight to be bf16, got torch.float32
Expected layers.42.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.44.attention_norm.weight to be bf16, got torch.float32

Expected layers.49.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.43.attention.wv.weight to be bf16, got torch.float32
Expected layers.49.attention.wq.weight to be bf16, got torch.float32
Expected layers.45.attention.wk.weight to be bf16, got torch.float32
Expected layers.50.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.42.ffn_norm.weight to be bf16, got torch.float32
Expected layers.42.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.43.attention.wo.weight to be bf16, got torch.float32
Expected layers.44.ffn_norm.weight to be bf16, got torch.float32
Expected layers.49.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.49.attention.wk.weight to be bf16, got torch.float32
Expected layers.45.attention.wv.weight to be bf16, got torch.float32
Expected layers.50.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.43.attention.wq.weight to be bf16, got torch.float32
Expected layers.42.attention_norm.weight to be bf16, got torch.float32Expected layers.49.attention_norm.weight to be bf16, got torch.float32
Expected layers.43.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.45.attention.wq.weight to be bf16, got torch.float32

Expected layers.49.attention.wv.weight to be bf16, got torch.float32
Expected layers.45.attention.wo.weight to be bf16, got torch.float32
Expected layers.50.attention_norm.weight to be bf16, got torch.float32
Expected layers.43.attention.wk.weight to be bf16, got torch.float32
Expected layers.42.ffn_norm.weight to be bf16, got torch.float32
Expected layers.49.ffn_norm.weight to be bf16, got torch.float32Expected layers.43.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.45.attention.wk.weight to be bf16, got torch.float32

Expected layers.50.ffn_norm.weight to be bf16, got torch.float32
Expected layers.45.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.49.attention.wo.weight to be bf16, got torch.float32Expected layers.43.attention.wv.weight to be bf16, got torch.float32
Expected layers.43.attention.wq.weight to be bf16, got torch.float32Expected layers.50.attention.wq.weight to be bf16, got torch.float32
Expected layers.43.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.45.attention.wv.weight to be bf16, got torch.float32
Expected layers.51.attention.wq.weight to be bf16, got torch.float32

Expected layers.45.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.49.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.43.attention.wo.weight to be bf16, got torch.float32
Expected layers.43.attention.wk.weight to be bf16, got torch.float32
Expected layers.50.attention.wk.weight to be bf16, got torch.float32Expected layers.43.attention_norm.weight to be bf16, got torch.float32
Expected layers.51.attention.wk.weight to be bf16, got torch.float32
Expected layers.45.attention.wo.weight to be bf16, got torch.float32
Expected layers.45.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.49.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.43.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.43.attention.wv.weight to be bf16, got torch.float32
Expected layers.50.attention.wv.weight to be bf16, got torch.float32
Expected layers.43.ffn_norm.weight to be bf16, got torch.float32
Expected layers.51.attention.wv.weight to be bf16, got torch.float32
Expected layers.45.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.45.attention_norm.weight to be bf16, got torch.float32

Expected layers.49.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.43.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.51.attention.wo.weight to be bf16, got torch.float32Expected layers.43.attention.wo.weight to be bf16, got torch.float32Expected layers.50.attention.wo.weight to be bf16, got torch.float32

Expected layers.44.attention.wq.weight to be bf16, got torch.float32
Expected layers.45.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.45.ffn_norm.weight to be bf16, got torch.float32

Expected layers.49.attention_norm.weight to be bf16, got torch.float32Expected layers.43.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.51.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.43.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.50.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.44.attention.wk.weight to be bf16, got torch.float32Expected layers.45.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.46.attention.wq.weight to be bf16, got torch.float32Expected layers.49.ffn_norm.weight to be bf16, got torch.float32

Expected layers.43.attention_norm.weight to be bf16, got torch.float32
Expected layers.51.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.43.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.50.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.44.attention.wv.weight to be bf16, got torch.float32
Expected layers.45.attention_norm.weight to be bf16, got torch.float32
Expected layers.46.attention.wk.weight to be bf16, got torch.float32
Expected layers.50.attention.wq.weight to be bf16, got torch.float32
Expected layers.43.ffn_norm.weight to be bf16, got torch.float32
Expected layers.51.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.43.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.50.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.44.attention.wo.weight to be bf16, got torch.float32
Expected layers.45.ffn_norm.weight to be bf16, got torch.float32
Expected layers.46.attention.wv.weight to be bf16, got torch.float32
Expected layers.50.attention.wk.weight to be bf16, got torch.float32
Expected layers.44.attention.wq.weight to be bf16, got torch.float32
Expected layers.51.attention_norm.weight to be bf16, got torch.float32
Expected layers.43.attention_norm.weight to be bf16, got torch.float32
Expected layers.50.attention_norm.weight to be bf16, got torch.float32
Expected layers.44.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.46.attention.wq.weight to be bf16, got torch.float32
Expected layers.46.attention.wo.weight to be bf16, got torch.float32
Expected layers.50.attention.wv.weight to be bf16, got torch.float32
Expected layers.44.attention.wk.weight to be bf16, got torch.float32Expected layers.51.ffn_norm.weight to be bf16, got torch.float32
Expected layers.43.ffn_norm.weight to be bf16, got torch.float32
Expected layers.50.ffn_norm.weight to be bf16, got torch.float32

Expected layers.44.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.46.attention.wk.weight to be bf16, got torch.float32Expected layers.46.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.50.attention.wo.weight to be bf16, got torch.float32
Expected layers.44.attention.wv.weight to be bf16, got torch.float32Expected layers.52.attention.wq.weight to be bf16, got torch.float32

Expected layers.44.attention.wq.weight to be bf16, got torch.float32Expected layers.51.attention.wq.weight to be bf16, got torch.float32

Expected layers.44.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.46.attention.wv.weight to be bf16, got torch.float32
Expected layers.46.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.50.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.44.attention.wo.weight to be bf16, got torch.float32
Expected layers.52.attention.wk.weight to be bf16, got torch.float32
Expected layers.44.attention.wk.weight to be bf16, got torch.float32
Expected layers.51.attention.wk.weight to be bf16, got torch.float32
Expected layers.44.attention_norm.weight to be bf16, got torch.float32Expected layers.46.attention.wo.weight to be bf16, got torch.float32
Expected layers.44.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.46.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.50.feed_forward.w2.weight to be bf16, got torch.float32


Expected layers.52.attention.wv.weight to be bf16, got torch.float32

Expected layers.44.attention.wv.weight to be bf16, got torch.float32Expected layers.51.attention.wv.weight to be bf16, got torch.float32Expected layers.46.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.44.ffn_norm.weight to be bf16, got torch.float32

Expected layers.44.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.46.attention_norm.weight to be bf16, got torch.float32Expected layers.50.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.52.attention.wo.weight to be bf16, got torch.float32

Expected layers.44.attention.wo.weight to be bf16, got torch.float32Expected layers.51.attention.wo.weight to be bf16, got torch.float32
Expected layers.46.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.45.attention.wq.weight to be bf16, got torch.float32
Expected layers.44.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.46.ffn_norm.weight to be bf16, got torch.float32Expected layers.50.attention_norm.weight to be bf16, got torch.float32

Expected layers.52.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.44.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.51.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.46.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.45.attention.wk.weight to be bf16, got torch.float32
Expected layers.44.attention_norm.weight to be bf16, got torch.float32
Expected layers.47.attention.wq.weight to be bf16, got torch.float32
Expected layers.50.ffn_norm.weight to be bf16, got torch.float32
Expected layers.52.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.44.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.51.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.46.attention_norm.weight to be bf16, got torch.float32
Expected layers.45.attention.wv.weight to be bf16, got torch.float32
Expected layers.44.ffn_norm.weight to be bf16, got torch.float32Expected layers.52.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.47.attention.wk.weight to be bf16, got torch.float32
Expected layers.51.attention.wq.weight to be bf16, got torch.float32

Expected layers.44.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.51.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.46.ffn_norm.weight to be bf16, got torch.float32
Expected layers.45.attention.wo.weight to be bf16, got torch.float32
Expected layers.45.attention.wq.weight to be bf16, got torch.float32
Expected layers.52.attention_norm.weight to be bf16, got torch.float32
Expected layers.47.attention.wv.weight to be bf16, got torch.float32
Expected layers.51.attention.wk.weight to be bf16, got torch.float32
Expected layers.44.attention_norm.weight to be bf16, got torch.float32Expected layers.51.attention_norm.weight to be bf16, got torch.float32

Expected layers.47.attention.wq.weight to be bf16, got torch.float32
Expected layers.45.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.45.attention.wk.weight to be bf16, got torch.float32
Expected layers.52.ffn_norm.weight to be bf16, got torch.float32
Expected layers.47.attention.wo.weight to be bf16, got torch.float32
Expected layers.51.attention.wv.weight to be bf16, got torch.float32
Expected layers.44.ffn_norm.weight to be bf16, got torch.float32
Expected layers.51.ffn_norm.weight to be bf16, got torch.float32
Expected layers.47.attention.wk.weight to be bf16, got torch.float32
Expected layers.45.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.45.attention.wv.weight to be bf16, got torch.float32
Expected layers.53.attention.wq.weight to be bf16, got torch.float32
Expected layers.47.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.51.attention.wo.weight to be bf16, got torch.float32

Expected layers.45.attention.wq.weight to be bf16, got torch.float32Expected layers.52.attention.wq.weight to be bf16, got torch.float32

Expected layers.47.attention.wv.weight to be bf16, got torch.float32Expected layers.45.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.45.attention.wo.weight to be bf16, got torch.float32Expected layers.53.attention.wk.weight to be bf16, got torch.float32

Expected layers.47.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.52.attention.wk.weight to be bf16, got torch.float32Expected layers.51.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.45.attention.wk.weight to be bf16, got torch.float32


Expected layers.47.attention.wo.weight to be bf16, got torch.float32Expected layers.45.attention_norm.weight to be bf16, got torch.float32
Expected layers.45.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.53.attention.wv.weight to be bf16, got torch.float32

Expected layers.47.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.52.attention.wv.weight to be bf16, got torch.float32Expected layers.51.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.45.attention.wv.weight to be bf16, got torch.float32

Expected layers.47.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.45.ffn_norm.weight to be bf16, got torch.float32
Expected layers.45.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.53.attention.wo.weight to be bf16, got torch.float32
Expected layers.47.attention_norm.weight to be bf16, got torch.float32
Expected layers.52.attention.wo.weight to be bf16, got torch.float32
Expected layers.51.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.45.attention.wo.weight to be bf16, got torch.float32
Expected layers.47.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.46.attention.wq.weight to be bf16, got torch.float32
Expected layers.45.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.52.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.53.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.47.ffn_norm.weight to be bf16, got torch.float32


Expected layers.51.attention_norm.weight to be bf16, got torch.float32
Expected layers.47.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.45.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.46.attention.wk.weight to be bf16, got torch.float32
Expected layers.45.attention_norm.weight to be bf16, got torch.float32Expected layers.52.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.53.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.48.attention.wq.weight to be bf16, got torch.float32

Expected layers.51.ffn_norm.weight to be bf16, got torch.float32
Expected layers.47.attention_norm.weight to be bf16, got torch.float32
Expected layers.45.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.46.attention.wv.weight to be bf16, got torch.float32
Expected layers.45.ffn_norm.weight to be bf16, got torch.float32
Expected layers.52.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.53.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.48.attention.wk.weight to be bf16, got torch.float32
Expected layers.52.attention.wq.weight to be bf16, got torch.float32
Expected layers.47.ffn_norm.weight to be bf16, got torch.float32
Expected layers.45.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.46.attention.wo.weight to be bf16, got torch.float32
Expected layers.46.attention.wq.weight to be bf16, got torch.float32
Expected layers.52.attention_norm.weight to be bf16, got torch.float32Expected layers.53.attention_norm.weight to be bf16, got torch.float32

Expected layers.48.attention.wv.weight to be bf16, got torch.float32
Expected layers.52.attention.wk.weight to be bf16, got torch.float32
Expected layers.48.attention.wq.weight to be bf16, got torch.float32Expected layers.45.attention_norm.weight to be bf16, got torch.float32

Expected layers.46.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.46.attention.wk.weight to be bf16, got torch.float32
Expected layers.52.ffn_norm.weight to be bf16, got torch.float32
Expected layers.53.ffn_norm.weight to be bf16, got torch.float32
Expected layers.48.attention.wo.weight to be bf16, got torch.float32
Expected layers.52.attention.wv.weight to be bf16, got torch.float32
Expected layers.48.attention.wk.weight to be bf16, got torch.float32
Expected layers.45.ffn_norm.weight to be bf16, got torch.float32

Expected layers.46.attention.wv.weight to be bf16, got torch.float32Expected layers.46.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.53.attention.wq.weight to be bf16, got torch.float32Expected layers.54.attention.wq.weight to be bf16, got torch.float32
Expected layers.48.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.52.attention.wo.weight to be bf16, got torch.float32

Expected layers.48.attention.wv.weight to be bf16, got torch.float32
Expected layers.46.attention.wq.weight to be bf16, got torch.float32
Expected layers.46.attention.wo.weight to be bf16, got torch.float32
Expected layers.46.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.53.attention.wk.weight to be bf16, got torch.float32Expected layers.54.attention.wk.weight to be bf16, got torch.float32

Expected layers.48.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.52.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.48.attention.wo.weight to be bf16, got torch.float32
Expected layers.46.attention.wk.weight to be bf16, got torch.float32

Expected layers.46.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.46.attention_norm.weight to be bf16, got torch.float32
Expected layers.53.attention.wv.weight to be bf16, got torch.float32Expected layers.54.attention.wv.weight to be bf16, got torch.float32

Expected layers.48.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.52.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.48.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.46.attention.wv.weight to be bf16, got torch.float32
Expected layers.46.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.46.ffn_norm.weight to be bf16, got torch.float32
Expected layers.53.attention.wo.weight to be bf16, got torch.float32Expected layers.54.attention.wo.weight to be bf16, got torch.float32

Expected layers.48.attention_norm.weight to be bf16, got torch.float32
Expected layers.52.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.48.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.46.attention.wo.weight to be bf16, got torch.float32

Expected layers.46.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.47.attention.wq.weight to be bf16, got torch.float32
Expected layers.53.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.54.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.48.ffn_norm.weight to be bf16, got torch.float32
Expected layers.52.attention_norm.weight to be bf16, got torch.float32
Expected layers.48.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.46.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.46.attention_norm.weight to be bf16, got torch.float32Expected layers.47.attention.wk.weight to be bf16, got torch.float32

Expected layers.53.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.54.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.49.attention.wq.weight to be bf16, got torch.float32
Expected layers.52.ffn_norm.weight to be bf16, got torch.float32
Expected layers.48.attention_norm.weight to be bf16, got torch.float32
Expected layers.46.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.46.ffn_norm.weight to be bf16, got torch.float32
Expected layers.47.attention.wv.weight to be bf16, got torch.float32
Expected layers.53.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.54.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.49.attention.wk.weight to be bf16, got torch.float32

Expected layers.53.attention.wq.weight to be bf16, got torch.float32Expected layers.48.ffn_norm.weight to be bf16, got torch.float32
Expected layers.46.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.47.attention.wq.weight to be bf16, got torch.float32Expected layers.47.attention.wo.weight to be bf16, got torch.float32
Expected layers.53.attention_norm.weight to be bf16, got torch.float32Expected layers.54.attention_norm.weight to be bf16, got torch.float32

Expected layers.49.attention.wv.weight to be bf16, got torch.float32
Expected layers.53.attention.wk.weight to be bf16, got torch.float32
Expected layers.49.attention.wq.weight to be bf16, got torch.float32
Expected layers.46.attention_norm.weight to be bf16, got torch.float32
Expected layers.47.attention.wk.weight to be bf16, got torch.float32
Expected layers.47.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.53.ffn_norm.weight to be bf16, got torch.float32Expected layers.54.ffn_norm.weight to be bf16, got torch.float32

Expected layers.49.attention.wk.weight to be bf16, got torch.float32Expected layers.49.attention.wo.weight to be bf16, got torch.float32
Expected layers.53.attention.wv.weight to be bf16, got torch.float32

Expected layers.47.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.46.ffn_norm.weight to be bf16, got torch.float32

Expected layers.47.attention.wv.weight to be bf16, got torch.float32

Expected layers.54.attention.wq.weight to be bf16, got torch.float32Expected layers.55.attention.wq.weight to be bf16, got torch.float32
Expected layers.49.attention.wv.weight to be bf16, got torch.float32

Expected layers.49.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.53.attention.wo.weight to be bf16, got torch.float32
Expected layers.47.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.47.attention.wq.weight to be bf16, got torch.float32

Expected layers.47.attention.wo.weight to be bf16, got torch.float32

Expected layers.54.attention.wk.weight to be bf16, got torch.float32Expected layers.55.attention.wk.weight to be bf16, got torch.float32
Expected layers.49.attention.wo.weight to be bf16, got torch.float32
Expected layers.49.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.53.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.47.attention_norm.weight to be bf16, got torch.float32Expected layers.47.attention.wk.weight to be bf16, got torch.float32

Expected layers.47.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.54.attention.wv.weight to be bf16, got torch.float32
Expected layers.55.attention.wv.weight to be bf16, got torch.float32Expected layers.49.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.49.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.53.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.47.ffn_norm.weight to be bf16, got torch.float32
Expected layers.47.attention.wv.weight to be bf16, got torch.float32

Expected layers.47.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.54.attention.wo.weight to be bf16, got torch.float32
Expected layers.55.attention.wo.weight to be bf16, got torch.float32Expected layers.49.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.49.attention_norm.weight to be bf16, got torch.float32
Expected layers.53.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.48.attention.wq.weight to be bf16, got torch.float32Expected layers.47.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.47.attention.wo.weight to be bf16, got torch.float32

Expected layers.54.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.55.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.49.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.49.ffn_norm.weight to be bf16, got torch.float32
Expected layers.53.attention_norm.weight to be bf16, got torch.float32
Expected layers.48.attention.wk.weight to be bf16, got torch.float32
Expected layers.47.attention_norm.weight to be bf16, got torch.float32
Expected layers.47.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.54.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.55.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.49.attention_norm.weight to be bf16, got torch.float32

Expected layers.50.attention.wq.weight to be bf16, got torch.float32
Expected layers.53.ffn_norm.weight to be bf16, got torch.float32
Expected layers.48.attention.wv.weight to be bf16, got torch.float32
Expected layers.47.ffn_norm.weight to be bf16, got torch.float32

Expected layers.47.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.54.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.55.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.49.ffn_norm.weight to be bf16, got torch.float32

Expected layers.50.attention.wk.weight to be bf16, got torch.float32
Expected layers.54.attention.wq.weight to be bf16, got torch.float32
Expected layers.48.attention.wo.weight to be bf16, got torch.float32

Expected layers.48.attention.wq.weight to be bf16, got torch.float32
Expected layers.47.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.54.attention_norm.weight to be bf16, got torch.float32Expected layers.55.attention_norm.weight to be bf16, got torch.float32
Expected layers.50.attention.wq.weight to be bf16, got torch.float32

Expected layers.50.attention.wv.weight to be bf16, got torch.float32
Expected layers.54.attention.wk.weight to be bf16, got torch.float32
Expected layers.48.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.48.attention.wk.weight to be bf16, got torch.float32
Expected layers.47.attention_norm.weight to be bf16, got torch.float32Expected layers.54.ffn_norm.weight to be bf16, got torch.float32Expected layers.55.ffn_norm.weight to be bf16, got torch.float32
Expected layers.50.attention.wk.weight to be bf16, got torch.float32

Expected layers.50.attention.wo.weight to be bf16, got torch.float32
Expected layers.54.attention.wv.weight to be bf16, got torch.float32
Expected layers.48.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.48.attention.wv.weight to be bf16, got torch.float32
Expected layers.47.ffn_norm.weight to be bf16, got torch.float32
Expected layers.55.attention.wq.weight to be bf16, got torch.float32
Expected layers.56.attention.wq.weight to be bf16, got torch.float32Expected layers.50.attention.wv.weight to be bf16, got torch.float32
Expected layers.50.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.54.attention.wo.weight to be bf16, got torch.float32

Expected layers.48.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.48.attention.wo.weight to be bf16, got torch.float32

Expected layers.48.attention.wq.weight to be bf16, got torch.float32
Expected layers.55.attention.wk.weight to be bf16, got torch.float32Expected layers.56.attention.wk.weight to be bf16, got torch.float32Expected layers.50.attention.wo.weight to be bf16, got torch.float32
Expected layers.50.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.54.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.48.attention_norm.weight to be bf16, got torch.float32
Expected layers.48.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.48.attention.wk.weight to be bf16, got torch.float32Expected layers.55.attention.wv.weight to be bf16, got torch.float32
Expected layers.56.attention.wv.weight to be bf16, got torch.float32Expected layers.50.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.50.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.54.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.48.ffn_norm.weight to be bf16, got torch.float32

Expected layers.48.feed_forward.w2.weight to be bf16, got torch.float32


Expected layers.48.attention.wv.weight to be bf16, got torch.float32Expected layers.55.attention.wo.weight to be bf16, got torch.float32
Expected layers.56.attention.wo.weight to be bf16, got torch.float32
Expected layers.50.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.50.attention_norm.weight to be bf16, got torch.float32
Expected layers.54.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.49.attention.wq.weight to be bf16, got torch.float32

Expected layers.48.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.56.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.48.attention.wo.weight to be bf16, got torch.float32Expected layers.55.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.50.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.50.ffn_norm.weight to be bf16, got torch.float32
Expected layers.54.attention_norm.weight to be bf16, got torch.float32
Expected layers.49.attention.wk.weight to be bf16, got torch.float32
Expected layers.48.attention_norm.weight to be bf16, got torch.float32
Expected layers.56.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.48.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.55.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.50.attention_norm.weight to be bf16, got torch.float32
Expected layers.51.attention.wq.weight to be bf16, got torch.float32
Expected layers.54.ffn_norm.weight to be bf16, got torch.float32
Expected layers.49.attention.wv.weight to be bf16, got torch.float32
Expected layers.48.ffn_norm.weight to be bf16, got torch.float32
Expected layers.56.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.48.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.55.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.50.ffn_norm.weight to be bf16, got torch.float32
Expected layers.51.attention.wk.weight to be bf16, got torch.float32
Expected layers.55.attention.wq.weight to be bf16, got torch.float32
Expected layers.49.attention.wo.weight to be bf16, got torch.float32
Expected layers.49.attention.wq.weight to be bf16, got torch.float32
Expected layers.56.attention_norm.weight to be bf16, got torch.float32
Expected layers.48.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.55.attention_norm.weight to be bf16, got torch.float32

Expected layers.51.attention.wq.weight to be bf16, got torch.float32
Expected layers.51.attention.wv.weight to be bf16, got torch.float32
Expected layers.55.attention.wk.weight to be bf16, got torch.float32
Expected layers.49.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.49.attention.wk.weight to be bf16, got torch.float32
Expected layers.56.ffn_norm.weight to be bf16, got torch.float32
Expected layers.48.attention_norm.weight to be bf16, got torch.float32
Expected layers.55.ffn_norm.weight to be bf16, got torch.float32
Expected layers.51.attention.wk.weight to be bf16, got torch.float32
Expected layers.51.attention.wo.weight to be bf16, got torch.float32
Expected layers.55.attention.wv.weight to be bf16, got torch.float32
Expected layers.49.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.49.attention.wv.weight to be bf16, got torch.float32
Expected layers.57.attention.wq.weight to be bf16, got torch.float32
Expected layers.48.ffn_norm.weight to be bf16, got torch.float32
Expected layers.56.attention.wq.weight to be bf16, got torch.float32
Expected layers.51.attention.wv.weight to be bf16, got torch.float32
Expected layers.51.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.55.attention.wo.weight to be bf16, got torch.float32
Expected layers.49.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.49.attention.wo.weight to be bf16, got torch.float32Expected layers.57.attention.wk.weight to be bf16, got torch.float32
Expected layers.49.attention.wq.weight to be bf16, got torch.float32
Expected layers.56.attention.wk.weight to be bf16, got torch.float32

Expected layers.51.attention.wo.weight to be bf16, got torch.float32
Expected layers.51.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.55.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.49.attention_norm.weight to be bf16, got torch.float32

Expected layers.49.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.57.attention.wv.weight to be bf16, got torch.float32
Expected layers.49.attention.wk.weight to be bf16, got torch.float32Expected layers.56.attention.wv.weight to be bf16, got torch.float32

Expected layers.51.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.51.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.55.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.49.ffn_norm.weight to be bf16, got torch.float32
Expected layers.49.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.57.attention.wo.weight to be bf16, got torch.float32
Expected layers.49.attention.wv.weight to be bf16, got torch.float32
Expected layers.56.attention.wo.weight to be bf16, got torch.float32
Expected layers.51.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.51.attention_norm.weight to be bf16, got torch.float32
Expected layers.55.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.50.attention.wq.weight to be bf16, got torch.float32
Expected layers.49.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.57.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.49.attention.wo.weight to be bf16, got torch.float32
Expected layers.56.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.51.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.51.ffn_norm.weight to be bf16, got torch.float32
Expected layers.55.attention_norm.weight to be bf16, got torch.float32
Expected layers.50.attention.wk.weight to be bf16, got torch.float32
Expected layers.49.attention_norm.weight to be bf16, got torch.float32
Expected layers.57.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.49.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.56.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.51.attention_norm.weight to be bf16, got torch.float32
Expected layers.52.attention.wq.weight to be bf16, got torch.float32
Expected layers.55.ffn_norm.weight to be bf16, got torch.float32
Expected layers.50.attention.wv.weight to be bf16, got torch.float32
Expected layers.49.ffn_norm.weight to be bf16, got torch.float32
Expected layers.57.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.49.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.56.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.51.ffn_norm.weight to be bf16, got torch.float32
Expected layers.52.attention.wk.weight to be bf16, got torch.float32
Expected layers.56.attention.wq.weight to be bf16, got torch.float32
Expected layers.50.attention.wo.weight to be bf16, got torch.float32
Expected layers.50.attention.wq.weight to be bf16, got torch.float32
Expected layers.57.attention_norm.weight to be bf16, got torch.float32
Expected layers.56.attention_norm.weight to be bf16, got torch.float32Expected layers.49.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.52.attention.wq.weight to be bf16, got torch.float32
Expected layers.52.attention.wv.weight to be bf16, got torch.float32
Expected layers.56.attention.wk.weight to be bf16, got torch.float32Expected layers.50.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.50.attention.wk.weight to be bf16, got torch.float32
Expected layers.57.ffn_norm.weight to be bf16, got torch.float32
Expected layers.56.ffn_norm.weight to be bf16, got torch.float32Expected layers.49.attention_norm.weight to be bf16, got torch.float32

Expected layers.52.attention.wk.weight to be bf16, got torch.float32Expected layers.52.attention.wo.weight to be bf16, got torch.float32

Expected layers.56.attention.wv.weight to be bf16, got torch.float32Expected layers.50.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.50.attention.wv.weight to be bf16, got torch.float32Expected layers.58.attention.wq.weight to be bf16, got torch.float32

Expected layers.57.attention.wq.weight to be bf16, got torch.float32
Expected layers.49.ffn_norm.weight to be bf16, got torch.float32
Expected layers.52.attention.wv.weight to be bf16, got torch.float32
Expected layers.52.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.56.attention.wo.weight to be bf16, got torch.float32
Expected layers.50.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.57.attention.wk.weight to be bf16, got torch.float32Expected layers.50.attention.wo.weight to be bf16, got torch.float32
Expected layers.58.attention.wk.weight to be bf16, got torch.float32

Expected layers.50.attention.wq.weight to be bf16, got torch.float32

Expected layers.52.attention.wo.weight to be bf16, got torch.float32Expected layers.52.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.56.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.50.attention_norm.weight to be bf16, got torch.float32
Expected layers.57.attention.wv.weight to be bf16, got torch.float32
Expected layers.50.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.58.attention.wv.weight to be bf16, got torch.float32

Expected layers.50.attention.wk.weight to be bf16, got torch.float32

Expected layers.52.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.52.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.56.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.50.ffn_norm.weight to be bf16, got torch.float32
Expected layers.57.attention.wo.weight to be bf16, got torch.float32
Expected layers.50.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.58.attention.wo.weight to be bf16, got torch.float32

Expected layers.50.attention.wv.weight to be bf16, got torch.float32
Expected layers.52.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.52.attention_norm.weight to be bf16, got torch.float32Expected layers.56.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.51.attention.wq.weight to be bf16, got torch.float32

Expected layers.57.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.50.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.58.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.50.attention.wo.weight to be bf16, got torch.float32Expected layers.52.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.51.attention.wk.weight to be bf16, got torch.float32Expected layers.52.ffn_norm.weight to be bf16, got torch.float32Expected layers.56.attention_norm.weight to be bf16, got torch.float32


Expected layers.57.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.50.attention_norm.weight to be bf16, got torch.float32
Expected layers.58.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.50.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.52.attention_norm.weight to be bf16, got torch.float32Expected layers.51.attention.wv.weight to be bf16, got torch.float32Expected layers.53.attention.wq.weight to be bf16, got torch.float32
Expected layers.56.ffn_norm.weight to be bf16, got torch.float32
Expected layers.57.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.50.ffn_norm.weight to be bf16, got torch.float32Expected layers.58.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.50.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.52.ffn_norm.weight to be bf16, got torch.float32Expected layers.57.attention.wq.weight to be bf16, got torch.float32Expected layers.51.attention.wo.weight to be bf16, got torch.float32
Expected layers.53.attention.wk.weight to be bf16, got torch.float32
Expected layers.57.attention_norm.weight to be bf16, got torch.float32

Expected layers.51.attention.wq.weight to be bf16, got torch.float32Expected layers.58.attention_norm.weight to be bf16, got torch.float32

Expected layers.50.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.53.attention.wq.weight to be bf16, got torch.float32
Expected layers.57.attention.wk.weight to be bf16, got torch.float32Expected layers.51.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.53.attention.wv.weight to be bf16, got torch.float32
Expected layers.57.ffn_norm.weight to be bf16, got torch.float32
Expected layers.51.attention.wk.weight to be bf16, got torch.float32Expected layers.58.ffn_norm.weight to be bf16, got torch.float32

Expected layers.50.attention_norm.weight to be bf16, got torch.float32

Expected layers.53.attention.wk.weight to be bf16, got torch.float32Expected layers.57.attention.wv.weight to be bf16, got torch.float32
Expected layers.51.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.53.attention.wo.weight to be bf16, got torch.float32
Expected layers.58.attention.wq.weight to be bf16, got torch.float32
Expected layers.51.attention.wv.weight to be bf16, got torch.float32Expected layers.59.attention.wq.weight to be bf16, got torch.float32

Expected layers.50.ffn_norm.weight to be bf16, got torch.float32
Expected layers.53.attention.wv.weight to be bf16, got torch.float32
Expected layers.57.attention.wo.weight to be bf16, got torch.float32
Expected layers.51.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.53.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.58.attention.wk.weight to be bf16, got torch.float32
Expected layers.51.attention.wo.weight to be bf16, got torch.float32
Expected layers.59.attention.wk.weight to be bf16, got torch.float32
Expected layers.51.attention.wq.weight to be bf16, got torch.float32
Expected layers.53.attention.wo.weight to be bf16, got torch.float32
Expected layers.57.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.51.attention_norm.weight to be bf16, got torch.float32
Expected layers.53.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.58.attention.wv.weight to be bf16, got torch.float32
Expected layers.51.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.59.attention.wv.weight to be bf16, got torch.float32
Expected layers.51.attention.wk.weight to be bf16, got torch.float32
Expected layers.53.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.57.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.51.ffn_norm.weight to be bf16, got torch.float32
Expected layers.53.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.58.attention.wo.weight to be bf16, got torch.float32
Expected layers.51.attention.wv.weight to be bf16, got torch.float32Expected layers.51.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.59.attention.wo.weight to be bf16, got torch.float32

Expected layers.53.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.57.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.52.attention.wq.weight to be bf16, got torch.float32Expected layers.58.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.51.attention.wo.weight to be bf16, got torch.float32

Expected layers.53.attention_norm.weight to be bf16, got torch.float32


Expected layers.51.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.59.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.53.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.57.attention_norm.weight to be bf16, got torch.float32Expected layers.51.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.51.attention_norm.weight to be bf16, got torch.float32


Expected layers.52.attention.wk.weight to be bf16, got torch.float32
Expected layers.53.ffn_norm.weight to be bf16, got torch.float32Expected layers.51.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.58.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.59.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.51.ffn_norm.weight to be bf16, got torch.float32
Expected layers.52.attention.wv.weight to be bf16, got torch.float32

Expected layers.53.attention_norm.weight to be bf16, got torch.float32Expected layers.54.attention.wq.weight to be bf16, got torch.float32
Expected layers.57.ffn_norm.weight to be bf16, got torch.float32Expected layers.51.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.58.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.59.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.52.attention.wq.weight to be bf16, got torch.float32

Expected layers.52.attention.wo.weight to be bf16, got torch.float32

Expected layers.53.ffn_norm.weight to be bf16, got torch.float32Expected layers.54.attention.wk.weight to be bf16, got torch.float32
Expected layers.58.attention.wq.weight to be bf16, got torch.float32Expected layers.51.attention_norm.weight to be bf16, got torch.float32
Expected layers.58.attention_norm.weight to be bf16, got torch.float32

Expected layers.59.attention_norm.weight to be bf16, got torch.float32
Expected layers.52.attention.wk.weight to be bf16, got torch.float32
Expected layers.52.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.54.attention.wq.weight to be bf16, got torch.float32
Expected layers.54.attention.wv.weight to be bf16, got torch.float32
Expected layers.58.attention.wk.weight to be bf16, got torch.float32Expected layers.51.ffn_norm.weight to be bf16, got torch.float32
Expected layers.58.ffn_norm.weight to be bf16, got torch.float32
Expected layers.59.ffn_norm.weight to be bf16, got torch.float32

Expected layers.52.attention.wv.weight to be bf16, got torch.float32

Expected layers.52.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.58.attention.wv.weight to be bf16, got torch.float32Expected layers.54.attention.wk.weight to be bf16, got torch.float32Expected layers.54.attention.wo.weight to be bf16, got torch.float32
Expected layers.52.attention.wq.weight to be bf16, got torch.float32

Expected layers.59.attention.wq.weight to be bf16, got torch.float32Expected layers.60.attention.wq.weight to be bf16, got torch.float32
Expected layers.52.attention.wo.weight to be bf16, got torch.float32


Expected layers.52.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.58.attention.wo.weight to be bf16, got torch.float32Expected layers.54.attention.wv.weight to be bf16, got torch.float32
Expected layers.54.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.52.attention.wk.weight to be bf16, got torch.float32
Expected layers.59.attention.wk.weight to be bf16, got torch.float32Expected layers.60.attention.wk.weight to be bf16, got torch.float32
Expected layers.52.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.52.attention_norm.weight to be bf16, got torch.float32
Expected layers.58.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.54.attention.wo.weight to be bf16, got torch.float32
Expected layers.54.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.52.attention.wv.weight to be bf16, got torch.float32
Expected layers.59.attention.wv.weight to be bf16, got torch.float32
Expected layers.60.attention.wv.weight to be bf16, got torch.float32
Expected layers.52.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.52.ffn_norm.weight to be bf16, got torch.float32
Expected layers.58.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.54.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.54.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.52.attention.wo.weight to be bf16, got torch.float32Expected layers.59.attention.wo.weight to be bf16, got torch.float32

Expected layers.60.attention.wo.weight to be bf16, got torch.float32Expected layers.52.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.53.attention.wq.weight to be bf16, got torch.float32Expected layers.54.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.58.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.54.attention_norm.weight to be bf16, got torch.float32

Expected layers.52.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.59.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.60.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.52.attention_norm.weight to be bf16, got torch.float32Expected layers.58.attention_norm.weight to be bf16, got torch.float32
Expected layers.53.attention.wk.weight to be bf16, got torch.float32
Expected layers.54.feed_forward.w3.weight to be bf16, got torch.float32


Expected layers.54.ffn_norm.weight to be bf16, got torch.float32Expected layers.52.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.59.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.60.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.52.ffn_norm.weight to be bf16, got torch.float32
Expected layers.58.ffn_norm.weight to be bf16, got torch.float32Expected layers.53.attention.wv.weight to be bf16, got torch.float32

Expected layers.54.attention_norm.weight to be bf16, got torch.float32
Expected layers.55.attention.wq.weight to be bf16, got torch.float32
Expected layers.52.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.60.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.59.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.53.attention.wq.weight to be bf16, got torch.float32
Expected layers.59.attention.wq.weight to be bf16, got torch.float32
Expected layers.53.attention.wo.weight to be bf16, got torch.float32

Expected layers.54.ffn_norm.weight to be bf16, got torch.float32Expected layers.55.attention.wk.weight to be bf16, got torch.float32
Expected layers.52.attention_norm.weight to be bf16, got torch.float32Expected layers.60.attention_norm.weight to be bf16, got torch.float32

Expected layers.59.attention_norm.weight to be bf16, got torch.float32
Expected layers.53.attention.wk.weight to be bf16, got torch.float32
Expected layers.59.attention.wk.weight to be bf16, got torch.float32
Expected layers.53.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.55.attention.wq.weight to be bf16, got torch.float32Expected layers.55.attention.wv.weight to be bf16, got torch.float32
Expected layers.52.ffn_norm.weight to be bf16, got torch.float32Expected layers.60.ffn_norm.weight to be bf16, got torch.float32

Expected layers.59.ffn_norm.weight to be bf16, got torch.float32
Expected layers.53.attention.wv.weight to be bf16, got torch.float32
Expected layers.59.attention.wv.weight to be bf16, got torch.float32
Expected layers.53.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.55.attention.wo.weight to be bf16, got torch.float32Expected layers.55.attention.wk.weight to be bf16, got torch.float32

Expected layers.53.attention.wq.weight to be bf16, got torch.float32Expected layers.61.attention.wq.weight to be bf16, got torch.float32

Expected layers.60.attention.wq.weight to be bf16, got torch.float32
Expected layers.53.attention.wo.weight to be bf16, got torch.float32
Expected layers.59.attention.wo.weight to be bf16, got torch.float32
Expected layers.53.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.55.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.55.attention.wv.weight to be bf16, got torch.float32
Expected layers.53.attention.wk.weight to be bf16, got torch.float32
Expected layers.61.attention.wk.weight to be bf16, got torch.float32
Expected layers.60.attention.wk.weight to be bf16, got torch.float32
Expected layers.53.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.59.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.53.attention_norm.weight to be bf16, got torch.float32
Expected layers.55.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.55.attention.wo.weight to be bf16, got torch.float32
Expected layers.53.attention.wv.weight to be bf16, got torch.float32
Expected layers.61.attention.wv.weight to be bf16, got torch.float32
Expected layers.60.attention.wv.weight to be bf16, got torch.float32
Expected layers.53.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.59.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.53.ffn_norm.weight to be bf16, got torch.float32
Expected layers.55.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.55.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.53.attention.wo.weight to be bf16, got torch.float32
Expected layers.61.attention.wo.weight to be bf16, got torch.float32
Expected layers.60.attention.wo.weight to be bf16, got torch.float32
Expected layers.53.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.59.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.54.attention.wq.weight to be bf16, got torch.float32
Expected layers.55.attention_norm.weight to be bf16, got torch.float32
Expected layers.55.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.53.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.61.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.60.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.53.attention_norm.weight to be bf16, got torch.float32Expected layers.59.attention_norm.weight to be bf16, got torch.float32

Expected layers.54.attention.wk.weight to be bf16, got torch.float32
Expected layers.55.ffn_norm.weight to be bf16, got torch.float32
Expected layers.55.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.53.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.61.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.60.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.53.ffn_norm.weight to be bf16, got torch.float32
Expected layers.59.ffn_norm.weight to be bf16, got torch.float32
Expected layers.54.attention.wv.weight to be bf16, got torch.float32
Expected layers.56.attention.wq.weight to be bf16, got torch.float32
Expected layers.55.attention_norm.weight to be bf16, got torch.float32
Expected layers.53.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.61.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.60.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.54.attention.wq.weight to be bf16, got torch.float32Expected layers.60.attention.wq.weight to be bf16, got torch.float32
Expected layers.54.attention.wo.weight to be bf16, got torch.float32Expected layers.56.attention.wk.weight to be bf16, got torch.float32

Expected layers.55.ffn_norm.weight to be bf16, got torch.float32
Expected layers.53.attention_norm.weight to be bf16, got torch.float32
Expected layers.61.attention_norm.weight to be bf16, got torch.float32
Expected layers.60.attention_norm.weight to be bf16, got torch.float32

Expected layers.54.attention.wk.weight to be bf16, got torch.float32Expected layers.60.attention.wk.weight to be bf16, got torch.float32
Expected layers.54.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.56.attention.wv.weight to be bf16, got torch.float32

Expected layers.56.attention.wq.weight to be bf16, got torch.float32
Expected layers.53.ffn_norm.weight to be bf16, got torch.float32
Expected layers.61.ffn_norm.weight to be bf16, got torch.float32Expected layers.54.attention.wv.weight to be bf16, got torch.float32
Expected layers.60.ffn_norm.weight to be bf16, got torch.float32

Expected layers.60.attention.wv.weight to be bf16, got torch.float32
Expected layers.54.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.56.attention.wo.weight to be bf16, got torch.float32
Expected layers.56.attention.wk.weight to be bf16, got torch.float32
Expected layers.54.attention.wq.weight to be bf16, got torch.float32Expected layers.62.attention.wq.weight to be bf16, got torch.float32

Expected layers.54.attention.wo.weight to be bf16, got torch.float32

Expected layers.61.attention.wq.weight to be bf16, got torch.float32Expected layers.60.attention.wo.weight to be bf16, got torch.float32
Expected layers.54.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.56.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.56.attention.wv.weight to be bf16, got torch.float32
Expected layers.54.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.54.attention.wk.weight to be bf16, got torch.float32Expected layers.62.attention.wk.weight to be bf16, got torch.float32



Expected layers.61.attention.wk.weight to be bf16, got torch.float32
Expected layers.60.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.54.attention_norm.weight to be bf16, got torch.float32

Expected layers.56.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.56.attention.wo.weight to be bf16, got torch.float32Expected layers.54.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.54.attention.wv.weight to be bf16, got torch.float32
Expected layers.60.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.62.attention.wv.weight to be bf16, got torch.float32

Expected layers.61.attention.wv.weight to be bf16, got torch.float32


Expected layers.54.ffn_norm.weight to be bf16, got torch.float32
Expected layers.56.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.56.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.54.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.54.attention.wo.weight to be bf16, got torch.float32
Expected layers.60.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.62.attention.wo.weight to be bf16, got torch.float32
Expected layers.61.attention.wo.weight to be bf16, got torch.float32

Expected layers.55.attention.wq.weight to be bf16, got torch.float32
Expected layers.56.attention_norm.weight to be bf16, got torch.float32
Expected layers.56.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.54.attention_norm.weight to be bf16, got torch.float32
Expected layers.54.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.60.attention_norm.weight to be bf16, got torch.float32
Expected layers.62.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.61.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.55.attention.wk.weight to be bf16, got torch.float32

Expected layers.54.ffn_norm.weight to be bf16, got torch.float32Expected layers.56.ffn_norm.weight to be bf16, got torch.float32
Expected layers.56.feed_forward.w3.weight to be bf16, got torch.float32


Expected layers.54.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.60.ffn_norm.weight to be bf16, got torch.float32Expected layers.62.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.61.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.55.attention.wv.weight to be bf16, got torch.float32
Expected layers.55.attention.wq.weight to be bf16, got torch.float32Expected layers.57.attention.wq.weight to be bf16, got torch.float32
Expected layers.56.attention_norm.weight to be bf16, got torch.float32

Expected layers.54.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.61.attention.wq.weight to be bf16, got torch.float32
Expected layers.62.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.61.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.55.attention.wo.weight to be bf16, got torch.float32
Expected layers.55.attention.wk.weight to be bf16, got torch.float32
Expected layers.57.attention.wk.weight to be bf16, got torch.float32
Expected layers.56.ffn_norm.weight to be bf16, got torch.float32Expected layers.54.attention_norm.weight to be bf16, got torch.float32
Expected layers.61.attention.wk.weight to be bf16, got torch.float32
Expected layers.62.attention_norm.weight to be bf16, got torch.float32


Expected layers.61.attention_norm.weight to be bf16, got torch.float32
Expected layers.55.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.55.attention.wv.weight to be bf16, got torch.float32
Expected layers.57.attention.wv.weight to be bf16, got torch.float32
Expected layers.57.attention.wq.weight to be bf16, got torch.float32
Expected layers.54.ffn_norm.weight to be bf16, got torch.float32Expected layers.61.attention.wv.weight to be bf16, got torch.float32
Expected layers.62.ffn_norm.weight to be bf16, got torch.float32

Expected layers.61.ffn_norm.weight to be bf16, got torch.float32
Expected layers.55.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.55.attention.wo.weight to be bf16, got torch.float32
Expected layers.57.attention.wo.weight to be bf16, got torch.float32
Expected layers.57.attention.wk.weight to be bf16, got torch.float32
Expected layers.55.attention.wq.weight to be bf16, got torch.float32
Expected layers.61.attention.wo.weight to be bf16, got torch.float32Expected layers.63.attention.wq.weight to be bf16, got torch.float32
Expected layers.62.attention.wq.weight to be bf16, got torch.float32

Expected layers.55.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.55.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.57.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.57.attention.wv.weight to be bf16, got torch.float32
Expected layers.55.attention.wk.weight to be bf16, got torch.float32Expected layers.61.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.63.attention.wk.weight to be bf16, got torch.float32
Expected layers.62.attention.wk.weight to be bf16, got torch.float32
Expected layers.55.attention_norm.weight to be bf16, got torch.float32
Expected layers.55.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.57.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.57.attention.wo.weight to be bf16, got torch.float32Expected layers.55.attention.wv.weight to be bf16, got torch.float32
Expected layers.61.feed_forward.w2.weight to be bf16, got torch.float32


Expected layers.62.attention.wv.weight to be bf16, got torch.float32
Expected layers.63.attention.wv.weight to be bf16, got torch.float32Expected layers.55.ffn_norm.weight to be bf16, got torch.float32
Expected layers.55.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.57.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.57.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.55.attention.wo.weight to be bf16, got torch.float32
Expected layers.61.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.62.attention.wo.weight to be bf16, got torch.float32

Expected layers.63.attention.wo.weight to be bf16, got torch.float32
Expected layers.56.attention.wq.weight to be bf16, got torch.float32
Expected layers.55.attention_norm.weight to be bf16, got torch.float32
Expected layers.57.attention_norm.weight to be bf16, got torch.float32
Expected layers.57.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.55.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.61.attention_norm.weight to be bf16, got torch.float32
Expected layers.62.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.63.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.56.attention.wk.weight to be bf16, got torch.float32
Expected layers.55.ffn_norm.weight to be bf16, got torch.float32
Expected layers.57.ffn_norm.weight to be bf16, got torch.float32
Expected layers.57.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.55.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.61.ffn_norm.weight to be bf16, got torch.float32
Expected layers.62.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.63.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.56.attention.wv.weight to be bf16, got torch.float32
Expected layers.56.attention.wq.weight to be bf16, got torch.float32

Expected layers.58.attention.wq.weight to be bf16, got torch.float32
Expected layers.57.attention_norm.weight to be bf16, got torch.float32

Expected layers.55.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.62.attention.wq.weight to be bf16, got torch.float32Expected layers.62.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.63.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.56.attention.wo.weight to be bf16, got torch.float32

Expected layers.56.attention.wk.weight to be bf16, got torch.float32
Expected layers.58.attention.wk.weight to be bf16, got torch.float32
Expected layers.57.ffn_norm.weight to be bf16, got torch.float32

Expected layers.55.attention_norm.weight to be bf16, got torch.float32Expected layers.62.attention.wk.weight to be bf16, got torch.float32
Expected layers.62.attention_norm.weight to be bf16, got torch.float32Expected layers.63.attention_norm.weight to be bf16, got torch.float32

Expected layers.56.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.56.attention.wv.weight to be bf16, got torch.float32
Expected layers.58.attention.wv.weight to be bf16, got torch.float32Expected layers.58.attention.wq.weight to be bf16, got torch.float32

Expected layers.55.ffn_norm.weight to be bf16, got torch.float32
Expected layers.63.ffn_norm.weight to be bf16, got torch.float32Expected layers.62.attention.wv.weight to be bf16, got torch.float32
Expected layers.62.ffn_norm.weight to be bf16, got torch.float32


Expected layers.56.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.56.attention.wo.weight to be bf16, got torch.float32
Expected layers.58.attention.wo.weight to be bf16, got torch.float32Expected layers.58.attention.wk.weight to be bf16, got torch.float32

Expected layers.56.attention.wq.weight to be bf16, got torch.float32
Expected layers.64.attention.wq.weight to be bf16, got torch.float32
Expected layers.62.attention.wo.weight to be bf16, got torch.float32
Expected layers.63.attention.wq.weight to be bf16, got torch.float32
Expected layers.56.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.56.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.58.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.58.attention.wv.weight to be bf16, got torch.float32
Expected layers.56.attention.wk.weight to be bf16, got torch.float32
Expected layers.64.attention.wk.weight to be bf16, got torch.float32

Expected layers.62.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.63.attention.wk.weight to be bf16, got torch.float32
Expected layers.58.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.56.attention_norm.weight to be bf16, got torch.float32
Expected layers.56.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.58.attention.wo.weight to be bf16, got torch.float32

Expected layers.56.attention.wv.weight to be bf16, got torch.float32
Expected layers.64.attention.wv.weight to be bf16, got torch.float32
Expected layers.62.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.63.attention.wv.weight to be bf16, got torch.float32
Expected layers.58.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.56.ffn_norm.weight to be bf16, got torch.float32

Expected layers.56.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.58.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.56.attention.wo.weight to be bf16, got torch.float32
Expected layers.64.attention.wo.weight to be bf16, got torch.float32
Expected layers.62.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.63.attention.wo.weight to be bf16, got torch.float32
Expected layers.58.attention_norm.weight to be bf16, got torch.float32
Expected layers.57.attention.wq.weight to be bf16, got torch.float32Expected layers.56.attention_norm.weight to be bf16, got torch.float32
Expected layers.58.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.56.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.64.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.62.attention_norm.weight to be bf16, got torch.float32
Expected layers.63.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.58.ffn_norm.weight to be bf16, got torch.float32
Expected layers.57.attention.wk.weight to be bf16, got torch.float32Expected layers.56.ffn_norm.weight to be bf16, got torch.float32

Expected layers.58.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.56.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.64.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.62.ffn_norm.weight to be bf16, got torch.float32

Expected layers.63.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.59.attention.wq.weight to be bf16, got torch.float32
Expected layers.57.attention.wv.weight to be bf16, got torch.float32Expected layers.57.attention.wq.weight to be bf16, got torch.float32

Expected layers.58.attention_norm.weight to be bf16, got torch.float32
Expected layers.56.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.64.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.63.attention.wq.weight to be bf16, got torch.float32

Expected layers.63.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.59.attention.wk.weight to be bf16, got torch.float32
Expected layers.57.attention.wo.weight to be bf16, got torch.float32
Expected layers.57.attention.wk.weight to be bf16, got torch.float32
Expected layers.58.ffn_norm.weight to be bf16, got torch.float32
Expected layers.56.attention_norm.weight to be bf16, got torch.float32
Expected layers.64.attention_norm.weight to be bf16, got torch.float32Expected layers.63.attention.wk.weight to be bf16, got torch.float32

Expected layers.63.attention_norm.weight to be bf16, got torch.float32
Expected layers.59.attention.wv.weight to be bf16, got torch.float32
Expected layers.57.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.57.attention.wv.weight to be bf16, got torch.float32

Expected layers.59.attention.wq.weight to be bf16, got torch.float32Expected layers.56.ffn_norm.weight to be bf16, got torch.float32
Expected layers.64.ffn_norm.weight to be bf16, got torch.float32Expected layers.63.attention.wv.weight to be bf16, got torch.float32

Expected layers.63.ffn_norm.weight to be bf16, got torch.float32
Expected layers.59.attention.wo.weight to be bf16, got torch.float32
Expected layers.57.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.57.attention.wo.weight to be bf16, got torch.float32

Expected layers.59.attention.wk.weight to be bf16, got torch.float32Expected layers.57.attention.wq.weight to be bf16, got torch.float32
Expected layers.65.attention.wq.weight to be bf16, got torch.float32Expected layers.63.attention.wo.weight to be bf16, got torch.float32

Expected layers.64.attention.wq.weight to be bf16, got torch.float32
Expected layers.59.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.57.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.57.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.59.attention.wv.weight to be bf16, got torch.float32
Expected layers.57.attention.wk.weight to be bf16, got torch.float32
Expected layers.65.attention.wk.weight to be bf16, got torch.float32Expected layers.63.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.64.attention.wk.weight to be bf16, got torch.float32

Expected layers.59.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.57.attention_norm.weight to be bf16, got torch.float32
Expected layers.57.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.59.attention.wo.weight to be bf16, got torch.float32
Expected layers.57.attention.wv.weight to be bf16, got torch.float32Expected layers.65.attention.wv.weight to be bf16, got torch.float32Expected layers.63.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.64.attention.wv.weight to be bf16, got torch.float32


Expected layers.59.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.57.ffn_norm.weight to be bf16, got torch.float32
Expected layers.57.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.59.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.57.attention.wo.weight to be bf16, got torch.float32
Expected layers.65.attention.wo.weight to be bf16, got torch.float32Expected layers.63.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.64.attention.wo.weight to be bf16, got torch.float32
Expected layers.59.attention_norm.weight to be bf16, got torch.float32
Expected layers.58.attention.wq.weight to be bf16, got torch.float32
Expected layers.57.attention_norm.weight to be bf16, got torch.float32
Expected layers.59.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.57.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.65.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.63.attention_norm.weight to be bf16, got torch.float32
Expected layers.64.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.59.ffn_norm.weight to be bf16, got torch.float32
Expected layers.58.attention.wk.weight to be bf16, got torch.float32
Expected layers.57.ffn_norm.weight to be bf16, got torch.float32
Expected layers.59.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.57.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.65.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.60.attention.wq.weight to be bf16, got torch.float32Expected layers.63.ffn_norm.weight to be bf16, got torch.float32
Expected layers.64.feed_forward.w2.weight to be bf16, got torch.float32



Expected layers.58.attention.wv.weight to be bf16, got torch.float32
Expected layers.58.attention.wq.weight to be bf16, got torch.float32Expected layers.59.attention_norm.weight to be bf16, got torch.float32
Expected layers.57.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.65.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.60.attention.wk.weight to be bf16, got torch.float32Expected layers.64.attention.wq.weight to be bf16, got torch.float32
Expected layers.64.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.58.attention.wo.weight to be bf16, got torch.float32
Expected layers.58.attention.wk.weight to be bf16, got torch.float32
Expected layers.59.ffn_norm.weight to be bf16, got torch.float32
Expected layers.57.attention_norm.weight to be bf16, got torch.float32
Expected layers.65.attention_norm.weight to be bf16, got torch.float32Expected layers.60.attention.wv.weight to be bf16, got torch.float32
Expected layers.64.attention.wk.weight to be bf16, got torch.float32

Expected layers.64.attention_norm.weight to be bf16, got torch.float32
Expected layers.58.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.58.attention.wv.weight to be bf16, got torch.float32
Expected layers.60.attention.wq.weight to be bf16, got torch.float32
Expected layers.65.ffn_norm.weight to be bf16, got torch.float32Expected layers.57.ffn_norm.weight to be bf16, got torch.float32
Expected layers.60.attention.wo.weight to be bf16, got torch.float32
Expected layers.64.attention.wv.weight to be bf16, got torch.float32

Expected layers.64.ffn_norm.weight to be bf16, got torch.float32
Expected layers.58.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.58.attention.wo.weight to be bf16, got torch.float32
Expected layers.60.attention.wk.weight to be bf16, got torch.float32
Expected layers.66.attention.wq.weight to be bf16, got torch.float32Expected layers.58.attention.wq.weight to be bf16, got torch.float32

Expected layers.58.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.60.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.64.attention.wo.weight to be bf16, got torch.float32Expected layers.65.attention.wq.weight to be bf16, got torch.float32





Expected layers.58.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.60.attention.wv.weight to be bf16, got torch.float32Expected layers.66.attention.wk.weight to be bf16, got torch.float32
Expected layers.58.attention.wk.weight to be bf16, got torch.float32
Expected layers.58.attention_norm.weight to be bf16, got torch.float32
Expected layers.60.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.64.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.65.attention.wk.weight to be bf16, got torch.float32


Expected layers.58.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.60.attention.wo.weight to be bf16, got torch.float32Expected layers.66.attention.wv.weight to be bf16, got torch.float32
Expected layers.58.attention.wv.weight to be bf16, got torch.float32Expected layers.58.ffn_norm.weight to be bf16, got torch.float32
Expected layers.60.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.64.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.65.attention.wv.weight to be bf16, got torch.float32

Expected layers.58.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.60.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.59.attention.wq.weight to be bf16, got torch.float32Expected layers.66.attention.wo.weight to be bf16, got torch.float32
Expected layers.58.attention.wo.weight to be bf16, got torch.float32
Expected layers.60.attention_norm.weight to be bf16, got torch.float32

Expected layers.64.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.65.attention.wo.weight to be bf16, got torch.float32

Expected layers.58.attention_norm.weight to be bf16, got torch.float32

Expected layers.60.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.59.attention.wk.weight to be bf16, got torch.float32
Expected layers.66.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.64.attention_norm.weight to be bf16, got torch.float32Expected layers.58.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.60.ffn_norm.weight to be bf16, got torch.float32
Expected layers.65.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.58.ffn_norm.weight to be bf16, got torch.float32
Expected layers.60.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.59.attention.wv.weight to be bf16, got torch.float32Expected layers.66.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.64.ffn_norm.weight to be bf16, got torch.float32Expected layers.58.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.59.attention.wq.weight to be bf16, got torch.float32
Expected layers.61.attention.wq.weight to be bf16, got torch.float32Expected layers.65.feed_forward.w2.weight to be bf16, got torch.float32


Expected layers.60.attention_norm.weight to be bf16, got torch.float32

Expected layers.59.attention.wo.weight to be bf16, got torch.float32Expected layers.66.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.65.attention.wq.weight to be bf16, got torch.float32
Expected layers.58.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.59.attention.wk.weight to be bf16, got torch.float32Expected layers.61.attention.wk.weight to be bf16, got torch.float32
Expected layers.65.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.60.ffn_norm.weight to be bf16, got torch.float32

Expected layers.59.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.66.attention_norm.weight to be bf16, got torch.float32
Expected layers.65.attention.wk.weight to be bf16, got torch.float32
Expected layers.58.attention_norm.weight to be bf16, got torch.float32
Expected layers.59.attention.wv.weight to be bf16, got torch.float32
Expected layers.61.attention.wv.weight to be bf16, got torch.float32Expected layers.65.attention_norm.weight to be bf16, got torch.float32
Expected layers.61.attention.wq.weight to be bf16, got torch.float32
Expected layers.59.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.66.ffn_norm.weight to be bf16, got torch.float32
Expected layers.65.attention.wv.weight to be bf16, got torch.float32

Expected layers.58.ffn_norm.weight to be bf16, got torch.float32Expected layers.59.attention.wo.weight to be bf16, got torch.float32
Expected layers.61.attention.wo.weight to be bf16, got torch.float32Expected layers.65.ffn_norm.weight to be bf16, got torch.float32

Expected layers.61.attention.wk.weight to be bf16, got torch.float32
Expected layers.59.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.67.attention.wq.weight to be bf16, got torch.float32
Expected layers.65.attention.wo.weight to be bf16, got torch.float32Expected layers.59.attention.wq.weight to be bf16, got torch.float32
Expected layers.59.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.61.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.59.attention_norm.weight to be bf16, got torch.float32Expected layers.66.attention.wq.weight to be bf16, got torch.float32

Expected layers.65.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.61.attention.wv.weight to be bf16, got torch.float32
Expected layers.67.attention.wk.weight to be bf16, got torch.float32

Expected layers.59.attention.wk.weight to be bf16, got torch.float32

Expected layers.59.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.61.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.59.ffn_norm.weight to be bf16, got torch.float32
Expected layers.66.attention.wk.weight to be bf16, got torch.float32Expected layers.65.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.61.attention.wo.weight to be bf16, got torch.float32Expected layers.67.attention.wv.weight to be bf16, got torch.float32

Expected layers.59.attention.wv.weight to be bf16, got torch.float32
Expected layers.59.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.61.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.60.attention.wq.weight to be bf16, got torch.float32
Expected layers.66.attention.wv.weight to be bf16, got torch.float32Expected layers.65.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.61.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.67.attention.wo.weight to be bf16, got torch.float32

Expected layers.59.attention.wo.weight to be bf16, got torch.float32

Expected layers.59.attention_norm.weight to be bf16, got torch.float32Expected layers.61.attention_norm.weight to be bf16, got torch.float32
Expected layers.60.attention.wk.weight to be bf16, got torch.float32
Expected layers.66.attention.wo.weight to be bf16, got torch.float32Expected layers.65.attention_norm.weight to be bf16, got torch.float32

Expected layers.61.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.67.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.59.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.59.ffn_norm.weight to be bf16, got torch.float32
Expected layers.61.ffn_norm.weight to be bf16, got torch.float32
Expected layers.60.attention.wv.weight to be bf16, got torch.float32
Expected layers.66.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.65.ffn_norm.weight to be bf16, got torch.float32
Expected layers.61.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.67.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.59.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.60.attention.wq.weight to be bf16, got torch.float32
Expected layers.62.attention.wq.weight to be bf16, got torch.float32
Expected layers.60.attention.wo.weight to be bf16, got torch.float32
Expected layers.66.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.66.attention.wq.weight to be bf16, got torch.float32Expected layers.61.attention_norm.weight to be bf16, got torch.float32
Expected layers.67.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.59.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.60.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.60.attention.wk.weight to be bf16, got torch.float32
Expected layers.62.attention.wk.weight to be bf16, got torch.float32

Expected layers.66.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.66.attention.wk.weight to be bf16, got torch.float32Expected layers.61.ffn_norm.weight to be bf16, got torch.float32Expected layers.60.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.67.attention_norm.weight to be bf16, got torch.float32
Expected layers.59.attention_norm.weight to be bf16, got torch.float32

Expected layers.60.attention.wv.weight to be bf16, got torch.float32Expected layers.66.attention_norm.weight to be bf16, got torch.float32
Expected layers.62.attention.wv.weight to be bf16, got torch.float32


Expected layers.66.attention.wv.weight to be bf16, got torch.float32
Expected layers.62.attention.wq.weight to be bf16, got torch.float32Expected layers.60.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.67.ffn_norm.weight to be bf16, got torch.float32
Expected layers.59.ffn_norm.weight to be bf16, got torch.float32
Expected layers.60.attention.wo.weight to be bf16, got torch.float32Expected layers.66.ffn_norm.weight to be bf16, got torch.float32
Expected layers.62.attention.wo.weight to be bf16, got torch.float32

Expected layers.66.attention.wo.weight to be bf16, got torch.float32

Expected layers.62.attention.wk.weight to be bf16, got torch.float32Expected layers.60.attention_norm.weight to be bf16, got torch.float32
Expected layers.68.attention.wq.weight to be bf16, got torch.float32
Expected layers.60.attention.wq.weight to be bf16, got torch.float32
Expected layers.60.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.67.attention.wq.weight to be bf16, got torch.float32Expected layers.62.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.66.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.62.attention.wv.weight to be bf16, got torch.float32Expected layers.60.ffn_norm.weight to be bf16, got torch.float32
Expected layers.68.attention.wk.weight to be bf16, got torch.float32
Expected layers.60.attention.wk.weight to be bf16, got torch.float32
Expected layers.60.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.67.attention.wk.weight to be bf16, got torch.float32Expected layers.62.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.66.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.62.attention.wo.weight to be bf16, got torch.float32Expected layers.61.attention.wq.weight to be bf16, got torch.float32
Expected layers.68.attention.wv.weight to be bf16, got torch.float32

Expected layers.60.attention.wv.weight to be bf16, got torch.float32Expected layers.60.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.67.attention.wv.weight to be bf16, got torch.float32Expected layers.62.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.66.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.62.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.61.attention.wk.weight to be bf16, got torch.float32
Expected layers.68.attention.wo.weight to be bf16, got torch.float32
Expected layers.60.attention.wo.weight to be bf16, got torch.float32
Expected layers.60.attention_norm.weight to be bf16, got torch.float32
Expected layers.67.attention.wo.weight to be bf16, got torch.float32Expected layers.62.attention_norm.weight to be bf16, got torch.float32

Expected layers.66.attention_norm.weight to be bf16, got torch.float32

Expected layers.62.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.61.attention.wv.weight to be bf16, got torch.float32Expected layers.68.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.60.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.60.ffn_norm.weight to be bf16, got torch.float32
Expected layers.67.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.62.ffn_norm.weight to be bf16, got torch.float32

Expected layers.66.ffn_norm.weight to be bf16, got torch.float32
Expected layers.62.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.61.attention.wo.weight to be bf16, got torch.float32
Expected layers.68.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.60.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.61.attention.wq.weight to be bf16, got torch.float32
Expected layers.67.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.63.attention.wq.weight to be bf16, got torch.float32
Expected layers.67.attention.wq.weight to be bf16, got torch.float32
Expected layers.62.attention_norm.weight to be bf16, got torch.float32
Expected layers.61.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.68.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.60.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.61.attention.wk.weight to be bf16, got torch.float32
Expected layers.67.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.63.attention.wk.weight to be bf16, got torch.float32
Expected layers.67.attention.wk.weight to be bf16, got torch.float32
Expected layers.62.ffn_norm.weight to be bf16, got torch.float32
Expected layers.61.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.68.attention_norm.weight to be bf16, got torch.float32
Expected layers.60.attention_norm.weight to be bf16, got torch.float32
Expected layers.61.attention.wv.weight to be bf16, got torch.float32
Expected layers.67.attention_norm.weight to be bf16, got torch.float32
Expected layers.63.attention.wv.weight to be bf16, got torch.float32
Expected layers.67.attention.wv.weight to be bf16, got torch.float32
Expected layers.63.attention.wq.weight to be bf16, got torch.float32
Expected layers.61.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.68.ffn_norm.weight to be bf16, got torch.float32
Expected layers.60.ffn_norm.weight to be bf16, got torch.float32
Expected layers.61.attention.wo.weight to be bf16, got torch.float32
Expected layers.67.ffn_norm.weight to be bf16, got torch.float32
Expected layers.63.attention.wo.weight to be bf16, got torch.float32
Expected layers.67.attention.wo.weight to be bf16, got torch.float32

Expected layers.61.attention_norm.weight to be bf16, got torch.float32Expected layers.63.attention.wk.weight to be bf16, got torch.float32
Expected layers.69.attention.wq.weight to be bf16, got torch.float32Expected layers.61.attention.wq.weight to be bf16, got torch.float32

Expected layers.61.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.68.attention.wq.weight to be bf16, got torch.float32
Expected layers.63.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.67.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.61.ffn_norm.weight to be bf16, got torch.float32
Expected layers.63.attention.wv.weight to be bf16, got torch.float32
Expected layers.69.attention.wk.weight to be bf16, got torch.float32
Expected layers.61.attention.wk.weight to be bf16, got torch.float32Expected layers.61.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.68.attention.wk.weight to be bf16, got torch.float32
Expected layers.63.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.67.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.62.attention.wq.weight to be bf16, got torch.float32
Expected layers.63.attention.wo.weight to be bf16, got torch.float32
Expected layers.69.attention.wv.weight to be bf16, got torch.float32
Expected layers.61.attention.wv.weight to be bf16, got torch.float32
Expected layers.61.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.68.attention.wv.weight to be bf16, got torch.float32
Expected layers.63.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.67.feed_forward.w3.weight to be bf16, got torch.float32


Expected layers.62.attention.wk.weight to be bf16, got torch.float32Expected layers.63.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.69.attention.wo.weight to be bf16, got torch.float32
Expected layers.61.attention.wo.weight to be bf16, got torch.float32

Expected layers.61.attention_norm.weight to be bf16, got torch.float32Expected layers.68.attention.wo.weight to be bf16, got torch.float32Expected layers.63.attention_norm.weight to be bf16, got torch.float32
Expected layers.67.attention_norm.weight to be bf16, got torch.float32

Expected layers.62.attention.wv.weight to be bf16, got torch.float32
Expected layers.63.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.69.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.61.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.61.ffn_norm.weight to be bf16, got torch.float32
Expected layers.68.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.63.ffn_norm.weight to be bf16, got torch.float32
Expected layers.67.ffn_norm.weight to be bf16, got torch.float32
Expected layers.62.attention.wo.weight to be bf16, got torch.float32
Expected layers.63.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.69.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.61.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.62.attention.wq.weight to be bf16, got torch.float32
Expected layers.68.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.64.attention.wq.weight to be bf16, got torch.float32
Expected layers.68.attention.wq.weight to be bf16, got torch.float32
Expected layers.62.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.63.attention_norm.weight to be bf16, got torch.float32
Expected layers.69.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.61.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.62.attention.wk.weight to be bf16, got torch.float32Expected layers.68.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.64.attention.wk.weight to be bf16, got torch.float32
Expected layers.68.attention.wk.weight to be bf16, got torch.float32
Expected layers.62.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.63.ffn_norm.weight to be bf16, got torch.float32

Expected layers.69.attention_norm.weight to be bf16, got torch.float32
Expected layers.61.attention_norm.weight to be bf16, got torch.float32
Expected layers.62.attention.wv.weight to be bf16, got torch.float32
Expected layers.68.attention_norm.weight to be bf16, got torch.float32

Expected layers.64.attention.wv.weight to be bf16, got torch.float32Expected layers.68.attention.wv.weight to be bf16, got torch.float32
Expected layers.62.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.64.attention.wq.weight to be bf16, got torch.float32

Expected layers.69.ffn_norm.weight to be bf16, got torch.float32

Expected layers.61.ffn_norm.weight to be bf16, got torch.float32Expected layers.62.attention.wo.weight to be bf16, got torch.float32
Expected layers.68.ffn_norm.weight to be bf16, got torch.float32

Expected layers.64.attention.wo.weight to be bf16, got torch.float32Expected layers.68.attention.wo.weight to be bf16, got torch.float32
Expected layers.62.attention_norm.weight to be bf16, got torch.float32Expected layers.64.attention.wk.weight to be bf16, got torch.float32

Expected layers.70.attention.wq.weight to be bf16, got torch.float32
Expected layers.62.attention.wq.weight to be bf16, got torch.float32
Expected layers.62.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.69.attention.wq.weight to be bf16, got torch.float32

Expected layers.64.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.68.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.62.ffn_norm.weight to be bf16, got torch.float32
Expected layers.64.attention.wv.weight to be bf16, got torch.float32
Expected layers.70.attention.wk.weight to be bf16, got torch.float32

Expected layers.62.attention.wk.weight to be bf16, got torch.float32Expected layers.62.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.69.attention.wk.weight to be bf16, got torch.float32

Expected layers.64.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.68.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.63.attention.wq.weight to be bf16, got torch.float32
Expected layers.64.attention.wo.weight to be bf16, got torch.float32
Expected layers.70.attention.wv.weight to be bf16, got torch.float32
Expected layers.62.attention.wv.weight to be bf16, got torch.float32Expected layers.62.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.69.attention.wv.weight to be bf16, got torch.float32
Expected layers.64.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.68.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.63.attention.wk.weight to be bf16, got torch.float32
Expected layers.64.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.70.attention.wo.weight to be bf16, got torch.float32

Expected layers.62.attention.wo.weight to be bf16, got torch.float32
Expected layers.62.attention_norm.weight to be bf16, got torch.float32
Expected layers.69.attention.wo.weight to be bf16, got torch.float32
Expected layers.64.attention_norm.weight to be bf16, got torch.float32
Expected layers.68.attention_norm.weight to be bf16, got torch.float32
Expected layers.63.attention.wv.weight to be bf16, got torch.float32
Expected layers.64.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.70.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.62.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.62.ffn_norm.weight to be bf16, got torch.float32

Expected layers.69.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.64.ffn_norm.weight to be bf16, got torch.float32Expected layers.68.ffn_norm.weight to be bf16, got torch.float32
Expected layers.63.attention.wo.weight to be bf16, got torch.float32

Expected layers.64.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.70.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.62.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.63.attention.wq.weight to be bf16, got torch.float32

Expected layers.69.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.65.attention.wq.weight to be bf16, got torch.float32Expected layers.69.attention.wq.weight to be bf16, got torch.float32

Expected layers.63.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.64.attention_norm.weight to be bf16, got torch.float32Expected layers.70.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.62.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.63.attention.wk.weight to be bf16, got torch.float32Expected layers.69.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.65.attention.wk.weight to be bf16, got torch.float32
Expected layers.69.attention.wk.weight to be bf16, got torch.float32

Expected layers.63.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.64.ffn_norm.weight to be bf16, got torch.float32
Expected layers.62.attention_norm.weight to be bf16, got torch.float32Expected layers.69.attention_norm.weight to be bf16, got torch.float32
Expected layers.70.attention_norm.weight to be bf16, got torch.float32
Expected layers.63.attention.wv.weight to be bf16, got torch.float32

Expected layers.65.attention.wv.weight to be bf16, got torch.float32
Expected layers.69.attention.wv.weight to be bf16, got torch.float32
Expected layers.63.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.65.attention.wq.weight to be bf16, got torch.float32
Expected layers.62.ffn_norm.weight to be bf16, got torch.float32
Expected layers.69.ffn_norm.weight to be bf16, got torch.float32Expected layers.70.ffn_norm.weight to be bf16, got torch.float32

Expected layers.63.attention.wo.weight to be bf16, got torch.float32
Expected layers.65.attention.wo.weight to be bf16, got torch.float32
Expected layers.69.attention.wo.weight to be bf16, got torch.float32Expected layers.63.attention_norm.weight to be bf16, got torch.float32

Expected layers.65.attention.wk.weight to be bf16, got torch.float32
Expected layers.63.attention.wq.weight to be bf16, got torch.float32
Expected layers.70.attention.wq.weight to be bf16, got torch.float32
Expected layers.71.attention.wq.weight to be bf16, got torch.float32
Expected layers.63.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.65.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.69.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.63.ffn_norm.weight to be bf16, got torch.float32
Expected layers.65.attention.wv.weight to be bf16, got torch.float32
Expected layers.63.attention.wk.weight to be bf16, got torch.float32
Expected layers.70.attention.wk.weight to be bf16, got torch.float32
Expected layers.71.attention.wk.weight to be bf16, got torch.float32
Expected layers.63.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.65.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.69.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.64.attention.wq.weight to be bf16, got torch.float32

Expected layers.65.attention.wo.weight to be bf16, got torch.float32

Expected layers.63.attention.wv.weight to be bf16, got torch.float32Expected layers.70.attention.wv.weight to be bf16, got torch.float32
Expected layers.71.attention.wv.weight to be bf16, got torch.float32
Expected layers.63.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.65.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.69.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.64.attention.wk.weight to be bf16, got torch.float32

Expected layers.65.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.63.attention.wo.weight to be bf16, got torch.float32Expected layers.70.attention.wo.weight to be bf16, got torch.float32
Expected layers.71.attention.wo.weight to be bf16, got torch.float32Expected layers.63.attention_norm.weight to be bf16, got torch.float32
Expected layers.65.attention_norm.weight to be bf16, got torch.float32
Expected layers.69.attention_norm.weight to be bf16, got torch.float32
Expected layers.64.attention.wv.weight to be bf16, got torch.float32

Expected layers.65.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.63.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.70.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.71.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.63.ffn_norm.weight to be bf16, got torch.float32
Expected layers.65.ffn_norm.weight to be bf16, got torch.float32Expected layers.69.ffn_norm.weight to be bf16, got torch.float32
Expected layers.64.attention.wo.weight to be bf16, got torch.float32


Expected layers.65.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.63.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.70.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.71.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.64.attention.wq.weight to be bf16, got torch.float32Expected layers.70.attention.wq.weight to be bf16, got torch.float32Expected layers.66.attention.wq.weight to be bf16, got torch.float32
Expected layers.64.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.70.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.65.attention_norm.weight to be bf16, got torch.float32
Expected layers.63.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.71.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.64.attention.wk.weight to be bf16, got torch.float32Expected layers.70.attention.wk.weight to be bf16, got torch.float32
Expected layers.66.attention.wk.weight to be bf16, got torch.float32

Expected layers.64.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.70.attention_norm.weight to be bf16, got torch.float32
Expected layers.65.ffn_norm.weight to be bf16, got torch.float32
Expected layers.63.attention_norm.weight to be bf16, got torch.float32
Expected layers.71.attention_norm.weight to be bf16, got torch.float32
Expected layers.64.attention.wv.weight to be bf16, got torch.float32
Expected layers.70.attention.wv.weight to be bf16, got torch.float32
Expected layers.66.attention.wv.weight to be bf16, got torch.float32
Expected layers.64.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.70.ffn_norm.weight to be bf16, got torch.float32
Expected layers.66.attention.wq.weight to be bf16, got torch.float32
Expected layers.63.ffn_norm.weight to be bf16, got torch.float32
Expected layers.71.ffn_norm.weight to be bf16, got torch.float32
Expected layers.64.attention.wo.weight to be bf16, got torch.float32
Expected layers.70.attention.wo.weight to be bf16, got torch.float32
Expected layers.66.attention.wo.weight to be bf16, got torch.float32
Expected layers.64.attention_norm.weight to be bf16, got torch.float32
Expected layers.71.attention.wq.weight to be bf16, got torch.float32
Expected layers.66.attention.wk.weight to be bf16, got torch.float32
Expected layers.64.attention.wq.weight to be bf16, got torch.float32
Expected layers.72.attention.wq.weight to be bf16, got torch.float32
Expected layers.64.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.70.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.66.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.64.ffn_norm.weight to be bf16, got torch.float32

Expected layers.71.attention.wk.weight to be bf16, got torch.float32
Expected layers.66.attention.wv.weight to be bf16, got torch.float32
Expected layers.64.attention.wk.weight to be bf16, got torch.float32
Expected layers.72.attention.wk.weight to be bf16, got torch.float32

Expected layers.64.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.70.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.66.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.65.attention.wq.weight to be bf16, got torch.float32

Expected layers.71.attention.wv.weight to be bf16, got torch.float32
Expected layers.66.attention.wo.weight to be bf16, got torch.float32
Expected layers.64.attention.wv.weight to be bf16, got torch.float32
Expected layers.72.attention.wv.weight to be bf16, got torch.float32

Expected layers.64.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.70.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.66.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.65.attention.wk.weight to be bf16, got torch.float32

Expected layers.71.attention.wo.weight to be bf16, got torch.float32
Expected layers.66.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.64.attention.wo.weight to be bf16, got torch.float32
Expected layers.72.attention.wo.weight to be bf16, got torch.float32

Expected layers.64.attention_norm.weight to be bf16, got torch.float32Expected layers.70.attention_norm.weight to be bf16, got torch.float32
Expected layers.66.attention_norm.weight to be bf16, got torch.float32Expected layers.65.attention.wv.weight to be bf16, got torch.float32

Expected layers.71.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.66.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.64.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.72.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.64.ffn_norm.weight to be bf16, got torch.float32Expected layers.70.ffn_norm.weight to be bf16, got torch.float32
Expected layers.66.ffn_norm.weight to be bf16, got torch.float32Expected layers.65.attention.wo.weight to be bf16, got torch.float32
Expected layers.71.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.66.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.64.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.72.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.65.attention.wq.weight to be bf16, got torch.float32Expected layers.71.attention.wq.weight to be bf16, got torch.float32
Expected layers.67.attention.wq.weight to be bf16, got torch.float32
Expected layers.65.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.71.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.66.attention_norm.weight to be bf16, got torch.float32
Expected layers.64.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.72.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.65.attention.wk.weight to be bf16, got torch.float32
Expected layers.71.attention.wk.weight to be bf16, got torch.float32
Expected layers.67.attention.wk.weight to be bf16, got torch.float32
Expected layers.65.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.71.attention_norm.weight to be bf16, got torch.float32
Expected layers.66.ffn_norm.weight to be bf16, got torch.float32
Expected layers.64.attention_norm.weight to be bf16, got torch.float32
Expected layers.72.attention_norm.weight to be bf16, got torch.float32
Expected layers.65.attention.wv.weight to be bf16, got torch.float32
Expected layers.71.attention.wv.weight to be bf16, got torch.float32
Expected layers.67.attention.wv.weight to be bf16, got torch.float32
Expected layers.65.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.71.ffn_norm.weight to be bf16, got torch.float32
Expected layers.72.ffn_norm.weight to be bf16, got torch.float32Expected layers.67.attention.wq.weight to be bf16, got torch.float32
Expected layers.64.ffn_norm.weight to be bf16, got torch.float32

Expected layers.65.attention.wo.weight to be bf16, got torch.float32
Expected layers.71.attention.wo.weight to be bf16, got torch.float32
Expected layers.67.attention.wo.weight to be bf16, got torch.float32
Expected layers.65.attention_norm.weight to be bf16, got torch.float32
Expected layers.72.attention.wq.weight to be bf16, got torch.float32
Expected layers.65.attention.wq.weight to be bf16, got torch.float32Expected layers.73.attention.wq.weight to be bf16, got torch.float32
Expected layers.67.attention.wk.weight to be bf16, got torch.float32

Expected layers.65.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.71.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.67.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.65.ffn_norm.weight to be bf16, got torch.float32
Expected layers.72.attention.wk.weight to be bf16, got torch.float32
Expected layers.65.attention.wk.weight to be bf16, got torch.float32Expected layers.73.attention.wk.weight to be bf16, got torch.float32

Expected layers.67.attention.wv.weight to be bf16, got torch.float32

Expected layers.65.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.71.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.67.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.66.attention.wq.weight to be bf16, got torch.float32
Expected layers.72.attention.wv.weight to be bf16, got torch.float32
Expected layers.65.attention.wv.weight to be bf16, got torch.float32Expected layers.73.attention.wv.weight to be bf16, got torch.float32

Expected layers.67.attention.wo.weight to be bf16, got torch.float32

Expected layers.65.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.66.attention.wk.weight to be bf16, got torch.float32Expected layers.71.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.67.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.72.attention.wo.weight to be bf16, got torch.float32

Expected layers.65.attention.wo.weight to be bf16, got torch.float32Expected layers.73.attention.wo.weight to be bf16, got torch.float32

Expected layers.67.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.65.attention_norm.weight to be bf16, got torch.float32
Expected layers.66.attention.wv.weight to be bf16, got torch.float32
Expected layers.71.attention_norm.weight to be bf16, got torch.float32
Expected layers.67.attention_norm.weight to be bf16, got torch.float32Expected layers.72.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.65.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.73.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.67.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.65.ffn_norm.weight to be bf16, got torch.float32
Expected layers.66.attention.wo.weight to be bf16, got torch.float32
Expected layers.71.ffn_norm.weight to be bf16, got torch.float32
Expected layers.67.ffn_norm.weight to be bf16, got torch.float32
Expected layers.72.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.65.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.73.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.67.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.66.attention.wq.weight to be bf16, got torch.float32
Expected layers.68.attention.wq.weight to be bf16, got torch.float32Expected layers.66.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.72.attention.wq.weight to be bf16, got torch.float32

Expected layers.72.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.65.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.73.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.68.attention.wk.weight to be bf16, got torch.float32Expected layers.67.attention_norm.weight to be bf16, got torch.float32
Expected layers.66.attention.wk.weight to be bf16, got torch.float32

Expected layers.66.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.72.attention.wk.weight to be bf16, got torch.float32Expected layers.72.attention_norm.weight to be bf16, got torch.float32
Expected layers.65.attention_norm.weight to be bf16, got torch.float32
Expected layers.73.attention_norm.weight to be bf16, got torch.float32
Expected layers.68.attention.wv.weight to be bf16, got torch.float32
Expected layers.67.ffn_norm.weight to be bf16, got torch.float32Expected layers.66.attention.wv.weight to be bf16, got torch.float32

Expected layers.66.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.72.attention.wv.weight to be bf16, got torch.float32Expected layers.72.ffn_norm.weight to be bf16, got torch.float32
Expected layers.65.ffn_norm.weight to be bf16, got torch.float32

Expected layers.73.ffn_norm.weight to be bf16, got torch.float32Expected layers.68.attention.wo.weight to be bf16, got torch.float32
Expected layers.68.attention.wq.weight to be bf16, got torch.float32Expected layers.66.attention.wo.weight to be bf16, got torch.float32

Expected layers.66.attention_norm.weight to be bf16, got torch.float32

Expected layers.72.attention.wo.weight to be bf16, got torch.float32Expected layers.73.attention.wq.weight to be bf16, got torch.float32
Expected layers.66.attention.wq.weight to be bf16, got torch.float32
Expected layers.74.attention.wq.weight to be bf16, got torch.float32
Expected layers.68.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.68.attention.wk.weight to be bf16, got torch.float32Expected layers.66.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.66.ffn_norm.weight to be bf16, got torch.float32

Expected layers.72.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.73.attention.wk.weight to be bf16, got torch.float32
Expected layers.66.attention.wk.weight to be bf16, got torch.float32
Expected layers.74.attention.wk.weight to be bf16, got torch.float32
Expected layers.68.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.68.attention.wv.weight to be bf16, got torch.float32
Expected layers.66.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.67.attention.wq.weight to be bf16, got torch.float32

Expected layers.72.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.73.attention.wv.weight to be bf16, got torch.float32
Expected layers.66.attention.wv.weight to be bf16, got torch.float32
Expected layers.74.attention.wv.weight to be bf16, got torch.float32
Expected layers.68.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.68.attention.wo.weight to be bf16, got torch.float32
Expected layers.66.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.67.attention.wk.weight to be bf16, got torch.float32
Expected layers.72.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.74.attention.wo.weight to be bf16, got torch.float32
Expected layers.73.attention.wo.weight to be bf16, got torch.float32Expected layers.66.attention.wo.weight to be bf16, got torch.float32


Expected layers.68.attention_norm.weight to be bf16, got torch.float32

Expected layers.68.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.66.attention_norm.weight to be bf16, got torch.float32
Expected layers.67.attention.wv.weight to be bf16, got torch.float32
Expected layers.72.attention_norm.weight to be bf16, got torch.float32
Expected layers.74.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.73.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.66.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.68.ffn_norm.weight to be bf16, got torch.float32
Expected layers.68.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.66.ffn_norm.weight to be bf16, got torch.float32
Expected layers.67.attention.wo.weight to be bf16, got torch.float32

Expected layers.72.ffn_norm.weight to be bf16, got torch.float32Expected layers.74.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.73.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.66.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.69.attention.wq.weight to be bf16, got torch.float32
Expected layers.68.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.67.attention.wq.weight to be bf16, got torch.float32
Expected layers.67.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.73.attention.wq.weight to be bf16, got torch.float32
Expected layers.74.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.73.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.66.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.69.attention.wk.weight to be bf16, got torch.float32Expected layers.68.attention_norm.weight to be bf16, got torch.float32

Expected layers.67.attention.wk.weight to be bf16, got torch.float32
Expected layers.67.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.73.attention.wk.weight to be bf16, got torch.float32Expected layers.74.attention_norm.weight to be bf16, got torch.float32

Expected layers.73.attention_norm.weight to be bf16, got torch.float32Expected layers.66.attention_norm.weight to be bf16, got torch.float32

Expected layers.69.attention.wv.weight to be bf16, got torch.float32
Expected layers.68.ffn_norm.weight to be bf16, got torch.float32

Expected layers.67.attention.wv.weight to be bf16, got torch.float32Expected layers.67.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.73.attention.wv.weight to be bf16, got torch.float32Expected layers.74.ffn_norm.weight to be bf16, got torch.float32
Expected layers.73.ffn_norm.weight to be bf16, got torch.float32

Expected layers.66.ffn_norm.weight to be bf16, got torch.float32
Expected layers.69.attention.wo.weight to be bf16, got torch.float32
Expected layers.69.attention.wq.weight to be bf16, got torch.float32
Expected layers.67.attention.wo.weight to be bf16, got torch.float32
Expected layers.67.attention_norm.weight to be bf16, got torch.float32Expected layers.73.attention.wo.weight to be bf16, got torch.float32
Expected layers.75.attention.wq.weight to be bf16, got torch.float32
Expected layers.74.attention.wq.weight to be bf16, got torch.float32

Expected layers.67.attention.wq.weight to be bf16, got torch.float32
Expected layers.69.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.69.attention.wk.weight to be bf16, got torch.float32
Expected layers.73.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.67.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.67.ffn_norm.weight to be bf16, got torch.float32

Expected layers.75.attention.wk.weight to be bf16, got torch.float32
Expected layers.74.attention.wk.weight to be bf16, got torch.float32
Expected layers.67.attention.wk.weight to be bf16, got torch.float32
Expected layers.69.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.69.attention.wv.weight to be bf16, got torch.float32
Expected layers.73.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.67.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.68.attention.wq.weight to be bf16, got torch.float32
Expected layers.75.attention.wv.weight to be bf16, got torch.float32
Expected layers.74.attention.wv.weight to be bf16, got torch.float32
Expected layers.67.attention.wv.weight to be bf16, got torch.float32
Expected layers.69.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.69.attention.wo.weight to be bf16, got torch.float32
Expected layers.73.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.67.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.68.attention.wk.weight to be bf16, got torch.float32
Expected layers.75.attention.wo.weight to be bf16, got torch.float32
Expected layers.74.attention.wo.weight to be bf16, got torch.float32
Expected layers.67.attention.wo.weight to be bf16, got torch.float32
Expected layers.69.attention_norm.weight to be bf16, got torch.float32
Expected layers.69.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.73.attention_norm.weight to be bf16, got torch.float32
Expected layers.67.attention_norm.weight to be bf16, got torch.float32
Expected layers.68.attention.wv.weight to be bf16, got torch.float32
Expected layers.75.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.74.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.67.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.69.ffn_norm.weight to be bf16, got torch.float32
Expected layers.69.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.73.ffn_norm.weight to be bf16, got torch.float32
Expected layers.67.ffn_norm.weight to be bf16, got torch.float32
Expected layers.68.attention.wo.weight to be bf16, got torch.float32
Expected layers.75.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.74.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.67.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.70.attention.wq.weight to be bf16, got torch.float32
Expected layers.69.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.74.attention.wq.weight to be bf16, got torch.float32
Expected layers.68.attention.wq.weight to be bf16, got torch.float32
Expected layers.68.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.75.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.74.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.67.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.70.attention.wk.weight to be bf16, got torch.float32
Expected layers.69.attention_norm.weight to be bf16, got torch.float32
Expected layers.74.attention.wk.weight to be bf16, got torch.float32
Expected layers.68.attention.wk.weight to be bf16, got torch.float32
Expected layers.68.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.75.attention_norm.weight to be bf16, got torch.float32
Expected layers.74.attention_norm.weight to be bf16, got torch.float32Expected layers.67.attention_norm.weight to be bf16, got torch.float32
Expected layers.70.attention.wv.weight to be bf16, got torch.float32
Expected layers.69.ffn_norm.weight to be bf16, got torch.float32

Expected layers.74.attention.wv.weight to be bf16, got torch.float32
Expected layers.68.attention.wv.weight to be bf16, got torch.float32
Expected layers.68.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.75.ffn_norm.weight to be bf16, got torch.float32
Expected layers.74.ffn_norm.weight to be bf16, got torch.float32
Expected layers.67.ffn_norm.weight to be bf16, got torch.float32
Expected layers.70.attention.wo.weight to be bf16, got torch.float32
Expected layers.70.attention.wq.weight to be bf16, got torch.float32
Expected layers.74.attention.wo.weight to be bf16, got torch.float32
Expected layers.68.attention.wo.weight to be bf16, got torch.float32
Expected layers.68.attention_norm.weight to be bf16, got torch.float32
Expected layers.76.attention.wq.weight to be bf16, got torch.float32
Expected layers.75.attention.wq.weight to be bf16, got torch.float32
Expected layers.68.attention.wq.weight to be bf16, got torch.float32
Expected layers.70.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.70.attention.wk.weight to be bf16, got torch.float32
Expected layers.74.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.68.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.68.ffn_norm.weight to be bf16, got torch.float32
Expected layers.76.attention.wk.weight to be bf16, got torch.float32
Expected layers.75.attention.wk.weight to be bf16, got torch.float32
Expected layers.68.attention.wk.weight to be bf16, got torch.float32
Expected layers.70.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.70.attention.wv.weight to be bf16, got torch.float32
Expected layers.74.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.68.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.69.attention.wq.weight to be bf16, got torch.float32
Expected layers.76.attention.wv.weight to be bf16, got torch.float32
Expected layers.75.attention.wv.weight to be bf16, got torch.float32
Expected layers.68.attention.wv.weight to be bf16, got torch.float32
Expected layers.70.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.70.attention.wo.weight to be bf16, got torch.float32
Expected layers.74.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.68.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.69.attention.wk.weight to be bf16, got torch.float32
Expected layers.76.attention.wo.weight to be bf16, got torch.float32
Expected layers.75.attention.wo.weight to be bf16, got torch.float32
Expected layers.68.attention.wo.weight to be bf16, got torch.float32
Expected layers.70.attention_norm.weight to be bf16, got torch.float32Expected layers.70.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.74.attention_norm.weight to be bf16, got torch.float32
Expected layers.68.attention_norm.weight to be bf16, got torch.float32
Expected layers.69.attention.wv.weight to be bf16, got torch.float32
Expected layers.76.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.75.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.68.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.70.ffn_norm.weight to be bf16, got torch.float32
Expected layers.70.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.74.ffn_norm.weight to be bf16, got torch.float32
Expected layers.68.ffn_norm.weight to be bf16, got torch.float32
Expected layers.69.attention.wo.weight to be bf16, got torch.float32Expected layers.76.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.75.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.68.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.71.attention.wq.weight to be bf16, got torch.float32
Expected layers.70.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.75.attention.wq.weight to be bf16, got torch.float32
Expected layers.69.attention.wq.weight to be bf16, got torch.float32
Expected layers.69.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.76.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.75.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.68.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.71.attention.wk.weight to be bf16, got torch.float32
Expected layers.70.attention_norm.weight to be bf16, got torch.float32
Expected layers.75.attention.wk.weight to be bf16, got torch.float32
Expected layers.69.attention.wk.weight to be bf16, got torch.float32
Expected layers.69.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.76.attention_norm.weight to be bf16, got torch.float32
Expected layers.75.attention_norm.weight to be bf16, got torch.float32
Expected layers.68.attention_norm.weight to be bf16, got torch.float32
Expected layers.71.attention.wv.weight to be bf16, got torch.float32
Expected layers.70.ffn_norm.weight to be bf16, got torch.float32
Expected layers.75.attention.wv.weight to be bf16, got torch.float32
Expected layers.69.attention.wv.weight to be bf16, got torch.float32Expected layers.69.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.76.ffn_norm.weight to be bf16, got torch.float32

Expected layers.75.ffn_norm.weight to be bf16, got torch.float32
Expected layers.68.ffn_norm.weight to be bf16, got torch.float32
Expected layers.71.attention.wo.weight to be bf16, got torch.float32
Expected layers.71.attention.wq.weight to be bf16, got torch.float32
Expected layers.75.attention.wo.weight to be bf16, got torch.float32
Expected layers.69.attention.wo.weight to be bf16, got torch.float32
Expected layers.69.attention_norm.weight to be bf16, got torch.float32
Expected layers.77.attention.wq.weight to be bf16, got torch.float32
Expected layers.76.attention.wq.weight to be bf16, got torch.float32
Expected layers.69.attention.wq.weight to be bf16, got torch.float32
Expected layers.71.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.71.attention.wk.weight to be bf16, got torch.float32
Expected layers.75.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.69.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.69.ffn_norm.weight to be bf16, got torch.float32
Expected layers.77.attention.wk.weight to be bf16, got torch.float32
Expected layers.76.attention.wk.weight to be bf16, got torch.float32
Expected layers.69.attention.wk.weight to be bf16, got torch.float32
Expected layers.71.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.71.attention.wv.weight to be bf16, got torch.float32
Expected layers.75.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.69.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.70.attention.wq.weight to be bf16, got torch.float32
Expected layers.77.attention.wv.weight to be bf16, got torch.float32
Expected layers.76.attention.wv.weight to be bf16, got torch.float32
Expected layers.69.attention.wv.weight to be bf16, got torch.float32
Expected layers.71.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.71.attention.wo.weight to be bf16, got torch.float32
Expected layers.75.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.69.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.70.attention.wk.weight to be bf16, got torch.float32
Expected layers.77.attention.wo.weight to be bf16, got torch.float32
Expected layers.76.attention.wo.weight to be bf16, got torch.float32
Expected layers.69.attention.wo.weight to be bf16, got torch.float32
Expected layers.71.attention_norm.weight to be bf16, got torch.float32
Expected layers.71.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.75.attention_norm.weight to be bf16, got torch.float32
Expected layers.69.attention_norm.weight to be bf16, got torch.float32Expected layers.70.attention.wv.weight to be bf16, got torch.float32
Expected layers.77.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.76.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.69.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.71.ffn_norm.weight to be bf16, got torch.float32


Expected layers.75.ffn_norm.weight to be bf16, got torch.float32Expected layers.71.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.69.ffn_norm.weight to be bf16, got torch.float32Expected layers.70.attention.wo.weight to be bf16, got torch.float32
Expected layers.77.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.76.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.69.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.72.attention.wq.weight to be bf16, got torch.float32

Expected layers.76.attention.wq.weight to be bf16, got torch.float32

Expected layers.71.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.70.attention.wq.weight to be bf16, got torch.float32
Expected layers.70.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.77.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.76.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.69.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.72.attention.wk.weight to be bf16, got torch.float32

Expected layers.76.attention.wk.weight to be bf16, got torch.float32
Expected layers.71.attention_norm.weight to be bf16, got torch.float32

Expected layers.70.attention.wk.weight to be bf16, got torch.float32Expected layers.70.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.77.attention_norm.weight to be bf16, got torch.float32Expected layers.76.attention_norm.weight to be bf16, got torch.float32

Expected layers.69.attention_norm.weight to be bf16, got torch.float32
Expected layers.71.ffn_norm.weight to be bf16, got torch.float32Expected layers.72.attention.wv.weight to be bf16, got torch.float32
Expected layers.76.attention.wv.weight to be bf16, got torch.float32


Expected layers.70.attention.wv.weight to be bf16, got torch.float32Expected layers.70.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.77.ffn_norm.weight to be bf16, got torch.float32
Expected layers.76.ffn_norm.weight to be bf16, got torch.float32
Expected layers.69.ffn_norm.weight to be bf16, got torch.float32
Expected layers.72.attention.wq.weight to be bf16, got torch.float32Expected layers.72.attention.wo.weight to be bf16, got torch.float32

Expected layers.76.attention.wo.weight to be bf16, got torch.float32
Expected layers.70.attention.wo.weight to be bf16, got torch.float32
Expected layers.70.attention_norm.weight to be bf16, got torch.float32
Expected layers.78.attention.wq.weight to be bf16, got torch.float32Expected layers.77.attention.wq.weight to be bf16, got torch.float32
Expected layers.70.attention.wq.weight to be bf16, got torch.float32
Expected layers.72.attention.wk.weight to be bf16, got torch.float32

Expected layers.72.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.76.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.70.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.70.ffn_norm.weight to be bf16, got torch.float32
Expected layers.78.attention.wk.weight to be bf16, got torch.float32Expected layers.77.attention.wk.weight to be bf16, got torch.float32

Expected layers.70.attention.wk.weight to be bf16, got torch.float32
Expected layers.72.attention.wv.weight to be bf16, got torch.float32
Expected layers.72.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.76.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.70.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.71.attention.wq.weight to be bf16, got torch.float32
Expected layers.78.attention.wv.weight to be bf16, got torch.float32

Expected layers.77.attention.wv.weight to be bf16, got torch.float32
Expected layers.70.attention.wv.weight to be bf16, got torch.float32
Expected layers.72.attention.wo.weight to be bf16, got torch.float32
Expected layers.72.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.76.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.70.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.71.attention.wk.weight to be bf16, got torch.float32
Expected layers.78.attention.wo.weight to be bf16, got torch.float32
Expected layers.77.attention.wo.weight to be bf16, got torch.float32Expected layers.70.attention.wo.weight to be bf16, got torch.float32

Expected layers.72.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.72.attention_norm.weight to be bf16, got torch.float32
Expected layers.76.attention_norm.weight to be bf16, got torch.float32Expected layers.70.attention_norm.weight to be bf16, got torch.float32

Expected layers.71.attention.wv.weight to be bf16, got torch.float32Expected layers.70.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.78.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.77.feed_forward.w1.weight to be bf16, got torch.float32


Expected layers.72.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.72.ffn_norm.weight to be bf16, got torch.float32
Expected layers.76.ffn_norm.weight to be bf16, got torch.float32
Expected layers.70.ffn_norm.weight to be bf16, got torch.float32
Expected layers.71.attention.wo.weight to be bf16, got torch.float32
Expected layers.70.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.78.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.77.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.72.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.73.attention.wq.weight to be bf16, got torch.float32Expected layers.77.attention.wq.weight to be bf16, got torch.float32

Expected layers.71.attention.wq.weight to be bf16, got torch.float32
Expected layers.71.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.70.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.78.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.77.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.72.attention_norm.weight to be bf16, got torch.float32
Expected layers.73.attention.wk.weight to be bf16, got torch.float32
Expected layers.77.attention.wk.weight to be bf16, got torch.float32
Expected layers.71.attention.wk.weight to be bf16, got torch.float32
Expected layers.71.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.70.attention_norm.weight to be bf16, got torch.float32
Expected layers.78.attention_norm.weight to be bf16, got torch.float32
Expected layers.77.attention_norm.weight to be bf16, got torch.float32
Expected layers.72.ffn_norm.weight to be bf16, got torch.float32
Expected layers.73.attention.wv.weight to be bf16, got torch.float32
Expected layers.77.attention.wv.weight to be bf16, got torch.float32

Expected layers.71.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.71.attention.wv.weight to be bf16, got torch.float32
Expected layers.70.ffn_norm.weight to be bf16, got torch.float32Expected layers.78.ffn_norm.weight to be bf16, got torch.float32
Expected layers.77.ffn_norm.weight to be bf16, got torch.float32
Expected layers.73.attention.wq.weight to be bf16, got torch.float32
Expected layers.73.attention.wo.weight to be bf16, got torch.float32

Expected layers.77.attention.wo.weight to be bf16, got torch.float32

Expected layers.71.attention_norm.weight to be bf16, got torch.float32
Expected layers.71.attention.wo.weight to be bf16, got torch.float32Expected layers.71.attention.wq.weight to be bf16, got torch.float32
Expected layers.79.attention.wq.weight to be bf16, got torch.float32Expected layers.78.attention.wq.weight to be bf16, got torch.float32

Expected layers.73.attention.wk.weight to be bf16, got torch.float32Expected layers.73.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.77.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.71.ffn_norm.weight to be bf16, got torch.float32


Expected layers.71.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.71.attention.wk.weight to be bf16, got torch.float32Expected layers.78.attention.wk.weight to be bf16, got torch.float32
Expected layers.79.attention.wk.weight to be bf16, got torch.float32


Expected layers.73.attention.wv.weight to be bf16, got torch.float32Expected layers.73.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.77.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.72.attention.wq.weight to be bf16, got torch.float32


Expected layers.71.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.71.attention.wv.weight to be bf16, got torch.float32
Expected layers.78.attention.wv.weight to be bf16, got torch.float32
Expected layers.79.attention.wv.weight to be bf16, got torch.float32
Expected layers.73.attention.wo.weight to be bf16, got torch.float32
Expected layers.73.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.77.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.72.attention.wk.weight to be bf16, got torch.float32

Expected layers.71.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.71.attention.wo.weight to be bf16, got torch.float32
Expected layers.78.attention.wo.weight to be bf16, got torch.float32
Expected layers.79.attention.wo.weight to be bf16, got torch.float32
Expected layers.73.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.73.attention_norm.weight to be bf16, got torch.float32Expected layers.77.attention_norm.weight to be bf16, got torch.float32
Expected layers.72.attention.wv.weight to be bf16, got torch.float32

Expected layers.71.attention_norm.weight to be bf16, got torch.float32

Expected layers.71.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.78.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.79.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.73.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.73.ffn_norm.weight to be bf16, got torch.float32
Expected layers.77.ffn_norm.weight to be bf16, got torch.float32
Expected layers.72.attention.wo.weight to be bf16, got torch.float32

Expected layers.71.ffn_norm.weight to be bf16, got torch.float32

Expected layers.71.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.78.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.79.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.73.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.74.attention.wq.weight to be bf16, got torch.float32
Expected layers.78.attention.wq.weight to be bf16, got torch.float32
Expected layers.72.feed_forward.w1.weight to be bf16, got torch.float32


Expected layers.72.attention.wq.weight to be bf16, got torch.float32
Expected layers.71.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.78.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.79.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.73.attention_norm.weight to be bf16, got torch.float32
Expected layers.74.attention.wk.weight to be bf16, got torch.float32Expected layers.78.attention.wk.weight to be bf16, got torch.float32
Expected layers.72.feed_forward.w2.weight to be bf16, got torch.float32


Expected layers.72.attention.wk.weight to be bf16, got torch.float32
Expected layers.71.attention_norm.weight to be bf16, got torch.float32Expected layers.78.attention_norm.weight to be bf16, got torch.float32
Expected layers.79.attention_norm.weight to be bf16, got torch.float32
Expected layers.73.ffn_norm.weight to be bf16, got torch.float32
Expected layers.74.attention.wv.weight to be bf16, got torch.float32Expected layers.78.attention.wv.weight to be bf16, got torch.float32
Expected layers.72.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.72.attention.wv.weight to be bf16, got torch.float32

Expected layers.71.ffn_norm.weight to be bf16, got torch.float32
Expected layers.78.ffn_norm.weight to be bf16, got torch.float32
Expected layers.79.ffn_norm.weight to be bf16, got torch.float32Expected layers.78.attention.wo.weight to be bf16, got torch.float32
Expected layers.74.attention.wq.weight to be bf16, got torch.float32
Expected layers.74.attention.wo.weight to be bf16, got torch.float32

Expected layers.72.attention_norm.weight to be bf16, got torch.float32
Expected layers.72.attention.wo.weight to be bf16, got torch.float32

Expected layers.72.attention.wq.weight to be bf16, got torch.float32
Expected layers.79.attention.wq.weight to be bf16, got torch.float32Expected norm.weight to be bf16, got torch.float32Expected layers.78.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.74.attention.wk.weight to be bf16, got torch.float32

Expected layers.74.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.72.ffn_norm.weight to be bf16, got torch.float32
Expected layers.72.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.72.attention.wk.weight to be bf16, got torch.float32
Expected layers.79.attention.wk.weight to be bf16, got torch.float32Expected output.weight to be bf16, got torch.float32Expected layers.78.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.74.attention.wv.weight to be bf16, got torch.float32


Expected layers.74.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.73.attention.wq.weight to be bf16, got torch.float32
Expected layers.72.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.72.attention.wv.weight to be bf16, got torch.float32
Expected layers.79.attention.wv.weight to be bf16, got torch.float32Expected layers.78.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.74.attention.wo.weight to be bf16, got torch.float32

Expected layers.72.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.74.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.73.attention.wk.weight to be bf16, got torch.float32


Expected layers.72.attention.wo.weight to be bf16, got torch.float32
Expected layers.79.attention.wo.weight to be bf16, got torch.float32Expected layers.78.attention_norm.weight to be bf16, got torch.float32

Expected layers.74.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.72.attention_norm.weight to be bf16, got torch.float32Expected layers.74.attention_norm.weight to be bf16, got torch.float32
Expected layers.73.attention.wv.weight to be bf16, got torch.float32
Expected layers.72.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.79.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.78.ffn_norm.weight to be bf16, got torch.float32

Expected layers.74.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.72.ffn_norm.weight to be bf16, got torch.float32Expected layers.74.ffn_norm.weight to be bf16, got torch.float32
Expected layers.73.attention.wo.weight to be bf16, got torch.float32
Expected layers.72.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.79.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.79.attention.wq.weight to be bf16, got torch.float32

Expected layers.74.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.73.attention.wq.weight to be bf16, got torch.float32Expected layers.75.attention.wq.weight to be bf16, got torch.float32
Expected layers.73.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.72.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.79.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.79.attention.wk.weight to be bf16, got torch.float32

Expected layers.74.attention_norm.weight to be bf16, got torch.float32Expected layers.73.attention.wk.weight to be bf16, got torch.float32
Expected layers.75.attention.wk.weight to be bf16, got torch.float32
Expected layers.73.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.72.attention_norm.weight to be bf16, got torch.float32
Expected layers.79.attention_norm.weight to be bf16, got torch.float32

Expected layers.79.attention.wv.weight to be bf16, got torch.float32
Expected layers.74.ffn_norm.weight to be bf16, got torch.float32Expected layers.73.attention.wv.weight to be bf16, got torch.float32
Expected layers.75.attention.wv.weight to be bf16, got torch.float32

Expected layers.73.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.72.ffn_norm.weight to be bf16, got torch.float32
Expected layers.79.ffn_norm.weight to be bf16, got torch.float32

Expected layers.79.attention.wo.weight to be bf16, got torch.float32
Expected layers.75.attention.wq.weight to be bf16, got torch.float32Expected layers.73.attention.wo.weight to be bf16, got torch.float32
Expected layers.75.attention.wo.weight to be bf16, got torch.float32

Expected layers.73.attention_norm.weight to be bf16, got torch.float32Expected layers.73.attention.wq.weight to be bf16, got torch.float32
Expected norm.weight to be bf16, got torch.float32

Expected layers.79.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.75.attention.wk.weight to be bf16, got torch.float32Expected layers.73.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.75.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.73.ffn_norm.weight to be bf16, got torch.float32Expected layers.73.attention.wk.weight to be bf16, got torch.float32
Expected output.weight to be bf16, got torch.float32

Expected layers.79.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.75.attention.wv.weight to be bf16, got torch.float32Expected layers.73.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.75.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.74.attention.wq.weight to be bf16, got torch.float32Expected layers.73.attention.wv.weight to be bf16, got torch.float32


Expected layers.79.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.75.attention.wo.weight to be bf16, got torch.float32Expected layers.73.feed_forward.w3.weight to be bf16, got torch.float32


Expected layers.75.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.74.attention.wk.weight to be bf16, got torch.float32Expected layers.73.attention.wo.weight to be bf16, got torch.float32

Expected layers.79.attention_norm.weight to be bf16, got torch.float32
Expected layers.75.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.73.attention_norm.weight to be bf16, got torch.float32


Expected layers.75.attention_norm.weight to be bf16, got torch.float32Expected layers.74.attention.wv.weight to be bf16, got torch.float32Expected layers.73.feed_forward.w1.weight to be bf16, got torch.float32


Expected layers.79.ffn_norm.weight to be bf16, got torch.float32Expected layers.75.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.73.ffn_norm.weight to be bf16, got torch.float32Expected layers.75.ffn_norm.weight to be bf16, got torch.float32


Expected layers.74.attention.wo.weight to be bf16, got torch.float32
Expected layers.73.feed_forward.w2.weight to be bf16, got torch.float32

Expected norm.weight to be bf16, got torch.float32Expected layers.75.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.74.attention.wq.weight to be bf16, got torch.float32

Expected layers.76.attention.wq.weight to be bf16, got torch.float32
Expected layers.74.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.73.feed_forward.w3.weight to be bf16, got torch.float32


Expected output.weight to be bf16, got torch.float32Expected layers.75.attention_norm.weight to be bf16, got torch.float32Expected layers.74.attention.wk.weight to be bf16, got torch.float32


Expected layers.74.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.73.attention_norm.weight to be bf16, got torch.float32Expected layers.76.attention.wk.weight to be bf16, got torch.float32


Expected layers.75.ffn_norm.weight to be bf16, got torch.float32Expected layers.74.attention.wv.weight to be bf16, got torch.float32
Expected layers.74.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.73.ffn_norm.weight to be bf16, got torch.float32
Expected layers.76.attention.wv.weight to be bf16, got torch.float32
Expected layers.76.attention.wq.weight to be bf16, got torch.float32

Expected layers.74.attention.wo.weight to be bf16, got torch.float32Expected layers.74.attention_norm.weight to be bf16, got torch.float32

Expected layers.74.attention.wq.weight to be bf16, got torch.float32Expected layers.76.attention.wo.weight to be bf16, got torch.float32
Expected layers.76.attention.wk.weight to be bf16, got torch.float32

Expected layers.74.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.74.ffn_norm.weight to be bf16, got torch.float32

Expected layers.74.attention.wk.weight to be bf16, got torch.float32Expected layers.76.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.76.attention.wv.weight to be bf16, got torch.float32

Expected layers.74.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.75.attention.wq.weight to be bf16, got torch.float32

Expected layers.74.attention.wv.weight to be bf16, got torch.float32Expected layers.76.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.76.attention.wo.weight to be bf16, got torch.float32

Expected layers.74.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.75.attention.wk.weight to be bf16, got torch.float32

Expected layers.74.attention.wo.weight to be bf16, got torch.float32Expected layers.76.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.76.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.74.attention_norm.weight to be bf16, got torch.float32Expected layers.75.attention.wv.weight to be bf16, got torch.float32

Expected layers.74.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.76.attention_norm.weight to be bf16, got torch.float32
Expected layers.76.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.74.ffn_norm.weight to be bf16, got torch.float32Expected layers.75.attention.wo.weight to be bf16, got torch.float32

Expected layers.74.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.76.ffn_norm.weight to be bf16, got torch.float32
Expected layers.76.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.75.attention.wq.weight to be bf16, got torch.float32Expected layers.75.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.74.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.77.attention.wq.weight to be bf16, got torch.float32
Expected layers.76.attention_norm.weight to be bf16, got torch.float32

Expected layers.75.attention.wk.weight to be bf16, got torch.float32Expected layers.75.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.74.attention_norm.weight to be bf16, got torch.float32Expected layers.77.attention.wk.weight to be bf16, got torch.float32
Expected layers.76.ffn_norm.weight to be bf16, got torch.float32

Expected layers.75.attention.wv.weight to be bf16, got torch.float32Expected layers.75.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.74.ffn_norm.weight to be bf16, got torch.float32
Expected layers.77.attention.wv.weight to be bf16, got torch.float32Expected layers.77.attention.wq.weight to be bf16, got torch.float32

Expected layers.75.attention.wo.weight to be bf16, got torch.float32Expected layers.75.attention_norm.weight to be bf16, got torch.float32Expected layers.75.attention.wq.weight to be bf16, got torch.float32


Expected layers.77.attention.wo.weight to be bf16, got torch.float32Expected layers.77.attention.wk.weight to be bf16, got torch.float32

Expected layers.75.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.75.ffn_norm.weight to be bf16, got torch.float32Expected layers.75.attention.wk.weight to be bf16, got torch.float32


Expected layers.77.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.77.attention.wv.weight to be bf16, got torch.float32

Expected layers.75.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.76.attention.wq.weight to be bf16, got torch.float32Expected layers.75.attention.wv.weight to be bf16, got torch.float32


Expected layers.77.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.77.attention.wo.weight to be bf16, got torch.float32

Expected layers.75.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.76.attention.wk.weight to be bf16, got torch.float32
Expected layers.75.attention.wo.weight to be bf16, got torch.float32

Expected layers.77.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.77.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.75.attention_norm.weight to be bf16, got torch.float32Expected layers.76.attention.wv.weight to be bf16, got torch.float32Expected layers.75.feed_forward.w1.weight to be bf16, got torch.float32


Expected layers.77.attention_norm.weight to be bf16, got torch.float32Expected layers.77.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.75.ffn_norm.weight to be bf16, got torch.float32Expected layers.76.attention.wo.weight to be bf16, got torch.float32
Expected layers.75.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.77.ffn_norm.weight to be bf16, got torch.float32Expected layers.77.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.76.attention.wq.weight to be bf16, got torch.float32Expected layers.76.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.75.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.78.attention.wq.weight to be bf16, got torch.float32Expected layers.77.attention_norm.weight to be bf16, got torch.float32

Expected layers.76.attention.wk.weight to be bf16, got torch.float32Expected layers.76.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.75.attention_norm.weight to be bf16, got torch.float32

Expected layers.78.attention.wk.weight to be bf16, got torch.float32Expected layers.77.ffn_norm.weight to be bf16, got torch.float32

Expected layers.76.attention.wv.weight to be bf16, got torch.float32Expected layers.76.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.75.ffn_norm.weight to be bf16, got torch.float32

Expected layers.78.attention.wv.weight to be bf16, got torch.float32Expected layers.78.attention.wq.weight to be bf16, got torch.float32

Expected layers.76.attention.wo.weight to be bf16, got torch.float32Expected layers.76.attention_norm.weight to be bf16, got torch.float32
Expected layers.76.attention.wq.weight to be bf16, got torch.float32

Expected layers.78.attention.wo.weight to be bf16, got torch.float32Expected layers.78.attention.wk.weight to be bf16, got torch.float32

Expected layers.76.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.76.ffn_norm.weight to be bf16, got torch.float32
Expected layers.76.attention.wk.weight to be bf16, got torch.float32

Expected layers.78.attention.wv.weight to be bf16, got torch.float32Expected layers.78.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.76.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.77.attention.wq.weight to be bf16, got torch.float32
Expected layers.76.attention.wv.weight to be bf16, got torch.float32

Expected layers.78.attention.wo.weight to be bf16, got torch.float32Expected layers.78.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.76.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.77.attention.wk.weight to be bf16, got torch.float32Expected layers.76.attention.wo.weight to be bf16, got torch.float32


Expected layers.78.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.78.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.76.attention_norm.weight to be bf16, got torch.float32Expected layers.77.attention.wv.weight to be bf16, got torch.float32Expected layers.76.feed_forward.w1.weight to be bf16, got torch.float32


Expected layers.78.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.78.attention_norm.weight to be bf16, got torch.float32RV: [REF] missing [] unexpected ['_fsdp_wrapped_module.rope.freqs']

Expected layers.76.ffn_norm.weight to be bf16, got torch.float32
Expected layers.77.attention.wo.weight to be bf16, got torch.float32Expected layers.76.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.78.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.78.ffn_norm.weight to be bf16, got torch.float32

Expected layers.77.attention.wq.weight to be bf16, got torch.float32Expected layers.77.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.76.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.78.attention_norm.weight to be bf16, got torch.float32
Expected layers.79.attention.wq.weight to be bf16, got torch.float32

Expected layers.77.attention.wk.weight to be bf16, got torch.float32Expected layers.77.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.76.attention_norm.weight to be bf16, got torch.float32Expected layers.78.ffn_norm.weight to be bf16, got torch.float32
Expected layers.79.attention.wk.weight to be bf16, got torch.float32

Expected layers.77.attention.wv.weight to be bf16, got torch.float32Expected layers.77.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.76.ffn_norm.weight to be bf16, got torch.float32Expected layers.79.attention.wq.weight to be bf16, got torch.float32Expected layers.79.attention.wv.weight to be bf16, got torch.float32


Expected layers.77.attention.wo.weight to be bf16, got torch.float32Expected layers.77.attention_norm.weight to be bf16, got torch.float32

Expected layers.77.attention.wq.weight to be bf16, got torch.float32Expected layers.79.attention.wk.weight to be bf16, got torch.float32
Expected layers.79.attention.wo.weight to be bf16, got torch.float32

Expected layers.77.feed_forward.w1.weight to be bf16, got torch.float32Expected layers.77.ffn_norm.weight to be bf16, got torch.float32
Expected layers.77.attention.wk.weight to be bf16, got torch.float32

Expected layers.79.attention.wv.weight to be bf16, got torch.float32Expected layers.79.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.77.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.78.attention.wq.weight to be bf16, got torch.float32Expected layers.77.attention.wv.weight to be bf16, got torch.float32


Expected layers.79.feed_forward.w2.weight to be bf16, got torch.float32Expected layers.79.attention.wo.weight to be bf16, got torch.float32

Expected layers.77.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.78.attention.wk.weight to be bf16, got torch.float32
Expected layers.77.attention.wo.weight to be bf16, got torch.float32

Expected layers.79.feed_forward.w3.weight to be bf16, got torch.float32Expected layers.79.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.77.attention_norm.weight to be bf16, got torch.float32Expected layers.78.attention.wv.weight to be bf16, got torch.float32
Expected layers.77.feed_forward.w1.weight to be bf16, got torch.float32

Expected layers.79.attention_norm.weight to be bf16, got torch.float32Expected layers.79.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.77.ffn_norm.weight to be bf16, got torch.float32Expected layers.78.attention.wo.weight to be bf16, got torch.float32
Expected layers.77.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.79.ffn_norm.weight to be bf16, got torch.float32Expected layers.79.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.78.attention.wq.weight to be bf16, got torch.float32Expected layers.78.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.77.feed_forward.w3.weight to be bf16, got torch.float32
Expected norm.weight to be bf16, got torch.float32
Expected layers.79.attention_norm.weight to be bf16, got torch.float32
Expected layers.78.attention.wk.weight to be bf16, got torch.float32
Expected layers.78.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.77.attention_norm.weight to be bf16, got torch.float32Expected output.weight to be bf16, got torch.float32

Expected layers.79.ffn_norm.weight to be bf16, got torch.float32Expected layers.78.attention.wv.weight to be bf16, got torch.float32Expected layers.78.feed_forward.w3.weight to be bf16, got torch.float32


Expected layers.77.ffn_norm.weight to be bf16, got torch.float32
Expected norm.weight to be bf16, got torch.float32Expected layers.78.attention.wo.weight to be bf16, got torch.float32
Expected layers.78.attention_norm.weight to be bf16, got torch.float32
Expected layers.78.attention.wq.weight to be bf16, got torch.float32

Expected output.weight to be bf16, got torch.float32
Expected layers.78.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.78.ffn_norm.weight to be bf16, got torch.float32Expected layers.78.attention.wk.weight to be bf16, got torch.float32

Expected layers.78.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.79.attention.wq.weight to be bf16, got torch.float32
Expected layers.78.attention.wv.weight to be bf16, got torch.float32
Expected layers.78.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.79.attention.wk.weight to be bf16, got torch.float32
Expected layers.78.attention.wo.weight to be bf16, got torch.float32
Expected layers.78.attention_norm.weight to be bf16, got torch.float32
Expected layers.79.attention.wv.weight to be bf16, got torch.float32
Expected layers.78.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.78.ffn_norm.weight to be bf16, got torch.float32
Expected layers.79.attention.wo.weight to be bf16, got torch.float32
Expected layers.78.feed_forward.w2.weight to be bf16, got torch.float32
Expected layers.79.attention.wq.weight to be bf16, got torch.float32
Expected layers.79.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.78.feed_forward.w3.weight to be bf16, got torch.float32
Expected layers.79.attention.wk.weight to be bf16, got torch.float32Expected layers.79.feed_forward.w2.weight to be bf16, got torch.float32

Expected layers.78.attention_norm.weight to be bf16, got torch.float32
Expected layers.79.attention.wv.weight to be bf16, got torch.float32Expected layers.79.feed_forward.w3.weight to be bf16, got torch.float32

Expected layers.78.ffn_norm.weight to be bf16, got torch.float32
Expected layers.79.attention.wo.weight to be bf16, got torch.float32
Expected layers.79.attention_norm.weight to be bf16, got torch.float32Expected layers.79.attention.wq.weight to be bf16, got torch.float32

Expected layers.79.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.79.ffn_norm.weight to be bf16, got torch.float32Expected layers.79.attention.wk.weight to be bf16, got torch.float32

Expected layers.79.feed_forward.w2.weight to be bf16, got torch.float32
Expected norm.weight to be bf16, got torch.float32Expected layers.79.attention.wv.weight to be bf16, got torch.float32

Expected layers.79.feed_forward.w3.weight to be bf16, got torch.float32
Expected output.weight to be bf16, got torch.float32Expected layers.79.attention.wo.weight to be bf16, got torch.float32

Expected layers.79.attention_norm.weight to be bf16, got torch.float32
Expected layers.79.feed_forward.w1.weight to be bf16, got torch.float32
Expected layers.79.ffn_norm.weight to be bf16, got torch.float32
Expected layers.79.feed_forward.w2.weight to be bf16, got torch.float32
Expected norm.weight to be bf16, got torch.float32
Expected layers.79.feed_forward.w3.weight to be bf16, got torch.float32
Expected output.weight to be bf16, got torch.float32
Expected layers.79.attention_norm.weight to be bf16, got torch.float32
RV: [REF] missing [] unexpected ['_fsdp_wrapped_module.rope.freqs']Expected layers.79.ffn_norm.weight to be bf16, got torch.float32

Expected norm.weight to be bf16, got torch.float32
Expected output.weight to be bf16, got torch.float32
RV: [REF] missing [] unexpected ['_fsdp_wrapped_module.rope.freqs']
RV: [REF] missing [] unexpected ['_fsdp_wrapped_module.rope.freqs']
RV: [REF] missing [] unexpected ['_fsdp_wrapped_module.rope.freqs']
RV: [REF] missing [] unexpected ['_fsdp_wrapped_module.rope.freqs']
RV: [REF] missing [] unexpected ['_fsdp_wrapped_module.rope.freqs']
RV: [REF] missing [] unexpected ['_fsdp_wrapped_module.rope.freqs']
Got parity -437168.875 -437168.90625

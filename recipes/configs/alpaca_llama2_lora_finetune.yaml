# Dataset and Dataloader
dataset: alpaca
seed: null
shuffle: True

# Model Arguments
model: lora_llama2_7b
model_checkpoint: /tmp/llama2-7b
lora_attn_modules: ['q_proj', 'v_proj']
tokenizer: llama2_tokenizer
tokenizer_checkpoint: /tmp/tokenizer.model

# Fine-tuning arguments
batch_size: 2
lr: 2e-5
epochs: 3
optimizer: SGD
loss: CrossEntropyLoss
output_dir: /tmp/alpaca-llama2-lora-finetune
device: cuda
dtype: fp32
resume_from_checkpoint: False
